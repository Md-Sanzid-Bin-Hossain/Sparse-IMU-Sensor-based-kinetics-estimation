{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Md-Sanzid-Bin-Hossain/Sparse-IMU-Sensor-based-kinetics-estimation/blob/main/Sensor_Knowledge_distillation_Dataset_B_Kinetics_Multi_modal_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H__KTa0RNQDo",
        "outputId": "789d92e3-7bc4-435a-f6f4-878c3fb20e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.1+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4130 sha256=d8f3f834a16d72fa37409cc8578370928120a01be8912e3e629cc29a2a88309b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz\n",
        "# !pip install tsf\n",
        "\n",
        "\n",
        "import h5py\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy\n",
        "import statistics\n",
        "from numpy import loadtxt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statistics import stdev\n",
        "import math\n",
        "import h5py\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from scipy.signal import butter,filtfilt\n",
        "import sys\n",
        "import numpy as np # linear algebra\n",
        "from scipy.stats import randint\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
        "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
        "import seaborn as sns # used for plot interactive graph.\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from tsf.model import TransformerForecaster\n",
        "\n",
        "\n",
        "# from tensorflow.keras.utils import np_utils\n",
        "import itertools\n",
        "###  Library for attention layers\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "#from tqdm import tqdm # Processing time measurement\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import statistics\n",
        "import gc\n",
        "import torch.nn.init as init\n",
        "\n",
        "############################################################################################################################################################################\n",
        "############################################################################################################################################################################\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.utils.weight_norm as weight_norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrXnoV0Js5CO"
      },
      "source": [
        "# File path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtfW7Ij43QV9"
      },
      "source": [
        "# Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn4MLG7u3QWD"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    with h5py.File('/content/drive/My Drive/public dataset/all_17_subjects.h5', 'r') as hf:\n",
        "        data_all_sub = {subject: subject_data[:] for subject, subject_data in hf.items()}\n",
        "        data_fields = json.loads(hf.attrs['columns'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1Rygjvcmq90"
      },
      "outputs": [],
      "source": [
        "def data_extraction(A):\n",
        "  for k in range(len(A)):\n",
        "    zero_index_1=np.all(A[k:k+1,:,:] == 0, axis=0)\n",
        "    zero_index = np.multiply(zero_index_1, 1)\n",
        "    zero_index=np.array(zero_index)\n",
        "\n",
        "    for i in range(len(zero_index)):\n",
        "      if (sum(zero_index[i])==256):\n",
        "        index=i\n",
        "        break;\n",
        "\n",
        "    # print(index)\n",
        "### Taking only the stance phase of the gait\n",
        "###################################################################################################################################################\n",
        "    B=A[k:k+1,0:index,:]  ### Taking only the stance phase of the gait\n",
        "    C_1=B.reshape((B.shape[0]*B.shape[1],B.shape[2]))\n",
        "    if (k==0):\n",
        "      C=C_1\n",
        "    else:\n",
        "      C=np.append(C,C_1,axis=0)\n",
        "\n",
        "  index_24 = data_fields.index('body weight')\n",
        "  index_25 = data_fields.index('body height')\n",
        "\n",
        "  BW=(C[0:1, index_24]*9.8)\n",
        "  BWH=(C[0:1, index_24]*9.8)*C[:, index_25]\n",
        "\n",
        "  V=C[:,110:154]\n",
        "  V=V.reshape(V.shape[0],11,4)\n",
        "\n",
        "  V=(V-V[:,2:3,:])/C[0:1, index_25]\n",
        "  V=V.reshape(-1,44)\n",
        "\n",
        "      ### IMUs- Chest, Waist, Right Foot, Right shank, Right thigh, Left Foot, Left shank, Left thigh, 2D-body coordinate\n",
        "    ### 0:48- IMU, 48:92-2D body coordinate, 92:97-- Target\n",
        "\n",
        "  D=np.hstack((C[:,71:77],C[:,58:64],C[:,19:25],C[:,32:38],C[:,45:51],C[:,6:12],C[:,84:90],C[:,97:103],V,C[:,3:5],-C[:, 154:155]/BW,\n",
        "              -C[:, 156:157]/BW,-C[:, 155:156]/BW))\n",
        "\n",
        "\n",
        "  # D=np.hstack((C[:,70:76],C[:,57:63],C[:,18:24],C[:,31:37],C[:,44:50],C[:,5:11],C[:,83:89],C[:,96:102],C[:,109:153]))\n",
        "\n",
        "  return D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6sRqaUhw2S7"
      },
      "outputs": [],
      "source": [
        "# print(np.array(data_fields))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d7CRzCoShWB"
      },
      "outputs": [],
      "source": [
        "# index_21 = data_fields.index('plate_2_force_x')\n",
        "# print(index_21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZIw9tIU2Q7H",
        "outputId": "7f3342ea-aa12-4458-eff3-1f4f3283301f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(84490, 97)\n"
          ]
        }
      ],
      "source": [
        "data_subject_01 = data_all_sub['subject_01']\n",
        "subject_1=data_extraction(data_subject_01)\n",
        "\n",
        "print(subject_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwo6ALnFONNS"
      },
      "outputs": [],
      "source": [
        "data_subject_01 = data_all_sub['subject_01']\n",
        "data_subject_02 = data_all_sub['subject_02']\n",
        "data_subject_03 = data_all_sub['subject_03']\n",
        "data_subject_04 = data_all_sub['subject_04']\n",
        "data_subject_05 = data_all_sub['subject_05']\n",
        "data_subject_06 = data_all_sub['subject_06']\n",
        "data_subject_07 = data_all_sub['subject_07']\n",
        "data_subject_08 = data_all_sub['subject_08']\n",
        "data_subject_09 = data_all_sub['subject_09']\n",
        "data_subject_10 = data_all_sub['subject_10']\n",
        "data_subject_11 = data_all_sub['subject_11']\n",
        "data_subject_12 = data_all_sub['subject_12']\n",
        "data_subject_13 = data_all_sub['subject_13']\n",
        "data_subject_14 = data_all_sub['subject_14']\n",
        "data_subject_15 = data_all_sub['subject_15']\n",
        "data_subject_16 = data_all_sub['subject_16']\n",
        "data_subject_17 = data_all_sub['subject_17']\n",
        "\n",
        "\n",
        "subject_1=data_extraction(data_subject_01)\n",
        "subject_2=data_extraction(data_subject_02)\n",
        "subject_3=data_extraction(data_subject_03)\n",
        "subject_4=data_extraction(data_subject_04)\n",
        "subject_5=data_extraction(data_subject_05)\n",
        "subject_6=data_extraction(data_subject_06)\n",
        "subject_7=data_extraction(data_subject_07)\n",
        "subject_8=data_extraction(data_subject_08)\n",
        "subject_9=data_extraction(data_subject_09)\n",
        "subject_10=data_extraction(data_subject_10)\n",
        "subject_11=data_extraction(data_subject_11)\n",
        "subject_12=data_extraction(data_subject_12)\n",
        "subject_13=data_extraction(data_subject_13)\n",
        "subject_14=data_extraction(data_subject_14)\n",
        "subject_15=data_extraction(data_subject_15)\n",
        "subject_16=data_extraction(data_subject_16)\n",
        "subject_17=data_extraction(data_subject_17)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsCxXC1B-JXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd5e4737-7ab7-4932-b8bc-1c562134f5cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84490, 97)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "subject_1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2dkg_ho3QWF"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrGEmfn83QWF"
      },
      "outputs": [],
      "source": [
        "main_dir = \"/content/drive/My Drive/public dataset/Public_dataset_2/Subject01\"\n",
        "# os.mkdir(main_dir)\n",
        "path=\"/content/\"\n",
        "subject='Subject_01'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4mhEfV53QWF"
      },
      "outputs": [],
      "source": [
        "train_dataset=np.concatenate((subject_1,subject_2,subject_3,subject_4,subject_5,\n",
        "                              subject_6,subject_7,subject_8,subject_9,subject_10,subject_11,subject_12,subject_13,subject_14,subject_16,subject_17),axis=0)\n",
        "\n",
        "test_dataset=subject_15\n",
        "\n",
        "encoder='CNN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14923f6d-7c28-4143-a264-42810b849857",
        "id": "eu0buqDE3QWF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "(1233818, 97)\n",
            "(1233850, 5)\n",
            "(19741, 50, 92) (19741, 50, 5) (4936, 50, 92) (4936, 50, 5)\n",
            "19741\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Train features #\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# x_train_1=train_dataset[:,0:18]\n",
        "# x_train_2=train_dataset[:,23:67]\n",
        "\n",
        "# x_train=np.concatenate((x_train_1,x_train_2),axis=1)\n",
        "\n",
        "x_train=train_dataset[:,0:92]\n",
        "\n",
        "\n",
        "scale= StandardScaler()\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train_X_1_1=x_train\n",
        "\n",
        "# # Test features #\n",
        "# x_test_1=test_dataset[:,0:18]\n",
        "# x_test_2=test_dataset[:,23:67]\n",
        "\n",
        "# x_test=np.concatenate((x_test_1,x_test_2),axis=1)\n",
        "\n",
        "x_test=test_dataset[:,0:92]\n",
        "\n",
        "test_X_1_1=x_test\n",
        "\n",
        "m1=92\n",
        "m2=97\n",
        "\n",
        "\n",
        "  ### Label ###\n",
        "\n",
        "train_y_1_1=train_dataset[:,m1:m2]\n",
        "test_y_1_1=test_dataset[:,m1:m2]\n",
        "\n",
        "train_dataset_1=np.concatenate((train_X_1_1,train_y_1_1),axis=1)\n",
        "test_dataset_1=np.concatenate((test_X_1_1,test_y_1_1),axis=1)\n",
        "\n",
        "train_dataset_1=pd.DataFrame(train_dataset_1)\n",
        "test_dataset_1=pd.DataFrame(test_dataset_1)\n",
        "\n",
        "train_dataset_1.dropna(axis=0,inplace=True)\n",
        "test_dataset_1.dropna(axis=0,inplace=True)\n",
        "\n",
        "train_dataset_1=np.array(train_dataset_1)\n",
        "test_dataset_1=np.array(test_dataset_1)\n",
        "\n",
        "train_dataset_sum = np. sum(train_dataset_1)\n",
        "array_has_nan = np. isinf(train_dataset_1[:,48:92])\n",
        "\n",
        "print(array_has_nan)\n",
        "\n",
        "print(train_dataset_1.shape)\n",
        "\n",
        "\n",
        "\n",
        "train_X_1=train_dataset_1[:,0:m1]\n",
        "test_X_1=test_dataset_1[:,0:m1]\n",
        "\n",
        "train_y_1=train_dataset_1[:,m1:m2]\n",
        "test_y_1=test_dataset_1[:,m1:m2]\n",
        "\n",
        "\n",
        "\n",
        "L1=len(train_X_1)\n",
        "L2=len(test_X_1)\n",
        "\n",
        "\n",
        "w=50\n",
        "\n",
        "\n",
        "\n",
        "a1=L1//w\n",
        "b1=L1%w\n",
        "\n",
        "a2=L2//w\n",
        "b2=L2%w\n",
        "\n",
        "# a3=L3//w\n",
        "# b3=L3%w\n",
        "\n",
        "     #### Features ####\n",
        "train_X_2=train_X_1[L1-w+b1:L1,:]\n",
        "test_X_2=test_X_1[L2-w+b2:L2,:]\n",
        "# validation_X_2=validation_X_1[L3-w+b3:L3,:]\n",
        "\n",
        "\n",
        "    #### Output ####\n",
        "\n",
        "train_y_2=train_y_1[L1-w+b1:L1,:]\n",
        "test_y_2=test_y_1[L2-w+b2:L2,:]\n",
        "# validation_y_2=validation_y_1[L3-w+b3:L3,:]\n",
        "\n",
        "\n",
        "\n",
        "     #### Features ####\n",
        "\n",
        "train_X=np.concatenate((train_X_1,train_X_2),axis=0)\n",
        "test_X=np.concatenate((test_X_1,test_X_2),axis=0)\n",
        "# validation_X=np.concatenate((validation_X_1,validation_X_2),axis=0)\n",
        "\n",
        "\n",
        "    #### Output ####\n",
        "\n",
        "train_y=np.concatenate((train_y_1,train_y_2),axis=0)\n",
        "test_y=np.concatenate((test_y_1,test_y_2),axis=0)\n",
        "# validation_y=np.concatenate((validation_y_1,validation_y_2),axis=0)\n",
        "\n",
        "\n",
        "print(train_y.shape)\n",
        "    #### Reshaping ####\n",
        "train_X_3_p= train_X.reshape((a1+1,w,train_X.shape[1]))\n",
        "test_X = test_X.reshape((a2+1,w,test_X.shape[1]))\n",
        "\n",
        "\n",
        "train_y_3_p= train_y.reshape((a1+1,w,5))\n",
        "test_y= test_y.reshape((a2+1,w,5))\n",
        "\n",
        "\n",
        "\n",
        "# train_X_1D=train_X_3\n",
        "test_X_1D=test_X\n",
        "\n",
        "train_X_3=train_X_3_p\n",
        "train_y_3=train_y_3_p\n",
        "# print(train_X_4.shape,train_y_3.shape)\n",
        "\n",
        "\n",
        "train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)\n",
        "#train_X_1D, X_validation_1D_ridge, train_y, Y_validation_ridge = train_test_split(train_X_1D_m,train_y_m, test_size=0.10, random_state=True)   [0:2668,:,:]\n",
        "\n",
        "print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)\n",
        "\n",
        "features=6\n",
        "\n",
        "\n",
        "\n",
        "Bag_samples=train_X_1D.shape[0]\n",
        "print(Bag_samples)\n",
        "\n",
        "s=test_X_1D.shape[0]*w\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIBkg45H3QWG"
      },
      "outputs": [],
      "source": [
        "# from numpy import savetxt\n",
        "# savetxt('train_data_check.csv', train_dataset_1[:,48:92], delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHNxxQeC3QWG"
      },
      "outputs": [],
      "source": [
        "### IMUs- Chest, Waist, Right Foot, Right shank, Right thigh, Left Foot, Left shank, Left thigh, 2D-body coordinate\n",
        "### 0:48- IMU, 48:92-2D body coordinate, 92:97-- Target\n",
        "\n",
        "\n",
        "### Data Processing\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "val_targets = torch.Tensor(Y_validation)\n",
        "test_features = torch.Tensor(test_X_1D)\n",
        "test_targets = torch.Tensor(test_y)\n",
        "\n",
        "\n",
        "## all Modality Features\n",
        "\n",
        "train_features = torch.Tensor(train_X_1D)\n",
        "train_targets = torch.Tensor(train_y_5)\n",
        "val_features = torch.Tensor(X_validation_1D)\n",
        "\n",
        "\n",
        "train_features_acc_8=torch.cat((train_features[:,:,0:3],train_features[:,:,6:9],train_features[:,:,12:15],train_features[:,:,18:21],train_features[:,:,24:27]\\\n",
        "                             ,train_features[:,:,30:33],train_features[:,:,36:39],train_features[:,:,42:45]),axis=-1)\n",
        "test_features_acc_8=torch.cat((test_features[:,:,0:3],test_features[:,:,6:9],test_features[:,:,12:15],test_features[:,:,18:21],test_features[:,:,24:27]\\\n",
        "                             ,test_features[:,:,30:33],test_features[:,:,36:39],test_features[:,:,42:45]),axis=-1)\n",
        "val_features_acc_8=torch.cat((val_features[:,:,0:3],val_features[:,:,6:9],val_features[:,:,12:15],val_features[:,:,18:21],val_features[:,:,24:27]\\\n",
        "                             ,val_features[:,:,30:33],val_features[:,:,36:39],val_features[:,:,42:45]),axis=-1)\n",
        "\n",
        "\n",
        "train_features_gyr_8=torch.cat((train_features[:,:,3:6],train_features[:,:,9:12],train_features[:,:,15:18],train_features[:,:,21:24],train_features[:,:,27:30]\\\n",
        "                             ,train_features[:,:,33:36],train_features[:,:,39:42],train_features[:,:,45:48]),axis=-1)\n",
        "test_features_gyr_8=torch.cat((test_features[:,:,3:6],test_features[:,:,9:12],test_features[:,:,15:18],test_features[:,:,21:24],test_features[:,:,27:30]\\\n",
        "                             ,test_features[:,:,33:36],test_features[:,:,39:42],test_features[:,:,45:48]),axis=-1)\n",
        "val_features_gyr_8=torch.cat((val_features[:,:,3:6],val_features[:,:,9:12],val_features[:,:,15:18],val_features[:,:,21:24],val_features[:,:,27:30]\\\n",
        "                             ,val_features[:,:,33:36],val_features[:,:,39:42],val_features[:,:,45:48]),axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "train_features_2D_point=train_features[:,:,48:92]\n",
        "test_features_2D_point=test_features[:,:,48:92]\n",
        "val_features_2D_point=val_features[:,:,48:92]\n",
        "\n",
        "\n",
        "\n",
        "train = TensorDataset(train_features, train_features_acc_8,train_features_gyr_8, train_features_2D_point, train_targets)\n",
        "val = TensorDataset(val_features, val_features_acc_8, val_features_gyr_8, val_features_2D_point,val_targets)\n",
        "test = TensorDataset(test_features, test_features_acc_8, test_features_gyr_8, test_features_2D_point, test_targets)\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkrZS1Yo3QWG"
      },
      "source": [
        "# Important Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mXHDSlq3QWG"
      },
      "outputs": [],
      "source": [
        "def RMSE_prediction(yhat_4,test_y,s):\n",
        "\n",
        "  s1=yhat_4.shape[0]*yhat_4.shape[1]\n",
        "\n",
        "  test_o=test_y.reshape((s1,5))\n",
        "  yhat=yhat_4.reshape((s1,5))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  y_1_no=yhat[:,0]\n",
        "  y_2_no=yhat[:,1]\n",
        "  y_3_no=yhat[:,2]\n",
        "  y_4_no=yhat[:,3]\n",
        "  y_5_no=yhat[:,4]\n",
        "  # y_6_no=yhat[:,5]\n",
        "  # y_7_no=yhat[:,6]\n",
        "  #y_8_no=yhat[:,7]\n",
        "  #y_9_no=yhat[:,8]\n",
        "  #y_10_no=yhat[:,9]\n",
        "\n",
        "\n",
        "  y_1=y_1_no\n",
        "  y_2=y_2_no\n",
        "  y_3=y_3_no\n",
        "  y_4=y_4_no\n",
        "  y_5=y_5_no\n",
        "\n",
        "\n",
        "\n",
        "  y_test_1=test_o[:,0]\n",
        "  y_test_2=test_o[:,1]\n",
        "  y_test_3=test_o[:,2]\n",
        "  y_test_4=test_o[:,3]\n",
        "  y_test_5=test_o[:,4]\n",
        "  # y_test_6=test_o[:,5]\n",
        "  # y_test_7=test_o[:,6]\n",
        "  #y_test_8=test_o[:,7]\n",
        "  #y_test_9=test_o[:,8]\n",
        "  #y_test_10=test_o[:,9]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #print(y_1.shape,y_test_1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  Z_1=y_1\n",
        "  Z_2=y_2\n",
        "  Z_3=y_3\n",
        "  Z_4=y_4\n",
        "  Z_5=y_5\n",
        "  # Z_6=y_6\n",
        "  # Z_7=y_7\n",
        "  #Z_8=y_8\n",
        "  #Z_9=y_9\n",
        "  #Z_10=y_10\n",
        "\n",
        "\n",
        "\n",
        "  ###calculate RMSE\n",
        "\n",
        "  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100\n",
        "  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100\n",
        "  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100\n",
        "  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100\n",
        "  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100\n",
        "  # rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100\n",
        "  # rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100\n",
        "  #rmse_8 =((np.sqrt(mean_squared_error(y_test_8,y_8)))/(max(y_test_8)-min(y_test_8)))*100\n",
        "  #rmse_9 =((np.sqrt(mean_squared_error(y_test_9,y_9)))/(max(y_test_9)-min(y_test_9)))*100\n",
        "  #rmse_10 =((np.sqrt(mean_squared_error(y_test_10,y_10)))/(max(y_test_10)-min(y_test_10)))*100\n",
        "\n",
        "\n",
        "  print(rmse_1)\n",
        "  print(rmse_2)\n",
        "  print(rmse_3)\n",
        "  print(rmse_4)\n",
        "  print(rmse_5)\n",
        "  # print(rmse_6)\n",
        "  # print(rmse_7)\n",
        "  #print(rmse_8)\n",
        "  #print(rmse_9)\n",
        "  #print(rmse_10)\n",
        "\n",
        "\n",
        "  p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n",
        "  p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n",
        "  p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n",
        "  p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n",
        "  p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n",
        "  # p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n",
        "  # p_7=np.corrcoef(y_7, y_test_7)[0, 1]\n",
        "  #p_8=np.corrcoef(y_8, y_test_8)[0, 1]\n",
        "  #p_9=np.corrcoef(y_9, y_test_9)[0, 1]\n",
        "  #p_10=np.corrcoef(y_10, y_test_10)[0, 1]\n",
        "\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(p_1)\n",
        "  print(p_2)\n",
        "  print(p_3)\n",
        "  print(p_4)\n",
        "  print(p_5)\n",
        "  # print(p_6)\n",
        "  # print(p_7)\n",
        "  #print(p_8)\n",
        "  #print(p_9)\n",
        "  #print(p_10)\n",
        "\n",
        "\n",
        "              ### Correlation ###\n",
        "  p=np.array([p_1,p_2,p_3,p_4,p_5])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #### Mean and standard deviation ####\n",
        "\n",
        "  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])\n",
        "\n",
        "      #### Mean and standard deviation ####\n",
        "  m=statistics.mean(rmse)\n",
        "  SD=statistics.stdev(rmse)\n",
        "  print('Mean: %.3f' % m,'+/- %.3f' %SD)\n",
        "\n",
        "  m_c=statistics.mean(p)\n",
        "  SD_c=statistics.stdev(p)\n",
        "  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n",
        "\n",
        "\n",
        "\n",
        "  return rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzIQul3N3QWH"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RMSELoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        mse = nn.MSELoss()(pred, target)\n",
        "        rmse = torch.sqrt(mse)\n",
        "        return rmse\n"
      ],
      "metadata": {
        "id": "wVzTJJzbjfn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgJxMSflV52e"
      },
      "source": [
        "# Teacher Model Training -- IMU-2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FG-HuNBV52f"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0iJJkktV52f"
      },
      "outputs": [],
      "source": [
        "def train_mm_teacher(train_loader, learn_rate, EPOCHS, model,filename):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    # Defining loss function and optimizer\n",
        "    # criterion =nn.MSELoss()\n",
        "    criterion =RMSELoss()\n",
        "\n",
        "\n",
        "    # criterion=PearsonCorrLoss()\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output,x_1= model(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())\n",
        "\n",
        "            loss = criterion(output, target.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "       # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, data_acc, data_gyr, data_2D,  target in val_loader:\n",
        "                output,x_1= model(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())\n",
        "                val_loss += criterion(output, target.to(device).float())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        torch.set_printoptions(precision=4)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "\n",
        "                # Check if the validation loss has improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping if the validation loss hasn't improved for `patience` epochs\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Stopping early after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    # # Save the trained model\n",
        "    # torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Encoder Teacher Training"
      ],
      "metadata": {
        "id": "endRpZEuDUdp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0bdibsD_Ld2"
      },
      "outputs": [],
      "source": [
        "class Encoder_1(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_1, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder_2(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_2, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatingModule(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(GatingModule, self).__init__()\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(2*input_size, input_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # Apply gating mechanism\n",
        "        gate_output = self.gate(torch.cat((input1,input2),dim=-1))\n",
        "\n",
        "        # Scale the inputs based on the gate output\n",
        "        gated_input1 = input1 * gate_output\n",
        "        gated_input2 = input2 * (1 - gate_output)\n",
        "\n",
        "        # Combine the gated inputs\n",
        "        output = gated_input1 + gated_input2\n",
        "        return output"
      ],
      "metadata": {
        "id": "CYWS2knQJnoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRUKgIgADg4_"
      },
      "outputs": [],
      "source": [
        "class teacher(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):\n",
        "        super(teacher, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "        self.encoder_1_2d=Encoder_1(input_2D, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "        self.encoder_2_2d=Encoder_2(input_2D, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(2*3*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "        self.gate_3=GatingModule(128)\n",
        "\n",
        "        self.fc_kd = nn.Linear(3*128, 2*128)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*3*128+128, 2*3*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_2d):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_2d_1=self.BN_2d(x_2d_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "        x_2d_2=x_2d_1.view(-1, w, x_2d_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "        x_2d_1=self.encoder_1_2d(x_2d_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "        x_2d_2=self.encoder_2_2d(x_2d_2)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "        x_2d=self.gate_3(x_2d_1,x_2d_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)\n",
        "        x_kd=self.fc_kd(x)\n",
        "        # x_kd=F.interpolate(x, scale_factor=(2/3), mode='linear', align_corners=False)\n",
        "\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        x_3=weights_3*x[:,:,2*128:3*128]\n",
        "        out_3=x_1+x_2+x_3\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out,x_kd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "j5TbnYqfKb4I",
        "outputId": "dd85fc7a-e722-45a7-f8b1-8e0dc24e6c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 13.6080, Training Loss: 0.2877,  Validation loss: 0.1938\n",
            "Epoch: 2, time: 12.5050, Training Loss: 0.1894,  Validation loss: 0.1626\n",
            "Epoch: 3, time: 12.5804, Training Loss: 0.1671,  Validation loss: 0.1546\n",
            "Epoch: 4, time: 12.6570, Training Loss: 0.1554,  Validation loss: 0.1530\n",
            "Epoch: 5, time: 12.6972, Training Loss: 0.1461,  Validation loss: 0.1356\n",
            "Epoch: 6, time: 12.7764, Training Loss: 0.1385,  Validation loss: 0.1858\n",
            "Epoch: 7, time: 12.8257, Training Loss: 0.1349,  Validation loss: 0.1290\n",
            "Epoch: 8, time: 12.8552, Training Loss: 0.1283,  Validation loss: 0.1222\n",
            "Epoch: 9, time: 12.9173, Training Loss: 0.1274,  Validation loss: 0.1254\n",
            "Epoch: 10, time: 12.9403, Training Loss: 0.1206,  Validation loss: 0.1233\n",
            "Epoch: 11, time: 13.0063, Training Loss: 0.1193,  Validation loss: 0.1181\n",
            "Epoch: 12, time: 13.0287, Training Loss: 0.1173,  Validation loss: 0.1191\n",
            "Epoch: 13, time: 13.0739, Training Loss: 0.1138,  Validation loss: 0.1204\n",
            "Epoch: 14, time: 13.1127, Training Loss: 0.1107,  Validation loss: 0.1159\n",
            "Epoch: 15, time: 13.1312, Training Loss: 0.1078,  Validation loss: 0.1127\n",
            "Epoch: 16, time: 13.1369, Training Loss: 0.1064,  Validation loss: 0.1130\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2d971b5f0917>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m44\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_teacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_teacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_teacher.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-7730055e5478>\u001b[0m in \u001b[0;36mtrain_mm_teacher\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gyr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2D\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_worker_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;31m# If we're in a background process, concatenate directly into a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m# shared memory tensor to avoid an extra copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\u001b[0m in \u001b[0;36mget_worker_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_worker_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWorkerInfo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     r\"\"\"Returns the information about the current\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0miterator\u001b[0m \u001b[0mworker\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = teacher(24,24,44)\n",
        "\n",
        "mm_teacher = train_mm_teacher(train_loader, lr,40,model,path+'_teacher.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd803750-62c2-4925-e403-4c6f11124b72",
        "id": "kBU4maNYKb4O"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "2.5837136432528496\n",
            "6.848578900098801\n",
            "3.2832376658916473\n",
            "2.73500494658947\n",
            "2.7623312547802925\n",
            "\n",
            "\n",
            "0.9814581666472008\n",
            "0.9691122353323702\n",
            "0.9789636157441554\n",
            "0.9958326039006985\n",
            "0.983355563719899\n",
            "Mean: 3.643 +/- 1.812\n",
            "Mean: 0.982 +/- 0.010\n"
          ]
        }
      ],
      "source": [
        "mm_teacher= teacher(24,24,44)\n",
        "mm_teacher.load_state_dict(torch.load(path+'_teacher.pth'))\n",
        "mm_teacher.to(device)\n",
        "\n",
        "mm_teacher.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        output,x = mm_teacher(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_1=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Model-- 1. Foot"
      ],
      "metadata": {
        "id": "Tb38ILRtNX4D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTmA6QwQNiMn"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVoiiFkFNiM5"
      },
      "outputs": [],
      "source": [
        "def train_mm_student_1(train_loader, learn_rate, EPOCHS, model,filename):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    # Defining loss function and optimizer\n",
        "    criterion =RMSELoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=2e-5)\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_acc=torch.cat((data_acc[:,:,6:9],data_acc[:,:,15:18]),dim=-1)\n",
        "            data_gyr=torch.cat((data_gyr[:,:,6:9],data_gyr[:,:,15:18]),dim=-1)\n",
        "            output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "\n",
        "            loss = criterion(output, target.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "       # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, data_acc, data_gyr, data_2D,  target in val_loader:\n",
        "                data_acc=torch.cat((data_acc[:,:,6:9],data_acc[:,:,15:18]),dim=-1)\n",
        "                data_gyr=torch.cat((data_gyr[:,:,6:9],data_gyr[:,:,15:18]),dim=-1)\n",
        "                output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "                val_loss += criterion(output, target.to(device).float())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        torch.set_printoptions(precision=4)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "\n",
        "                # Check if the validation loss has improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping if the validation loss hasn't improved for `patience` epochs\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Stopping early after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    # # Save the trained model\n",
        "    # torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Encoder Student Training"
      ],
      "metadata": {
        "id": "qBeK43kJNiM6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOIvneh-NiM6"
      },
      "outputs": [],
      "source": [
        "class Encoder_1(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_1, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder_2(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_2, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatingModule(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(GatingModule, self).__init__()\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(2*input_size, input_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # Apply gating mechanism\n",
        "        gate_output = self.gate(torch.cat((input1,input2),dim=-1))\n",
        "\n",
        "        # Scale the inputs based on the gate output\n",
        "        gated_input1 = input1 * gate_output\n",
        "        gated_input2 = input2 * (1 - gate_output)\n",
        "\n",
        "        # Combine the gated inputs\n",
        "        output = gated_input1 + gated_input2\n",
        "        return output"
      ],
      "metadata": {
        "id": "7f3oMa1rNiM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53BTFKDfNiM7"
      },
      "outputs": [],
      "source": [
        "class student_1(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_1, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        out_3=x_1+x_2\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a0c3ca-5c0b-4f91-ab63-7eb0927942f8",
        "id": "s7iB4sFYNiM7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 8.4853, Training Loss: 0.4624,  Validation loss: 0.3626\n",
            "Epoch: 2, time: 8.5126, Training Loss: 0.3447,  Validation loss: 0.3173\n",
            "Epoch: 3, time: 8.5529, Training Loss: 0.3134,  Validation loss: 0.2932\n",
            "Epoch: 4, time: 8.5860, Training Loss: 0.2903,  Validation loss: 0.2754\n",
            "Epoch: 5, time: 8.5498, Training Loss: 0.2754,  Validation loss: 0.2631\n",
            "Epoch: 6, time: 8.5019, Training Loss: 0.2639,  Validation loss: 0.2590\n",
            "Epoch: 7, time: 8.4545, Training Loss: 0.2551,  Validation loss: 0.2517\n",
            "Epoch: 8, time: 8.4328, Training Loss: 0.2470,  Validation loss: 0.2399\n",
            "Epoch: 9, time: 8.4047, Training Loss: 0.2414,  Validation loss: 0.2407\n",
            "Epoch: 10, time: 8.4213, Training Loss: 0.2356,  Validation loss: 0.2345\n",
            "Epoch: 11, time: 8.4362, Training Loss: 0.2296,  Validation loss: 0.2333\n",
            "Epoch: 12, time: 8.4598, Training Loss: 0.2265,  Validation loss: 0.2333\n",
            "Epoch: 13, time: 8.5219, Training Loss: 0.2222,  Validation loss: 0.2308\n",
            "Epoch: 14, time: 8.4829, Training Loss: 0.2173,  Validation loss: 0.2230\n",
            "Epoch: 15, time: 8.4557, Training Loss: 0.2159,  Validation loss: 0.2201\n",
            "Epoch: 16, time: 8.4563, Training Loss: 0.2131,  Validation loss: 0.2209\n",
            "Epoch: 17, time: 8.4685, Training Loss: 0.2086,  Validation loss: 0.2202\n",
            "Epoch: 18, time: 8.4546, Training Loss: 0.2066,  Validation loss: 0.2196\n",
            "Epoch: 19, time: 8.4531, Training Loss: 0.2035,  Validation loss: 0.2373\n",
            "Epoch: 20, time: 8.4353, Training Loss: 0.2030,  Validation loss: 0.2182\n",
            "Epoch: 21, time: 8.4489, Training Loss: 0.1993,  Validation loss: 0.2167\n",
            "Epoch: 22, time: 8.4299, Training Loss: 0.1986,  Validation loss: 0.2202\n",
            "Epoch: 23, time: 8.4455, Training Loss: 0.1944,  Validation loss: 0.2146\n",
            "Epoch: 24, time: 8.4396, Training Loss: 0.1939,  Validation loss: 0.2106\n",
            "Epoch: 25, time: 8.4471, Training Loss: 0.1915,  Validation loss: 0.2088\n",
            "Epoch: 26, time: 8.4172, Training Loss: 0.1905,  Validation loss: 0.2151\n",
            "Epoch: 27, time: 8.4722, Training Loss: 0.1898,  Validation loss: 0.2069\n",
            "Epoch: 28, time: 8.4819, Training Loss: 0.1862,  Validation loss: 0.2124\n",
            "Epoch: 29, time: 8.4756, Training Loss: 0.1857,  Validation loss: 0.2042\n",
            "Epoch: 30, time: 8.4846, Training Loss: 0.1841,  Validation loss: 0.2048\n",
            "Epoch: 31, time: 8.4854, Training Loss: 0.1829,  Validation loss: 0.2082\n",
            "Epoch: 32, time: 8.4935, Training Loss: 0.1830,  Validation loss: 0.2130\n",
            "Epoch: 33, time: 8.4816, Training Loss: 0.1794,  Validation loss: 0.2106\n",
            "Epoch: 34, time: 8.4843, Training Loss: 0.1790,  Validation loss: 0.2077\n",
            "Epoch: 35, time: 8.4934, Training Loss: 0.1775,  Validation loss: 0.2074\n",
            "Epoch: 36, time: 8.5119, Training Loss: 0.1758,  Validation loss: 0.2114\n",
            "Epoch: 37, time: 8.4954, Training Loss: 0.1744,  Validation loss: 0.2099\n",
            "Epoch: 38, time: 8.5025, Training Loss: 0.1733,  Validation loss: 0.2035\n",
            "Epoch: 39, time: 8.4760, Training Loss: 0.1728,  Validation loss: 0.2043\n",
            "Epoch: 40, time: 8.4899, Training Loss: 0.1718,  Validation loss: 0.2061\n",
            "Training time: 339.38340067863464 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = student_1(6,6)\n",
        "\n",
        "mm_student_1 = train_mm_student_1(train_loader, lr,40,model,path+'_student_1.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQn-RnuTNiM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec5eb7d-bec9-4eba-d4cd-bd7ea5214ec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "6.880349665880203\n",
            "8.66403728723526\n",
            "5.513863265514374\n",
            "4.694035276770592\n",
            "4.181085526943207\n",
            "\n",
            "\n",
            "0.9176510275101243\n",
            "0.92578647941848\n",
            "0.950989652083842\n",
            "0.9875393119584376\n",
            "0.9571730967981562\n",
            "Mean: 5.987 +/- 1.811\n",
            "Mean: 0.948 +/- 0.028\n"
          ]
        }
      ],
      "source": [
        "mm_student_1= student_1(6,6)\n",
        "mm_student_1.load_state_dict(torch.load(path+'_student_1.pth'))\n",
        "mm_student_1.to(device)\n",
        "\n",
        "mm_student_1.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        data_acc=torch.cat((data_acc[:,:,6:9],data_acc[:,:,15:18]),dim=-1)\n",
        "        data_gyr=torch.cat((data_gyr[:,:,6:9],data_gyr[:,:,15:18]),dim=-1)\n",
        "        output = mm_student_1(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_2=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student+Pre-training"
      ],
      "metadata": {
        "id": "0uyvUraYOHCh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiO2hPE7RtPH"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Knowledge distillation\"\"\"\n",
        "\n",
        "def train_student_encoder_1(train_loader, learn_rate, EPOCHS, model, filename, teacher):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "\n",
        "    # Defining loss function and optimizer\n",
        "    criterion_2 =RMSELoss()\n",
        "    # criterion_2 =nn.MSELoss()\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    total_running_loss=0\n",
        "    running_loss=0\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_acc_1=torch.cat((data_acc[:,:,6:9],data_acc[:,:,15:18]),dim=-1)\n",
        "            data_gyr_1=torch.cat((data_gyr[:,:,6:9],data_gyr[:,:,15:18]),dim=-1)\n",
        "            x_student, x_student_1= model(data_acc_1.to(device).float(),data_gyr_1.to(device).float())\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_output,x_teacher_1= teacher(data_acc.to(device).float(),data_gyr.to(device).float(), data_2D.to(device).float())\n",
        "\n",
        "            # loss=criterion_2(x_student_1,x_teacher_1)+criterion_2(x_student_1,x_teacher_2)\n",
        "\n",
        "            loss=criterion_2(x_student_1,x_teacher_1)\n",
        "\n",
        "            total_loss= loss\n",
        "\n",
        "            total_running_loss += total_loss.item()\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        a=total_running_loss/len(train_loader)\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f}\")\n",
        "        torch.save(model.state_dict(), filename)\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class student_encoder_1(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_encoder_1, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc_kd=nn.Linear(2*128,2*128)\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,7)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "\n",
        "        # x_acc=torch.cat((x_acc_1,x_acc_2),dim=-1)\n",
        "        # x_gyr=torch.cat((x_gyr_1,x_gyr_2),dim=-1)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "\n",
        "        x_1=self.fc_kd(x)\n",
        "\n",
        "        # print(x_1.shape)\n",
        "\n",
        "        # out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        # gating_weights = self.gating_net(x)\n",
        "        # out_2=gating_weights*x\n",
        "\n",
        "        # weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        # weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        # x_1=weights_1*x[:,:,0:128]\n",
        "        # x_2=weights_2*x[:,:,128:2*128]\n",
        "        # out_3=x_1+x_2\n",
        "\n",
        "\n",
        "        # out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        # gating_weights_1 = self.gating_net_1(out)\n",
        "        # out=gating_weights_1*out\n",
        "\n",
        "        # out=self.fc(out)\n",
        "\n",
        "        return x,x"
      ],
      "metadata": {
        "id": "EJielY927HEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "student = student_encoder_1(6,6)\n",
        "\n",
        "teacher_trained= teacher(24,24,44)\n",
        "teacher_trained.load_state_dict(torch.load(path+'_teacher.pth'))\n",
        "teacher_trained.to(device)\n",
        "\n",
        "\n",
        "student_KD= train_student_encoder_1(train_loader, lr,30, student,path+'student_encoder_1_pretrained.pth', teacher_trained)"
      ],
      "metadata": {
        "id": "VeZh5DcW6WfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2133249-ba63-4039-ad28-370a18b66920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 9.8431, Training Loss: 0.1870\n",
            "Epoch: 2, time: 9.8769, Training Loss: 0.1730\n",
            "Epoch: 3, time: 9.9229, Training Loss: 0.1686\n",
            "Epoch: 4, time: 9.9818, Training Loss: 0.1662\n",
            "Epoch: 5, time: 9.8959, Training Loss: 0.1645\n",
            "Epoch: 6, time: 9.8446, Training Loss: 0.1634\n",
            "Epoch: 7, time: 9.8035, Training Loss: 0.1625\n",
            "Epoch: 8, time: 9.7808, Training Loss: 0.1616\n",
            "Epoch: 9, time: 9.7786, Training Loss: 0.1610\n",
            "Epoch: 10, time: 9.7886, Training Loss: 0.1605\n",
            "Epoch: 11, time: 9.8258, Training Loss: 0.1600\n",
            "Epoch: 12, time: 9.8498, Training Loss: 0.1595\n",
            "Epoch: 13, time: 9.8510, Training Loss: 0.1591\n",
            "Epoch: 14, time: 9.8396, Training Loss: 0.1588\n",
            "Epoch: 15, time: 9.8501, Training Loss: 0.1584\n",
            "Epoch: 16, time: 9.8486, Training Loss: 0.1581\n",
            "Epoch: 17, time: 9.8305, Training Loss: 0.1580\n",
            "Epoch: 18, time: 9.8394, Training Loss: 0.1576\n",
            "Epoch: 19, time: 9.8365, Training Loss: 0.1574\n",
            "Epoch: 20, time: 9.8598, Training Loss: 0.1572\n",
            "Epoch: 21, time: 9.8346, Training Loss: 0.1569\n",
            "Epoch: 22, time: 9.8443, Training Loss: 0.1567\n",
            "Epoch: 23, time: 9.8408, Training Loss: 0.1565\n",
            "Epoch: 24, time: 9.8191, Training Loss: 0.1563\n",
            "Epoch: 25, time: 9.8295, Training Loss: 0.1561\n",
            "Epoch: 26, time: 9.8320, Training Loss: 0.1560\n",
            "Epoch: 27, time: 9.8315, Training Loss: 0.1558\n",
            "Epoch: 28, time: 9.8199, Training Loss: 0.1555\n",
            "Epoch: 29, time: 9.8329, Training Loss: 0.1554\n",
            "Epoch: 30, time: 9.8319, Training Loss: 0.1553\n",
            "Training time: 295.8158826828003 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student Fine Tuning"
      ],
      "metadata": {
        "id": "7hWEu0rfUDF2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWZCK9AsU2GM"
      },
      "outputs": [],
      "source": [
        "class student_1_fine_tuning(nn.Module):\n",
        "    def __init__(self,model1, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_1_fine_tuning, self).__init__()\n",
        "\n",
        "        self.model1 = model1\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x,x_1 = self.model1(x_acc, x_gyr)\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        out_3=x_1+x_2\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_encoder= student_encoder_1(6,6)\n",
        "student_encoder.load_state_dict(torch.load(path+'student_encoder_1_pretrained.pth'))\n",
        "student_encoder.to(device)\n",
        "\n",
        "# Freeze the weights of model1\n",
        "for param in student_encoder.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "model = student_1_fine_tuning(student_encoder,6,6)\n",
        "\n",
        "student_1_fine_tuned= train_mm_student_1(train_loader, lr,40,model,path+'student_fine_tuning_1.pth')"
      ],
      "metadata": {
        "id": "6C2mTYx-VWWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01dfbbb4-766b-4791-8030-843d39f9d0d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 8.5008, Training Loss: 0.3062,  Validation loss: 0.2721\n",
            "Epoch: 2, time: 8.4692, Training Loss: 0.2232,  Validation loss: 0.2375\n",
            "Epoch: 3, time: 8.5513, Training Loss: 0.2129,  Validation loss: 0.2236\n",
            "Epoch: 4, time: 8.6113, Training Loss: 0.2102,  Validation loss: 0.2329\n",
            "Epoch: 5, time: 8.6211, Training Loss: 0.2059,  Validation loss: 0.2184\n",
            "Epoch: 6, time: 8.5923, Training Loss: 0.2043,  Validation loss: 0.2256\n",
            "Epoch: 7, time: 8.5236, Training Loss: 0.2025,  Validation loss: 0.2296\n",
            "Epoch: 8, time: 8.4942, Training Loss: 0.1980,  Validation loss: 0.2238\n",
            "Epoch: 9, time: 8.4729, Training Loss: 0.1952,  Validation loss: 0.2193\n",
            "Epoch: 10, time: 8.4531, Training Loss: 0.1954,  Validation loss: 0.2253\n",
            "Epoch: 11, time: 8.4699, Training Loss: 0.1925,  Validation loss: 0.2168\n",
            "Epoch: 12, time: 8.5181, Training Loss: 0.1912,  Validation loss: 0.2151\n",
            "Epoch: 13, time: 8.5313, Training Loss: 0.1888,  Validation loss: 0.2165\n",
            "Epoch: 14, time: 8.5648, Training Loss: 0.1867,  Validation loss: 0.2171\n",
            "Epoch: 15, time: 8.5420, Training Loss: 0.1875,  Validation loss: 0.2175\n",
            "Epoch: 16, time: 8.5304, Training Loss: 0.1856,  Validation loss: 0.2157\n",
            "Epoch: 17, time: 8.5250, Training Loss: 0.1829,  Validation loss: 0.2184\n",
            "Epoch: 18, time: 8.4983, Training Loss: 0.1812,  Validation loss: 0.2167\n",
            "Epoch: 19, time: 8.5200, Training Loss: 0.1821,  Validation loss: 0.2304\n",
            "Epoch: 20, time: 8.4996, Training Loss: 0.1808,  Validation loss: 0.2174\n",
            "Epoch: 21, time: 8.5142, Training Loss: 0.1787,  Validation loss: 0.2091\n",
            "Epoch: 22, time: 8.5482, Training Loss: 0.1767,  Validation loss: 0.2126\n",
            "Epoch: 23, time: 8.5512, Training Loss: 0.1765,  Validation loss: 0.2150\n",
            "Epoch: 24, time: 8.5253, Training Loss: 0.1775,  Validation loss: 0.2127\n",
            "Epoch: 25, time: 8.5333, Training Loss: 0.1761,  Validation loss: 0.2110\n",
            "Epoch: 26, time: 8.5291, Training Loss: 0.1725,  Validation loss: 0.2163\n",
            "Epoch: 27, time: 8.5296, Training Loss: 0.1717,  Validation loss: 0.2235\n",
            "Epoch: 28, time: 8.5196, Training Loss: 0.1714,  Validation loss: 0.2164\n",
            "Epoch: 29, time: 8.5024, Training Loss: 0.1684,  Validation loss: 0.2095\n",
            "Epoch: 30, time: 8.5193, Training Loss: 0.1697,  Validation loss: 0.2129\n",
            "Epoch: 31, time: 8.5217, Training Loss: 0.1684,  Validation loss: 0.2097\n",
            "Stopping early after 31 epochs\n",
            "Training time: 264.48987221717834 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm95J36gW1fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83eefc0a-09db-4868-bb22-3e1737313b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "6.693318486213684\n",
            "8.095352351665497\n",
            "5.365791171789169\n",
            "3.7403296679258347\n",
            "3.9224717766046524\n",
            "\n",
            "\n",
            "0.9238897533769465\n",
            "0.9402404440366783\n",
            "0.9516276157964697\n",
            "0.9918866072657103\n",
            "0.9651480793368843\n",
            "Mean: 5.563 +/- 1.854\n",
            "Mean: 0.955 +/- 0.026\n"
          ]
        }
      ],
      "source": [
        "student_1_fine_tuned= student_1_fine_tuning(student_encoder,6,6)\n",
        "student_1_fine_tuned.load_state_dict(torch.load(path+'student_fine_tuning_1.pth'))\n",
        "student_1_fine_tuned.to(device)\n",
        "\n",
        "student_1_fine_tuned.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        data_acc=torch.cat((data_acc[:,:,6:9],data_acc[:,:,15:18]),dim=-1)\n",
        "        data_gyr=torch.cat((data_gyr[:,:,6:9],data_gyr[:,:,15:18]),dim=-1)\n",
        "        output = student_1_fine_tuned(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_3=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Model-- 2. Foot+pelvis"
      ],
      "metadata": {
        "id": "mWaSVitPV9sp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUHJI234V9sv"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Lfpxn_PV9sv"
      },
      "outputs": [],
      "source": [
        "def train_mm_student_2(train_loader, learn_rate, EPOCHS, model,filename):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    # Defining loss function and optimizer\n",
        "    criterion =RMSELoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_acc=torch.cat((data_acc[:,:,3:9],data_acc[:,:,15:18]),dim=-1)\n",
        "            data_gyr=torch.cat((data_gyr[:,:,3:9],data_gyr[:,:,15:18]),dim=-1)\n",
        "            output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "\n",
        "            loss = criterion(output, target.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "       # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, data_acc, data_gyr, data_2D,  target in val_loader:\n",
        "                data_acc=torch.cat((data_acc[:,:,3:9],data_acc[:,:,15:18]),dim=-1)\n",
        "                data_gyr=torch.cat((data_gyr[:,:,3:9],data_gyr[:,:,15:18]),dim=-1)\n",
        "                output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "                val_loss += criterion(output, target.to(device).float())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        torch.set_printoptions(precision=4)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "\n",
        "                # Check if the validation loss has improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping if the validation loss hasn't improved for `patience` epochs\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Stopping early after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    # # Save the trained model\n",
        "    # torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Encoder Student Training"
      ],
      "metadata": {
        "id": "T7QDyI4BV9sv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYWaEBE0V9sw"
      },
      "outputs": [],
      "source": [
        "class Encoder_1(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_1, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder_2(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_2, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatingModule(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(GatingModule, self).__init__()\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(2*input_size, input_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # Apply gating mechanism\n",
        "        gate_output = self.gate(torch.cat((input1,input2),dim=-1))\n",
        "\n",
        "        # Scale the inputs based on the gate output\n",
        "        gated_input1 = input1 * gate_output\n",
        "        gated_input2 = input2 * (1 - gate_output)\n",
        "\n",
        "        # Combine the gated inputs\n",
        "        output = gated_input1 + gated_input2\n",
        "        return output"
      ],
      "metadata": {
        "id": "4hUx73BSV9sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2NCYQqfV9sw"
      },
      "outputs": [],
      "source": [
        "class student_2(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_2, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        out_3=x_1+x_2\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "49f1449b-9550-4e07-b835-9f3301c58d5b",
        "id": "EcidxSzfV9sw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 8.4793, Training Loss: 0.3917,  Validation loss: 0.2832\n",
            "Epoch: 2, time: 8.4411, Training Loss: 0.2734,  Validation loss: 0.2447\n",
            "Epoch: 3, time: 8.5685, Training Loss: 0.2441,  Validation loss: 0.2233\n",
            "Epoch: 4, time: 8.6621, Training Loss: 0.2254,  Validation loss: 0.2112\n",
            "Epoch: 5, time: 8.6437, Training Loss: 0.2133,  Validation loss: 0.2083\n",
            "Epoch: 6, time: 8.5710, Training Loss: 0.2038,  Validation loss: 0.2001\n",
            "Epoch: 7, time: 8.5046, Training Loss: 0.1956,  Validation loss: 0.1908\n",
            "Epoch: 8, time: 8.4677, Training Loss: 0.1910,  Validation loss: 0.1872\n",
            "Epoch: 9, time: 8.4834, Training Loss: 0.1855,  Validation loss: 0.1873\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-0efde719d6b1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_student_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_student_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_student_2.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-aa678e22a02b>\u001b[0m in \u001b[0;36mtrain_mm_student_2\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdata_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdata_gyr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = student_2(9,9)\n",
        "\n",
        "mm_student_2 = train_mm_student_2(train_loader, lr,40,model,path+'_student_2.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2a9cc0-8698-4d7b-9e78-4454bd897e48",
        "id": "Xwz1_rVcV9sw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "4.740343615412712\n",
            "8.694767951965332\n",
            "4.885757714509964\n",
            "3.4968066960573196\n",
            "3.2860320061445236\n",
            "\n",
            "\n",
            "0.9557668034596122\n",
            "0.9267932321882358\n",
            "0.9614063516630693\n",
            "0.992777594328073\n",
            "0.9733342633614612\n",
            "Mean: 5.021 +/- 2.175\n",
            "Mean: 0.962 +/- 0.024\n"
          ]
        }
      ],
      "source": [
        "mm_student_2= student_2(9,9)\n",
        "mm_student_2.load_state_dict(torch.load(path+'_student_2.pth'))\n",
        "mm_student_2.to(device)\n",
        "\n",
        "mm_student_2.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        data_acc=torch.cat((data_acc[:,:,3:9],data_acc[:,:,15:18]),dim=-1)\n",
        "        data_gyr=torch.cat((data_gyr[:,:,3:9],data_gyr[:,:,15:18]),dim=-1)\n",
        "        output = mm_student_2(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_4=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student+Pre-training"
      ],
      "metadata": {
        "id": "_rxsmIUwV9sw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeDXZdDFV9sw"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Knowledge distillation\"\"\"\n",
        "\n",
        "def train_student_encoder_2(train_loader, learn_rate, EPOCHS, model, filename, teacher):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "\n",
        "    # Defining loss function and optimizer\n",
        "    criterion_2 =RMSELoss()\n",
        "    # criterion_2 =nn.MSELoss()\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    total_running_loss=0\n",
        "    running_loss=0\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_acc_1=torch.cat((data_acc[:,:,3:9],data_acc[:,:,15:18]),dim=-1)\n",
        "            data_gyr_1=torch.cat((data_gyr[:,:,3:9],data_gyr[:,:,15:18]),dim=-1)\n",
        "            x_student, x_student_1= model(data_acc_1.to(device).float(),data_gyr_1.to(device).float())\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_output,x_teacher_1= teacher(data_acc.to(device).float(),data_gyr.to(device).float(), data_2D.to(device).float())\n",
        "\n",
        "            # loss=criterion_2(x_student_1,x_teacher_1)+criterion_2(x_student_1,x_teacher_2)\n",
        "\n",
        "            loss=criterion_2(x_student_1,x_teacher_1)\n",
        "\n",
        "            total_loss= loss\n",
        "\n",
        "            total_running_loss += total_loss.item()\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        a=total_running_loss/len(train_loader)\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f}\")\n",
        "        torch.save(model.state_dict(), filename)\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class student_encoder_2(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_encoder_2, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc_kd=nn.Linear(2*128,2*128)\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,7)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "\n",
        "        # x_acc=torch.cat((x_acc_1,x_acc_2),dim=-1)\n",
        "        # x_gyr=torch.cat((x_gyr_1,x_gyr_2),dim=-1)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "\n",
        "        x_1=self.fc_kd(x)\n",
        "\n",
        "        # print(x_1.shape)\n",
        "\n",
        "        # out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        # gating_weights = self.gating_net(x)\n",
        "        # out_2=gating_weights*x\n",
        "\n",
        "        # weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        # weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        # x_1=weights_1*x[:,:,0:128]\n",
        "        # x_2=weights_2*x[:,:,128:2*128]\n",
        "        # out_3=x_1+x_2\n",
        "\n",
        "\n",
        "        # out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        # gating_weights_1 = self.gating_net_1(out)\n",
        "        # out=gating_weights_1*out\n",
        "\n",
        "        # out=self.fc(out)\n",
        "\n",
        "        return x,x_1"
      ],
      "metadata": {
        "id": "EkyQ4Bi9V9sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "student = student_encoder_2(9,9)\n",
        "\n",
        "teacher_trained= teacher(24,24,44)\n",
        "teacher_trained.load_state_dict(torch.load(path+'_teacher.pth'))\n",
        "teacher_trained.to(device)\n",
        "\n",
        "\n",
        "student_KD= train_student_encoder_2(train_loader, lr,50, student,path+'student_encoder_2_pretrained.pth', teacher_trained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "3ecca3dc-ef11-478e-c3c5-761ce4726a12",
        "id": "mLt9qHRmV9sx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 9.8509, Training Loss: 0.1778\n",
            "Epoch: 2, time: 9.8952, Training Loss: 0.1634\n",
            "Epoch: 3, time: 10.0325, Training Loss: 0.1596\n",
            "Epoch: 4, time: 10.0508, Training Loss: 0.1576\n",
            "Epoch: 5, time: 9.9708, Training Loss: 0.1563\n",
            "Epoch: 6, time: 9.9157, Training Loss: 0.1553\n",
            "Epoch: 7, time: 9.8590, Training Loss: 0.1548\n",
            "Epoch: 8, time: 9.7788, Training Loss: 0.1540\n",
            "Epoch: 9, time: 9.8076, Training Loss: 0.1535\n",
            "Epoch: 10, time: 9.8303, Training Loss: 0.1531\n",
            "Epoch: 11, time: 9.8812, Training Loss: 0.1527\n",
            "Epoch: 12, time: 9.9033, Training Loss: 0.1522\n",
            "Epoch: 13, time: 9.9182, Training Loss: 0.1520\n",
            "Epoch: 14, time: 9.9036, Training Loss: 0.1517\n",
            "Epoch: 15, time: 9.8717, Training Loss: 0.1514\n",
            "Epoch: 16, time: 9.9397, Training Loss: 0.1512\n",
            "Epoch: 17, time: 9.8833, Training Loss: 0.1509\n",
            "Epoch: 18, time: 9.8668, Training Loss: 0.1507\n",
            "Epoch: 19, time: 9.9402, Training Loss: 0.1505\n",
            "Epoch: 20, time: 9.8968, Training Loss: 0.1504\n",
            "Epoch: 21, time: 9.9020, Training Loss: 0.1501\n",
            "Epoch: 22, time: 9.9019, Training Loss: 0.1500\n",
            "Epoch: 23, time: 9.8857, Training Loss: 0.1498\n",
            "Epoch: 24, time: 9.8782, Training Loss: 0.1497\n",
            "Epoch: 25, time: 9.9672, Training Loss: 0.1496\n",
            "Epoch: 26, time: 9.9695, Training Loss: 0.1495\n",
            "Epoch: 27, time: 9.9233, Training Loss: 0.1493\n",
            "Epoch: 28, time: 9.9058, Training Loss: 0.1492\n",
            "Epoch: 29, time: 9.9454, Training Loss: 0.1491\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-f637dcadfce0>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstudent_KD\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_student_encoder_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'student_encoder_2_pretrained.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_trained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-774422ff68f8>\u001b[0m in \u001b[0;36mtrain_student_encoder_2\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename, teacher)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mteacher_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_teacher_1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# loss=criterion_2(x_student_1,x_teacher_1)+criterion_2(x_student_1,x_teacher_2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student Fine Tuning"
      ],
      "metadata": {
        "id": "SScN_wwiV9sx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGiu6PEMV9sx"
      },
      "outputs": [],
      "source": [
        "class student_2_fine_tuning(nn.Module):\n",
        "    def __init__(self,model1, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_2_fine_tuning, self).__init__()\n",
        "\n",
        "        self.model1 = model1\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x,x_1 = self.model1(x_acc, x_gyr)\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        out_3=x_1+x_2\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_encoder= student_encoder_2(9,9)\n",
        "student_encoder.load_state_dict(torch.load(path+'student_encoder_2_pretrained.pth'))\n",
        "student_encoder.to(device)\n",
        "\n",
        "# Freeze the weights of model1\n",
        "for param in student_encoder.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "model = student_2_fine_tuning(student_encoder,9,9)\n",
        "\n",
        "student_2_fine_tuned= train_mm_student_2(train_loader, lr,40,model,path+'student_fine_tuning_2.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "7ff9dafe-5516-4a64-9bb5-4098f1c45816",
        "id": "vWiAY6yFV9sx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 8.6737, Training Loss: 0.2629,  Validation loss: 0.1974\n",
            "Epoch: 2, time: 8.6168, Training Loss: 0.1793,  Validation loss: 0.1829\n",
            "Epoch: 3, time: 8.6495, Training Loss: 0.1659,  Validation loss: 0.1759\n",
            "Epoch: 4, time: 8.6756, Training Loss: 0.1591,  Validation loss: 0.2119\n",
            "Epoch: 5, time: 8.6618, Training Loss: 0.1528,  Validation loss: 0.1907\n",
            "Epoch: 6, time: 8.6138, Training Loss: 0.1481,  Validation loss: 0.1789\n",
            "Epoch: 7, time: 8.6067, Training Loss: 0.1445,  Validation loss: 0.1747\n",
            "Epoch: 8, time: 8.5546, Training Loss: 0.1402,  Validation loss: 0.1688\n",
            "Epoch: 9, time: 8.5522, Training Loss: 0.1359,  Validation loss: 0.1760\n",
            "Epoch: 10, time: 8.5374, Training Loss: 0.1333,  Validation loss: 0.1731\n",
            "Epoch: 11, time: 8.5342, Training Loss: 0.1307,  Validation loss: 0.1693\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-5872ab94491b>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_2_fine_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mstudent_2_fine_tuned\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_student_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'student_fine_tuning_2.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-aa678e22a02b>\u001b[0m in \u001b[0;36mtrain_mm_student_2\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf89245-7d5c-457c-bea3-cc5eb364e188",
        "id": "-F1QEhExV9sx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "4.626137390732765\n",
            "8.844413608312607\n",
            "4.352302849292755\n",
            "3.302142024040222\n",
            "3.187388554215431\n",
            "\n",
            "\n",
            "0.9542574975241482\n",
            "0.9373610182523631\n",
            "0.9635497721199545\n",
            "0.9937158746497241\n",
            "0.9746039415259197\n",
            "Mean: 4.862 +/- 2.314\n",
            "Mean: 0.965 +/- 0.021\n"
          ]
        }
      ],
      "source": [
        "student_2_fine_tuned= student_2_fine_tuning(student_encoder,9,9)\n",
        "student_2_fine_tuned.load_state_dict(torch.load(path+'student_fine_tuning_2.pth'))\n",
        "student_2_fine_tuned.to(device)\n",
        "\n",
        "student_2_fine_tuned.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        data_acc=torch.cat((data_acc[:,:,3:9],data_acc[:,:,15:18]),dim=-1)\n",
        "        data_gyr=torch.cat((data_gyr[:,:,3:9],data_gyr[:,:,15:18]),dim=-1)\n",
        "        output = student_2_fine_tuned(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_5=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Model-- 3. Foot+pelvis+shank"
      ],
      "metadata": {
        "id": "yOXzCh7io3xt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uApdg2c-o3yI"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj-PnUj6o3yJ"
      },
      "outputs": [],
      "source": [
        "def train_mm_student_3(train_loader, learn_rate, EPOCHS, model,filename):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    # Defining loss function and optimizer\n",
        "    criterion =RMSELoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_acc=torch.cat((data_acc[:,:,3:12],data_acc[:,:,15:21]),dim=-1)\n",
        "            data_gyr=torch.cat((data_gyr[:,:,3:12],data_gyr[:,:,15:21]),dim=-1)\n",
        "            output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "\n",
        "            loss = criterion(output, target.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "       # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, data_acc, data_gyr, data_2D,  target in val_loader:\n",
        "                data_acc=torch.cat((data_acc[:,:,3:12],data_acc[:,:,15:21]),dim=-1)\n",
        "                data_gyr=torch.cat((data_gyr[:,:,3:12],data_gyr[:,:,15:21]),dim=-1)\n",
        "                output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "                val_loss += criterion(output, target.to(device).float())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        torch.set_printoptions(precision=4)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "\n",
        "                # Check if the validation loss has improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping if the validation loss hasn't improved for `patience` epochs\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Stopping early after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    # # Save the trained model\n",
        "    # torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Encoder Student Training"
      ],
      "metadata": {
        "id": "Wbjsj7lko3yJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lMjor8jo3yK"
      },
      "outputs": [],
      "source": [
        "class Encoder_1(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_1, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder_2(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_2, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatingModule(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(GatingModule, self).__init__()\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(2*input_size, input_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # Apply gating mechanism\n",
        "        gate_output = self.gate(torch.cat((input1,input2),dim=-1))\n",
        "\n",
        "        # Scale the inputs based on the gate output\n",
        "        gated_input1 = input1 * gate_output\n",
        "        gated_input2 = input2 * (1 - gate_output)\n",
        "\n",
        "        # Combine the gated inputs\n",
        "        output = gated_input1 + gated_input2\n",
        "        return output"
      ],
      "metadata": {
        "id": "YzsY8ZR1o3yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrSEW36Jo3yL"
      },
      "outputs": [],
      "source": [
        "class student_3(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_3, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        out_3=x_1+x_2\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "outputId": "def88538-bbdc-4fdc-a252-dc62f45f575d",
        "id": "fZs1OhY5o3yL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 8.5612, Training Loss: 0.3442,  Validation loss: 0.2510\n",
            "Epoch: 2, time: 8.5595, Training Loss: 0.2341,  Validation loss: 0.2019\n",
            "Epoch: 3, time: 8.6812, Training Loss: 0.2071,  Validation loss: 0.1986\n",
            "Epoch: 4, time: 8.7241, Training Loss: 0.1935,  Validation loss: 0.1745\n",
            "Epoch: 5, time: 8.6772, Training Loss: 0.1823,  Validation loss: 0.1699\n",
            "Epoch: 6, time: 8.6054, Training Loss: 0.1731,  Validation loss: 0.1652\n",
            "Epoch: 7, time: 8.5706, Training Loss: 0.1660,  Validation loss: 0.1592\n",
            "Epoch: 8, time: 8.5366, Training Loss: 0.1610,  Validation loss: 0.1651\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-c57d956198a6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_student_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_student_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_student_3.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-5af6e73c0d06>\u001b[0m in \u001b[0;36mtrain_mm_student_3\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdata_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdata_gyr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-a8dd840b7201>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_acc, x_gyr)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx_acc_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_1_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_acc_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx_gyr_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_1_gyr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gyr_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx_acc_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_2_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_acc_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-12d9c49ac8fc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mout_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mout_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mout_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = student_3(15,15)\n",
        "\n",
        "mm_student_3 = train_mm_student_3(train_loader, lr,40,model,path+'_student_3.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83bb2360-d4fa-4ffc-fe53-967a58177a4d",
        "id": "qfmEiOwzo3yM"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "3.8908474147319794\n",
            "10.739043354988098\n",
            "4.846644774079323\n",
            "3.7423156201839447\n",
            "3.755148872733116\n",
            "\n",
            "\n",
            "0.9685475872106778\n",
            "0.9029455479967018\n",
            "0.9565142082320094\n",
            "0.9918941433536236\n",
            "0.9700779513148762\n",
            "Mean: 5.395 +/- 3.023\n",
            "Mean: 0.958 +/- 0.033\n"
          ]
        }
      ],
      "source": [
        "mm_student_3= student_3(15,15)\n",
        "mm_student_3.load_state_dict(torch.load(path+'_student_3.pth'))\n",
        "mm_student_3.to(device)\n",
        "\n",
        "mm_student_3.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        data_acc=torch.cat((data_acc[:,:,3:12],data_acc[:,:,15:21]),dim=-1)\n",
        "        data_gyr=torch.cat((data_gyr[:,:,3:12],data_gyr[:,:,15:21]),dim=-1)\n",
        "        output = mm_student_3(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_6=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student+Pre-training"
      ],
      "metadata": {
        "id": "TpX765bzo3yM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAdfjfoBo3yN"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Knowledge distillation\"\"\"\n",
        "\n",
        "def train_student_encoder_3(train_loader, learn_rate, EPOCHS, model, filename, teacher):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "\n",
        "    # Defining loss function and optimizer\n",
        "    criterion_2 =RMSELoss()\n",
        "    # criterion_2 =nn.MSELoss()\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    total_running_loss=0\n",
        "    running_loss=0\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_acc_1=torch.cat((data_acc[:,:,3:12],data_acc[:,:,15:21]),dim=-1)\n",
        "            data_gyr_1=torch.cat((data_gyr[:,:,3:12],data_gyr[:,:,15:21]),dim=-1)\n",
        "            x_student, x_student_1= model(data_acc_1.to(device).float(),data_gyr_1.to(device).float())\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_output,x_teacher_1= teacher(data_acc.to(device).float(),data_gyr.to(device).float(), data_2D.to(device).float())\n",
        "\n",
        "            # loss=criterion_2(x_student_1,x_teacher_1)+criterion_2(x_student_1,x_teacher_2)\n",
        "\n",
        "            loss=criterion_2(x_student_1,x_teacher_1)\n",
        "\n",
        "            total_loss= loss\n",
        "\n",
        "            total_running_loss += total_loss.item()\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        a=total_running_loss/len(train_loader)\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f}\")\n",
        "        torch.save(model.state_dict(), filename)\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class student_encoder_3(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_encoder_3, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc_kd=nn.Linear(2*128,2*128)\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,7)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "\n",
        "        # x_acc=torch.cat((x_acc_1,x_acc_2),dim=-1)\n",
        "        # x_gyr=torch.cat((x_gyr_1,x_gyr_2),dim=-1)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "\n",
        "        x_1=self.fc_kd(x)\n",
        "\n",
        "        # print(x_1.shape)\n",
        "\n",
        "        # out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        # gating_weights = self.gating_net(x)\n",
        "        # out_2=gating_weights*x\n",
        "\n",
        "        # weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        # weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        # x_1=weights_1*x[:,:,0:128]\n",
        "        # x_2=weights_2*x[:,:,128:2*128]\n",
        "        # out_3=x_1+x_2\n",
        "\n",
        "\n",
        "        # out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        # gating_weights_1 = self.gating_net_1(out)\n",
        "        # out=gating_weights_1*out\n",
        "\n",
        "        # out=self.fc(out)\n",
        "\n",
        "        return x,x_1"
      ],
      "metadata": {
        "id": "kX6FUoUMo3yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "student = student_encoder_3(15,15)\n",
        "\n",
        "teacher_trained= teacher(24,24,44)\n",
        "teacher_trained.load_state_dict(torch.load(path+'_teacher.pth'))\n",
        "teacher_trained.to(device)\n",
        "\n",
        "\n",
        "student_KD= train_student_encoder_3(train_loader, lr,50, student,path+'student_encoder_3_pretrained.pth', teacher_trained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "0319328a-216b-453b-e979-f4dc645e1960",
        "id": "uziWBhDno3yN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 9.8857, Training Loss: 0.1728\n",
            "Epoch: 2, time: 9.9360, Training Loss: 0.1585\n",
            "Epoch: 3, time: 10.1198, Training Loss: 0.1553\n",
            "Epoch: 4, time: 10.1441, Training Loss: 0.1537\n",
            "Epoch: 5, time: 10.0367, Training Loss: 0.1526\n",
            "Epoch: 6, time: 9.9191, Training Loss: 0.1518\n",
            "Epoch: 7, time: 9.8252, Training Loss: 0.1511\n",
            "Epoch: 8, time: 9.8284, Training Loss: 0.1506\n",
            "Epoch: 9, time: 9.8634, Training Loss: 0.1502\n",
            "Epoch: 10, time: 10.0773, Training Loss: 0.1498\n",
            "Epoch: 11, time: 10.0450, Training Loss: 0.1494\n",
            "Epoch: 12, time: 10.1039, Training Loss: 0.1491\n",
            "Epoch: 13, time: 9.9873, Training Loss: 0.1488\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-8a35e54f2e4a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstudent_KD\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_student_encoder_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'student_encoder_3_pretrained.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_trained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-7383c627d851>\u001b[0m in \u001b[0;36mtrain_student_encoder_3\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename, teacher)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gyr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2D\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mdata_acc_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student Fine Tuning"
      ],
      "metadata": {
        "id": "oBQ-Qy-Oo3yO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbMdEJGJo3yO"
      },
      "outputs": [],
      "source": [
        "class student_3_fine_tuning(nn.Module):\n",
        "    def __init__(self,model1, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_3_fine_tuning, self).__init__()\n",
        "\n",
        "        self.model1 = model1\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "        self.gate=GatingModule(128)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x,x_1 = self.model1(x_acc, x_gyr)\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        out_3=x_1+x_2\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_encoder= student_encoder_3(15,15)\n",
        "student_encoder.load_state_dict(torch.load(path+'student_encoder_3_pretrained.pth'))\n",
        "student_encoder.to(device)\n",
        "\n",
        "# Freeze the weights of model1\n",
        "for param in student_encoder.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "model = student_3_fine_tuning(student_encoder,15,15)\n",
        "\n",
        "student_3_fine_tuned= train_mm_student_3(train_loader, lr,40,model,path+'student_fine_tuning_3.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "f0360a26-5999-4c01-8b36-ec1ffbbc776c",
        "id": "4HGwjKS1o3yO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 8.6448, Training Loss: 0.2535,  Validation loss: 0.1719\n",
            "Epoch: 2, time: 8.6364, Training Loss: 0.1687,  Validation loss: 0.1595\n",
            "Epoch: 3, time: 8.7134, Training Loss: 0.1578,  Validation loss: 0.1547\n",
            "Epoch: 4, time: 8.7755, Training Loss: 0.1489,  Validation loss: 0.1554\n",
            "Epoch: 5, time: 8.7202, Training Loss: 0.1434,  Validation loss: 0.1429\n",
            "Epoch: 6, time: 8.6405, Training Loss: 0.1394,  Validation loss: 0.1466\n",
            "Epoch: 7, time: 8.5694, Training Loss: 0.1368,  Validation loss: 0.1559\n",
            "Epoch: 8, time: 8.5484, Training Loss: 0.1308,  Validation loss: 0.1586\n",
            "Epoch: 9, time: 8.5298, Training Loss: 0.1278,  Validation loss: 0.1439\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-14504261e0ed>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_3_fine_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mstudent_3_fine_tuned\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_student_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'student_fine_tuning_3.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-5af6e73c0d06>\u001b[0m in \u001b[0;36mtrain_mm_student_3\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18de3a9-810d-4a31-f6a2-0020caa7f5c9",
        "id": "O-DLysCYo3yP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "3.851904720067978\n",
            "9.932241588830948\n",
            "4.327606409788132\n",
            "4.0343161672353745\n",
            "3.8625136017799377\n",
            "\n",
            "\n",
            "0.9747885253940132\n",
            "0.9337985253677229\n",
            "0.9631637019637446\n",
            "0.9904248537860196\n",
            "0.9744105938899327\n",
            "Mean: 5.202 +/- 2.651\n",
            "Mean: 0.967 +/- 0.021\n"
          ]
        }
      ],
      "source": [
        "student_3_fine_tuned= student_3_fine_tuning(student_encoder,15,15)\n",
        "student_3_fine_tuned.load_state_dict(torch.load(path+'student_fine_tuning_3.pth'))\n",
        "student_3_fine_tuned.to(device)\n",
        "\n",
        "student_3_fine_tuned.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        data_acc=torch.cat((data_acc[:,:,3:12],data_acc[:,:,15:21]),dim=-1)\n",
        "        data_gyr=torch.cat((data_gyr[:,:,3:12],data_gyr[:,:,15:21]),dim=-1)\n",
        "        output = student_3_fine_tuned(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_7=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Model-- 4. Foot+pelvis+shank+Thigh"
      ],
      "metadata": {
        "id": "dXSP_qmxqX3o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTbj7sCrqX3t"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwNtvzQ5qX3t"
      },
      "outputs": [],
      "source": [
        "def train_mm_student_4(train_loader, learn_rate, EPOCHS, model,filename):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    # Defining loss function and optimizer\n",
        "    criterion =RMSELoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_acc=torch.cat((data_acc[:,:,3:15],data_acc[:,:,15:24]),dim=-1)\n",
        "            data_gyr=torch.cat((data_gyr[:,:,3:15],data_gyr[:,:,15:24]),dim=-1)\n",
        "            output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "\n",
        "            loss = criterion(output, target.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "       # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, data_acc, data_gyr, data_2D,  target in val_loader:\n",
        "                data_acc=torch.cat((data_acc[:,:,3:15],data_acc[:,:,15:24]),dim=-1)\n",
        "                data_gyr=torch.cat((data_gyr[:,:,3:15],data_gyr[:,:,15:24]),dim=-1)\n",
        "                output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "                val_loss += criterion(output, target.to(device).float())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        torch.set_printoptions(precision=4)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "\n",
        "                # Check if the validation loss has improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping if the validation loss hasn't improved for `patience` epochs\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Stopping early after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    # # Save the trained model\n",
        "    # torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Encoder Student Training"
      ],
      "metadata": {
        "id": "KiviaALHqX3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haXKHt70qX3u"
      },
      "outputs": [],
      "source": [
        "class Encoder_1(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_1, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder_2(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_2, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatingModule(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(GatingModule, self).__init__()\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(2*input_size, input_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # Apply gating mechanism\n",
        "        gate_output = self.gate(torch.cat((input1,input2),dim=-1))\n",
        "\n",
        "        # Scale the inputs based on the gate output\n",
        "        gated_input1 = input1 * gate_output\n",
        "        gated_input2 = input2 * (1 - gate_output)\n",
        "\n",
        "        # Combine the gated inputs\n",
        "        output = gated_input1 + gated_input2\n",
        "        return output"
      ],
      "metadata": {
        "id": "fHVOolXsqX3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbTZkM3rqX3u"
      },
      "outputs": [],
      "source": [
        "class student_4(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_4, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        out_3=x_1+x_2\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "e31131a5-f95c-46c5-bbc1-ec3ffbc8df19",
        "id": "VfKoQl0TqX3u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 8.6364, Training Loss: 0.3260,  Validation loss: 0.2352\n",
            "Epoch: 2, time: 8.6294, Training Loss: 0.2156,  Validation loss: 0.1969\n",
            "Epoch: 3, time: 8.6997, Training Loss: 0.1896,  Validation loss: 0.1772\n",
            "Epoch: 4, time: 8.8691, Training Loss: 0.1768,  Validation loss: 0.1660\n",
            "Epoch: 5, time: 8.7738, Training Loss: 0.1684,  Validation loss: 0.1520\n",
            "Epoch: 6, time: 8.6576, Training Loss: 0.1607,  Validation loss: 0.1496\n",
            "Epoch: 7, time: 8.7099, Training Loss: 0.1542,  Validation loss: 0.1484\n",
            "Epoch: 8, time: 8.6365, Training Loss: 0.1488,  Validation loss: 0.1461\n",
            "Epoch: 9, time: 8.6282, Training Loss: 0.1434,  Validation loss: 0.1401\n",
            "Epoch: 10, time: 8.6842, Training Loss: 0.1400,  Validation loss: 0.1426\n",
            "Epoch: 11, time: 8.7024, Training Loss: 0.1369,  Validation loss: 0.1374\n",
            "Epoch: 12, time: 8.6295, Training Loss: 0.1325,  Validation loss: 0.1371\n",
            "Epoch: 13, time: 8.6921, Training Loss: 0.1287,  Validation loss: 0.1303\n",
            "Epoch: 14, time: 8.6988, Training Loss: 0.1272,  Validation loss: 0.1448\n",
            "Epoch: 15, time: 8.6817, Training Loss: 0.1245,  Validation loss: 0.1329\n",
            "Epoch: 16, time: 8.7280, Training Loss: 0.1217,  Validation loss: 0.1319\n",
            "Epoch: 17, time: 8.7053, Training Loss: 0.1202,  Validation loss: 0.1309\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-0cd2134d58e6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_student_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_student_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_student_4.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-a513270a03c9>\u001b[0m in \u001b[0;36mtrain_mm_student_4\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_addcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_exp_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mbias_correction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_addcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_exp_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mbias_correction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = student_4(21,21)\n",
        "\n",
        "mm_student_4 = train_mm_student_4(train_loader, lr,40,model,path+'_student_4.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f6bf6d-f3b7-41bc-d9a0-6759482e139b",
        "id": "CT1FdsFXqX3v"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "3.1763818114995956\n",
            "9.574662148952484\n",
            "4.259750619530678\n",
            "3.861631453037262\n",
            "3.5978350788354874\n",
            "\n",
            "\n",
            "0.973181313925462\n",
            "0.936040133820392\n",
            "0.9651863276308597\n",
            "0.9916814820077606\n",
            "0.9707103464877318\n",
            "Mean: 4.894 +/- 2.646\n",
            "Mean: 0.967 +/- 0.020\n"
          ]
        }
      ],
      "source": [
        "mm_student_4= student_4(21,21)\n",
        "mm_student_4.load_state_dict(torch.load(path+'_student_4.pth'))\n",
        "mm_student_4.to(device)\n",
        "\n",
        "mm_student_4.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        data_acc=torch.cat((data_acc[:,:,3:15],data_acc[:,:,15:24]),dim=-1)\n",
        "        data_gyr=torch.cat((data_gyr[:,:,3:15],data_gyr[:,:,15:24]),dim=-1)\n",
        "        output = mm_student_4(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_8=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student+Pre-training"
      ],
      "metadata": {
        "id": "NOUFX1oLqX3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiFoCfrOqX3v"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Knowledge distillation\"\"\"\n",
        "\n",
        "def train_student_encoder_4(train_loader, learn_rate, EPOCHS, model, filename, teacher):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "\n",
        "    # Defining loss function and optimizer\n",
        "    criterion_2 =RMSELoss()\n",
        "    # criterion_2 =nn.MSELoss()\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    total_running_loss=0\n",
        "    running_loss=0\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_acc_1=torch.cat((data_acc[:,:,3:15],data_acc[:,:,15:24]),dim=-1)\n",
        "            data_gyr_1=torch.cat((data_gyr[:,:,3:15],data_gyr[:,:,15:24]),dim=-1)\n",
        "            x_student, x_student_1= model(data_acc_1.to(device).float(),data_gyr_1.to(device).float())\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_output,x_teacher_1= teacher(data_acc.to(device).float(),data_gyr.to(device).float(), data_2D.to(device).float())\n",
        "\n",
        "            # loss=criterion_2(x_student_1,x_teacher_1)+criterion_2(x_student_1,x_teacher_2)\n",
        "\n",
        "            loss=criterion_2(x_student_1,x_teacher_1)\n",
        "\n",
        "            total_loss= loss\n",
        "\n",
        "            total_running_loss += total_loss.item()\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        a=total_running_loss/len(train_loader)\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f}\")\n",
        "        torch.save(model.state_dict(), filename)\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class student_encoder_4(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_encoder_4, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc_kd=nn.Linear(2*128,2*128)\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,7)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "\n",
        "        # x_acc=torch.cat((x_acc_1,x_acc_2),dim=-1)\n",
        "        # x_gyr=torch.cat((x_gyr_1,x_gyr_2),dim=-1)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "\n",
        "        x_1=self.fc_kd(x)\n",
        "\n",
        "        # print(x_1.shape)\n",
        "\n",
        "        # out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        # gating_weights = self.gating_net(x)\n",
        "        # out_2=gating_weights*x\n",
        "\n",
        "        # weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        # weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        # x_1=weights_1*x[:,:,0:128]\n",
        "        # x_2=weights_2*x[:,:,128:2*128]\n",
        "        # out_3=x_1+x_2\n",
        "\n",
        "\n",
        "        # out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        # gating_weights_1 = self.gating_net_1(out)\n",
        "        # out=gating_weights_1*out\n",
        "\n",
        "        # out=self.fc(out)\n",
        "\n",
        "        return x,x_1"
      ],
      "metadata": {
        "id": "ggxw5e10qX3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "student = student_encoder_4(21,21)\n",
        "\n",
        "teacher_trained= teacher(24,24,44)\n",
        "teacher_trained.load_state_dict(torch.load(path+'_teacher.pth'))\n",
        "teacher_trained.to(device)\n",
        "\n",
        "\n",
        "student_KD= train_student_encoder_4(train_loader, lr,50, student,path+'student_encoder_4_pretrained.pth', teacher_trained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "f4ce4e26-7976-4300-fa15-cdf7d5b80e6d",
        "id": "01fU033DqX3v"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 9.9873, Training Loss: 0.1705\n",
            "Epoch: 2, time: 10.0443, Training Loss: 0.1561\n",
            "Epoch: 3, time: 10.1462, Training Loss: 0.1532\n",
            "Epoch: 4, time: 10.2083, Training Loss: 0.1516\n",
            "Epoch: 5, time: 10.0773, Training Loss: 0.1505\n",
            "Epoch: 6, time: 10.2417, Training Loss: 0.1498\n",
            "Epoch: 7, time: 9.9681, Training Loss: 0.1492\n",
            "Epoch: 8, time: 9.8942, Training Loss: 0.1487\n",
            "Epoch: 9, time: 9.9934, Training Loss: 0.1483\n",
            "Epoch: 10, time: 10.0052, Training Loss: 0.1479\n",
            "Epoch: 11, time: 10.0408, Training Loss: 0.1475\n",
            "Epoch: 12, time: 10.0698, Training Loss: 0.1472\n",
            "Epoch: 13, time: 10.0172, Training Loss: 0.1470\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-8fbc5198b3e3>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstudent_KD\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_student_encoder_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'student_encoder_4_pretrained.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_trained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-315a21ddb36b>\u001b[0m in \u001b[0;36mtrain_student_encoder_4\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename, teacher)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mdata_acc_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mdata_gyr_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mx_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_student_1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-60240b0cf53a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_acc, x_gyr)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx_gyr_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx_acc_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBN_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_acc_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx_gyr_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBN_gyr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gyr_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student Fine Tuning"
      ],
      "metadata": {
        "id": "-x7hVUOWqX3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdU6v89rqX3v"
      },
      "outputs": [],
      "source": [
        "class student_4_fine_tuning(nn.Module):\n",
        "    def __init__(self,model1, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_4_fine_tuning, self).__init__()\n",
        "\n",
        "        self.model1 = model1\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x,x_1 = self.model1(x_acc, x_gyr)\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        out_3=x_1+x_2\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_encoder= student_encoder_4(21,21)\n",
        "student_encoder.load_state_dict(torch.load(path+'student_encoder_4_pretrained.pth'))\n",
        "student_encoder.to(device)\n",
        "\n",
        "# Freeze the weights of model1\n",
        "for param in student_encoder.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "model = student_4_fine_tuning(student_encoder,21,21)\n",
        "\n",
        "student_4_fine_tuned= train_mm_student_4(train_loader, lr,40,model,path+'student_fine_tuning_4.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "8078b0cb-bfe3-4846-bc3e-e4383b119c0d",
        "id": "vlMsKnOzqX3w"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 8.6968, Training Loss: 0.2407,  Validation loss: 0.1602\n",
            "Epoch: 2, time: 8.6862, Training Loss: 0.1598,  Validation loss: 0.1558\n",
            "Epoch: 3, time: 8.8027, Training Loss: 0.1474,  Validation loss: 0.1389\n",
            "Epoch: 4, time: 8.7404, Training Loss: 0.1411,  Validation loss: 0.1406\n",
            "Epoch: 5, time: 8.7321, Training Loss: 0.1348,  Validation loss: 0.1487\n",
            "Epoch: 6, time: 8.7471, Training Loss: 0.1294,  Validation loss: 0.1401\n",
            "Epoch: 7, time: 8.6786, Training Loss: 0.1252,  Validation loss: 0.1365\n",
            "Epoch: 8, time: 8.6620, Training Loss: 0.1211,  Validation loss: 0.1355\n",
            "Epoch: 9, time: 8.6439, Training Loss: 0.1186,  Validation loss: 0.1302\n",
            "Epoch: 10, time: 8.6702, Training Loss: 0.1160,  Validation loss: 0.1422\n",
            "Epoch: 11, time: 8.7967, Training Loss: 0.1153,  Validation loss: 0.1250\n",
            "Epoch: 12, time: 8.6734, Training Loss: 0.1112,  Validation loss: 0.1394\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-9ad277c68940>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_4_fine_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mstudent_4_fine_tuned\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_student_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'student_fine_tuning_4.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-a513270a03c9>\u001b[0m in \u001b[0;36mtrain_mm_student_4\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d24b1b-2d9e-4b58-d12c-252a07512ef1",
        "id": "-soMIQnMqX3w"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "3.3503375947475433\n",
            "9.336402267217636\n",
            "3.7811212241649628\n",
            "3.4340426325798035\n",
            "3.033808246254921\n",
            "\n",
            "\n",
            "0.9759305157374432\n",
            "0.9363842838667905\n",
            "0.9716942988605238\n",
            "0.993045856039963\n",
            "0.977818696709551\n",
            "Mean: 4.587 +/- 2.668\n",
            "Mean: 0.971 +/- 0.021\n"
          ]
        }
      ],
      "source": [
        "student_4_fine_tuned= student_4_fine_tuning(student_encoder,21,21)\n",
        "student_4_fine_tuned.load_state_dict(torch.load(path+'student_fine_tuning_4.pth'))\n",
        "student_4_fine_tuned.to(device)\n",
        "\n",
        "student_4_fine_tuned.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        data_acc=torch.cat((data_acc[:,:,3:15],data_acc[:,:,15:24]),dim=-1)\n",
        "        data_gyr=torch.cat((data_gyr[:,:,3:15],data_gyr[:,:,15:24]),dim=-1)\n",
        "        output = student_4_fine_tuned(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_9=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Model-- 5. Foot+pelvis+shank+Thigh+Chest"
      ],
      "metadata": {
        "id": "lXvkPd0_v0A_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZdA0z75v0BG"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LU0M0uZv0BG"
      },
      "outputs": [],
      "source": [
        "def train_mm_student_5(train_loader, learn_rate, EPOCHS, model,filename):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    # Defining loss function and optimizer\n",
        "    criterion =RMSELoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_acc=torch.cat((data_acc[:,:,0:15],data_acc[:,:,15:24]),dim=-1)\n",
        "            data_gyr=torch.cat((data_gyr[:,:,0:15],data_gyr[:,:,15:24]),dim=-1)\n",
        "            output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "\n",
        "            loss = criterion(output, target.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "       # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, data_acc, data_gyr, data_2D,  target in val_loader:\n",
        "                data_acc=torch.cat((data_acc[:,:,0:15],data_acc[:,:,15:24]),dim=-1)\n",
        "                data_gyr=torch.cat((data_gyr[:,:,0:15],data_gyr[:,:,15:24]),dim=-1)\n",
        "                output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "                val_loss += criterion(output, target.to(device).float())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        torch.set_printoptions(precision=4)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "\n",
        "                # Check if the validation loss has improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping if the validation loss hasn't improved for `patience` epochs\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Stopping early after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    # # Save the trained model\n",
        "    # torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Encoder Student Training"
      ],
      "metadata": {
        "id": "fFvuyCLsv0BG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHTvqraav0BG"
      },
      "outputs": [],
      "source": [
        "class Encoder_1(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_1, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder_2(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_2, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "\n",
        "        return out_2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatingModule(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(GatingModule, self).__init__()\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(2*input_size, input_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # Apply gating mechanism\n",
        "        gate_output = self.gate(torch.cat((input1,input2),dim=-1))\n",
        "\n",
        "        # Scale the inputs based on the gate output\n",
        "        gated_input1 = input1 * gate_output\n",
        "        gated_input2 = input2 * (1 - gate_output)\n",
        "\n",
        "        # Combine the gated inputs\n",
        "        output = gated_input1 + gated_input2\n",
        "        return output"
      ],
      "metadata": {
        "id": "2zZgWASZv0BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ0U_azZv0BH"
      },
      "outputs": [],
      "source": [
        "class student_5(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_5, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        out_3=x_1+x_2\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "211bf013-73ba-4879-8427-c7e56d2db318",
        "id": "e6ogDLhCv0BH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 8.6556, Training Loss: 0.3158,  Validation loss: 0.2191\n",
            "Epoch: 2, time: 8.6114, Training Loss: 0.2088,  Validation loss: 0.1781\n",
            "Epoch: 3, time: 8.7792, Training Loss: 0.1826,  Validation loss: 0.1874\n",
            "Epoch: 4, time: 8.8098, Training Loss: 0.1691,  Validation loss: 0.1613\n",
            "Epoch: 5, time: 8.6770, Training Loss: 0.1617,  Validation loss: 0.1492\n",
            "Epoch: 6, time: 8.6499, Training Loss: 0.1524,  Validation loss: 0.1527\n",
            "Epoch: 7, time: 8.5876, Training Loss: 0.1472,  Validation loss: 0.1416\n",
            "Epoch: 8, time: 8.5253, Training Loss: 0.1406,  Validation loss: 0.1385\n",
            "Epoch: 9, time: 8.5496, Training Loss: 0.1395,  Validation loss: 0.1334\n",
            "Epoch: 10, time: 8.5907, Training Loss: 0.1345,  Validation loss: 0.1318\n",
            "Epoch: 11, time: 8.6295, Training Loss: 0.1322,  Validation loss: 0.1277\n",
            "Epoch: 12, time: 8.7181, Training Loss: 0.1280,  Validation loss: 0.1348\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-e8f3f7928ad9>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_student_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_student_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_student_5.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-286b6c359f90>\u001b[0m in \u001b[0;36mtrain_mm_student_5\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdata_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdata_gyr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-66871113605f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_acc, x_gyr)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx_gyr_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx_acc_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBN_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_acc_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mx_gyr_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBN_gyr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gyr_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = student_5(24,24)\n",
        "\n",
        "mm_student_5 = train_mm_student_5(train_loader, lr,40,model,path+'_student_5.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee91985a-c545-4c7e-afcc-9fe9ab64829c",
        "id": "4MaK-E5Zv0BH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "2.7664324268698692\n",
            "8.931922167539597\n",
            "3.902064263820648\n",
            "3.558630868792534\n",
            "3.5716187208890915\n",
            "\n",
            "\n",
            "0.9777189855469618\n",
            "0.9411450344339893\n",
            "0.9712181009570595\n",
            "0.9928580902004392\n",
            "0.9752942001526899\n",
            "Mean: 4.546 +/- 2.487\n",
            "Mean: 0.972 +/- 0.019\n"
          ]
        }
      ],
      "source": [
        "mm_student_5= student_5(24,24)\n",
        "mm_student_5.load_state_dict(torch.load(path+'_student_5.pth'))\n",
        "mm_student_5.to(device)\n",
        "\n",
        "mm_student_5.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        data_acc=torch.cat((data_acc[:,:,0:15],data_acc[:,:,15:24]),dim=-1)\n",
        "        data_gyr=torch.cat((data_gyr[:,:,0:15],data_gyr[:,:,15:24]),dim=-1)\n",
        "        output = mm_student_5(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_10=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student+Pre-training"
      ],
      "metadata": {
        "id": "ScdsqCrxv0BH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_RgHcSnv0BH"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Knowledge distillation\"\"\"\n",
        "\n",
        "def train_student_encoder_5(train_loader, learn_rate, EPOCHS, model, filename, teacher):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "\n",
        "    # Defining loss function and optimizer\n",
        "    criterion_2 =RMSELoss()\n",
        "    # criterion_2 =nn.MSELoss()\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    total_running_loss=0\n",
        "    running_loss=0\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_acc_1=torch.cat((data_acc[:,:,0:15],data_acc[:,:,15:24]),dim=-1)\n",
        "            data_gyr_1=torch.cat((data_gyr[:,:,0:15],data_gyr[:,:,15:24]),dim=-1)\n",
        "            x_student, x_student_1= model(data_acc_1.to(device).float(),data_gyr_1.to(device).float())\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_output,x_teacher_1= teacher(data_acc.to(device).float(),data_gyr.to(device).float(), data_2D.to(device).float())\n",
        "\n",
        "            # loss=criterion_2(x_student_1,x_teacher_1)+criterion_2(x_student_1,x_teacher_2)\n",
        "\n",
        "            loss=criterion_2(x_student_1,x_teacher_1)\n",
        "\n",
        "            total_loss= loss\n",
        "\n",
        "            total_running_loss += total_loss.item()\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        a=total_running_loss/len(train_loader)\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f}\")\n",
        "        torch.save(model.state_dict(), filename)\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class student_encoder_5(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_encoder_5, self).__init__()\n",
        "\n",
        "        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n",
        "        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n",
        "        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc_kd=nn.Linear(2*128,2*128)\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,7)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "        self.gate_1=GatingModule(128)\n",
        "        self.gate_2=GatingModule(128)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_1=self.encoder_1_acc(x_acc_2)\n",
        "        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n",
        "\n",
        "        x_acc_2=self.encoder_2_acc(x_acc_2)\n",
        "        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n",
        "\n",
        "        # x_acc=torch.cat((x_acc_1,x_acc_2),dim=-1)\n",
        "        # x_gyr=torch.cat((x_gyr_1,x_gyr_2),dim=-1)\n",
        "\n",
        "        x_acc=self.gate_1(x_acc_1,x_acc_2)\n",
        "        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "\n",
        "        x_1=self.fc_kd(x)\n",
        "\n",
        "        # print(x_1.shape)\n",
        "\n",
        "        # out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        # gating_weights = self.gating_net(x)\n",
        "        # out_2=gating_weights*x\n",
        "\n",
        "        # weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        # weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        # x_1=weights_1*x[:,:,0:128]\n",
        "        # x_2=weights_2*x[:,:,128:2*128]\n",
        "        # out_3=x_1+x_2\n",
        "\n",
        "\n",
        "        # out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        # gating_weights_1 = self.gating_net_1(out)\n",
        "        # out=gating_weights_1*out\n",
        "\n",
        "        # out=self.fc(out)\n",
        "\n",
        "        return x,x_1"
      ],
      "metadata": {
        "id": "HWtffRRrv0BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "student = student_encoder_5(24,24)\n",
        "\n",
        "teacher_trained= teacher(24,24,44)\n",
        "teacher_trained.load_state_dict(torch.load(path+'_teacher.pth'))\n",
        "teacher_trained.to(device)\n",
        "\n",
        "\n",
        "student_KD= train_student_encoder_5(train_loader, lr,50, student,path+'student_encoder_5_pretrained.pth', teacher_trained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "4a071d9c-fe4d-4d53-bb5e-3e275f868dc5",
        "id": "OJJPXZ6Uv0BI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 10.0239, Training Loss: 0.1693\n",
            "Epoch: 2, time: 10.1287, Training Loss: 0.1551\n",
            "Epoch: 3, time: 10.2515, Training Loss: 0.1522\n",
            "Epoch: 4, time: 10.1440, Training Loss: 0.1507\n",
            "Epoch: 5, time: 10.0397, Training Loss: 0.1496\n",
            "Epoch: 6, time: 9.9832, Training Loss: 0.1488\n",
            "Epoch: 7, time: 9.9741, Training Loss: 0.1482\n",
            "Epoch: 8, time: 9.9343, Training Loss: 0.1477\n",
            "Epoch: 9, time: 9.9302, Training Loss: 0.1473\n",
            "Epoch: 10, time: 10.0102, Training Loss: 0.1469\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-db5453ccc83d>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstudent_KD\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_student_encoder_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'student_encoder_5_pretrained.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_trained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-cafc17659a8b>\u001b[0m in \u001b[0;36mtrain_student_encoder_5\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename, teacher)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mdata_acc_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mdata_gyr_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mx_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_student_1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-c9b09da6abd6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_acc, x_gyr)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx_gyr_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_gyr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_gyr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx_acc_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_1_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_acc_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mx_gyr_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_1_gyr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gyr_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-12d9c49ac8fc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mout_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mout_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student Fine Tuning"
      ],
      "metadata": {
        "id": "Hg8VfAogv0BI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cvDFeTOv0BI"
      },
      "outputs": [],
      "source": [
        "class student_5_fine_tuning(nn.Module):\n",
        "    def __init__(self,model1, input_acc, input_gyr, drop_prob=0.25):\n",
        "        super(student_5_fine_tuning, self).__init__()\n",
        "\n",
        "        self.model1 = model1\n",
        "\n",
        "        self.fc = nn.Linear(2*2*128+128,5)\n",
        "        self.dropout=nn.Dropout(p=0.05)\n",
        "\n",
        "               # Define the gating network\n",
        "        self.weighted_feat = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n",
        "        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n",
        "        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x,x_1 = self.model1(x_acc, x_gyr)\n",
        "        out_1, attn_output_weights=self.attention(x,x,x)\n",
        "\n",
        "        gating_weights = self.gating_net(x)\n",
        "        out_2=gating_weights*x\n",
        "\n",
        "        weights_1 = self.weighted_feat(x[:,:,0:128])\n",
        "        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n",
        "        x_1=weights_1*x[:,:,0:128]\n",
        "        x_2=weights_2*x[:,:,128:2*128]\n",
        "        out_3=x_1+x_2\n",
        "\n",
        "        out=torch.cat((out_1,out_2,out_3),dim=-1)\n",
        "\n",
        "        gating_weights_1 = self.gating_net_1(out)\n",
        "        out=gating_weights_1*out\n",
        "\n",
        "        out=self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_encoder= student_encoder_5(24,24)\n",
        "student_encoder.load_state_dict(torch.load(path+'student_encoder_5_pretrained.pth'))\n",
        "student_encoder.to(device)\n",
        "\n",
        "# Freeze the weights of model1\n",
        "for param in student_encoder.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "model = student_5_fine_tuning(student_encoder,24,24)\n",
        "\n",
        "student_5_fine_tuned= train_mm_student_5(train_loader, lr,40,model,path+'student_fine_tuning_5.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "LzmpnPsQv0BI",
        "outputId": "38b3a088-d308-4909-85ab-e6da60bf38f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 8.8471, Training Loss: 0.2343,  Validation loss: 0.1795\n",
            "Epoch: 2, time: 8.7206, Training Loss: 0.1553,  Validation loss: 0.1527\n",
            "Epoch: 3, time: 8.7632, Training Loss: 0.1446,  Validation loss: 0.1475\n",
            "Epoch: 4, time: 8.7470, Training Loss: 0.1385,  Validation loss: 0.1435\n",
            "Epoch: 5, time: 8.6877, Training Loss: 0.1307,  Validation loss: 0.1335\n",
            "Epoch: 6, time: 8.6795, Training Loss: 0.1276,  Validation loss: 0.1300\n",
            "Epoch: 7, time: 8.6329, Training Loss: 0.1236,  Validation loss: 0.1260\n",
            "Epoch: 8, time: 8.6831, Training Loss: 0.1189,  Validation loss: 0.1292\n",
            "Epoch: 9, time: 8.6723, Training Loss: 0.1163,  Validation loss: 0.1267\n",
            "Epoch: 10, time: 8.6839, Training Loss: 0.1152,  Validation loss: 0.1371\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-8c01e892299a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_5_fine_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mstudent_5_fine_tuned\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_student_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'student_fine_tuning_5.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-286b6c359f90>\u001b[0m in \u001b[0;36mtrain_mm_student_5\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPPno0S_v0BI",
        "outputId": "27e278b7-0883-41db-eaa2-0652e8c83101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1687, 50, 5)\n",
            "2.838077023625374\n",
            "8.909662812948227\n",
            "5.911599099636078\n",
            "3.2882001250982285\n",
            "4.032084718346596\n",
            "\n",
            "\n",
            "0.9770857538630414\n",
            "0.9527581581624945\n",
            "0.9402000081566408\n",
            "0.9936916521586443\n",
            "0.9734989086145829\n",
            "Mean: 4.996 +/- 2.483\n",
            "Mean: 0.967 +/- 0.021\n"
          ]
        }
      ],
      "source": [
        "student_5_fine_tuned= student_5_fine_tuning(student_encoder,24,24)\n",
        "student_5_fine_tuned.load_state_dict(torch.load(path+'student_fine_tuning_5.pth'))\n",
        "student_5_fine_tuned.to(device)\n",
        "\n",
        "student_5_fine_tuned.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):\n",
        "        data_acc=torch.cat((data_acc[:,:,0:15],data_acc[:,:,15:24]),dim=-1)\n",
        "        data_gyr=torch.cat((data_gyr[:,:,0:15],data_gyr[:,:,15:24]),dim=-1)\n",
        "        output = student_5_fine_tuned(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)\n",
        "\n",
        "ablation_11=np.hstack([rmse,p])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TrXnoV0Js5CO",
        "TtfW7Ij43QV9",
        "v2dkg_ho3QWF",
        "fkrZS1Yo3QWG",
        "FTmA6QwQNiMn"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNBq8pRNPrkGVbVhA7JYIjH",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}