# -*- coding: utf-8 -*-
"""Kinetics_DLR_FM_Net_SOTA_Comparison_Dataset_B_Kinetics_Multi_modal_Estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WqkxUTcCJqYW6kzBzVovlDI4HEqN__s3
"""





import h5py
import json
import matplotlib.pyplot as plt
import numpy as np
import numpy
import statistics
from numpy import loadtxt
import matplotlib.pyplot as plt
import pandas
import math
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statistics import stdev
import math
import h5py

import numpy as np
import time

from scipy.signal import butter,filtfilt
import sys
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph
import seaborn as sns # used for plot interactive graph.
import pandas
import matplotlib.pyplot as plt

# from tsf.model import TransformerForecaster


# from tensorflow.keras.utils import np_utils
import itertools
###  Library for attention layers
import pandas as pd
import os
import numpy as np
#from tqdm import tqdm # Processing time measurement
from sklearn.model_selection import train_test_split

import statistics
import gc
import torch.nn.init as init

############################################################################################################################################################################
############################################################################################################################################################################

import os
import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.nn.utils.weight_norm as weight_norm
from sklearn.preprocessing import StandardScaler


import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import torch.nn.functional as F
from torchsummary import summary
from torch.nn.parameter import Parameter

from torchsummary import summary



import torch.optim as optim
import gc

from tqdm import tqdm_notebook
from sklearn.preprocessing import MinMaxScaler



# Set random seeds for reproducibility
def set_seed(seed):
    random.seed(seed)  # Python random seed
    np.random.seed(seed)  # NumPy random seed
    torch.manual_seed(seed)  # PyTorch random seed
    torch.cuda.manual_seed(seed)  # PyTorch GPU random seed
    torch.cuda.manual_seed_all(seed)  # If using multi-GPU
    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior
    torch.backends.cudnn.benchmark = False  # Disable auto-tuner to find best algorithms

# Set your seed value
set_seed(42)



"""# File path

# Data loader
"""

if __name__ == '__main__':
    with h5py.File('/content/drive/My Drive/public dataset/all_17_subjects.h5', 'r') as hf:
        data_all_sub = {subject: subject_data[:] for subject, subject_data in hf.items()}
        data_fields = json.loads(hf.attrs['columns'])

def data_extraction(A):
  for k in range(len(A)):
    zero_index_1=np.all(A[k:k+1,:,:] == 0, axis=0)
    zero_index = np.multiply(zero_index_1, 1)
    zero_index=np.array(zero_index)

    for i in range(len(zero_index)):
      if (sum(zero_index[i])==256):
        index=i
        break;

    # print(index)
### Taking only the stance phase of the gait
###################################################################################################################################################
    B=A[k:k+1,0:index,:]  ### Taking only the stance phase of the gait
    C_1=B.reshape((B.shape[0]*B.shape[1],B.shape[2]))
    if (k==0):
      C=C_1
    else:
      C=np.append(C,C_1,axis=0)

  index_24 = data_fields.index('body weight')
  index_25 = data_fields.index('body height')

  BW=(C[0:1, index_24]*9.8)
  BWH=(C[0:1, index_24]*9.8)*C[:, index_25]

  V=C[:,110:154]
  V=V.reshape(V.shape[0],11,4)

  V=(V-V[:,2:3,:])/C[0:1, index_25]
  V=V.reshape(-1,44)

      ### IMUs- Chest, Waist, Right Foot, Right shank, Right thigh, Left Foot, Left shank, Left thigh, 2D-body coordinate
    ### 0:48- IMU, 48:92-2D body coordinate, 92:97-- Target

  D=np.hstack((C[:,71:77],C[:,58:64],C[:,19:25],C[:,32:38],C[:,45:51],C[:,6:12],C[:,84:90],C[:,97:103],V,C[:,3:5],-C[:, 154:155]/BW,
              -C[:, 156:157]/BW,-C[:, 155:156]/BW))


  # D=np.hstack((C[:,70:76],C[:,57:63],C[:,18:24],C[:,31:37],C[:,44:50],C[:,5:11],C[:,83:89],C[:,96:102],C[:,109:153]))

  return D

# print(np.array(data_fields))

# index_21 = data_fields.index('plate_2_force_x')
# print(index_21)

data_subject_01 = data_all_sub['subject_01']
subject_1=data_extraction(data_subject_01)

print(subject_1.shape)

data_subject_01 = data_all_sub['subject_01']
data_subject_02 = data_all_sub['subject_02']
data_subject_03 = data_all_sub['subject_03']
data_subject_04 = data_all_sub['subject_04']
data_subject_05 = data_all_sub['subject_05']
data_subject_06 = data_all_sub['subject_06']
data_subject_07 = data_all_sub['subject_07']
data_subject_08 = data_all_sub['subject_08']
data_subject_09 = data_all_sub['subject_09']
data_subject_10 = data_all_sub['subject_10']
data_subject_11 = data_all_sub['subject_11']
data_subject_12 = data_all_sub['subject_12']
data_subject_13 = data_all_sub['subject_13']
data_subject_14 = data_all_sub['subject_14']
data_subject_15 = data_all_sub['subject_15']
data_subject_16 = data_all_sub['subject_16']
data_subject_17 = data_all_sub['subject_17']


subject_1=data_extraction(data_subject_01)
subject_2=data_extraction(data_subject_02)
subject_3=data_extraction(data_subject_03)
subject_4=data_extraction(data_subject_04)
subject_5=data_extraction(data_subject_05)
subject_6=data_extraction(data_subject_06)
subject_7=data_extraction(data_subject_07)
subject_8=data_extraction(data_subject_08)
subject_9=data_extraction(data_subject_09)
subject_10=data_extraction(data_subject_10)
subject_11=data_extraction(data_subject_11)
subject_12=data_extraction(data_subject_12)
subject_13=data_extraction(data_subject_13)
subject_14=data_extraction(data_subject_14)
subject_15=data_extraction(data_subject_15)
subject_16=data_extraction(data_subject_16)
subject_17=data_extraction(data_subject_17)

subject_1.shape

"""# Data processing"""

main_dir = "/content/drive/My Drive/public dataset/Public_dataset_2/Subject01"
# os.mkdir(main_dir)
path="/content/"
subject='Subject_01'

train_dataset=np.concatenate((subject_1,subject_2,subject_3,subject_4,subject_5,
                              subject_6,subject_7,subject_8,subject_9,subject_10,subject_11,subject_12,subject_13,subject_14,subject_15,subject_16),axis=0)

test_dataset=subject_17

encoder='CNN'

# Train features #
from sklearn.preprocessing import StandardScaler

# x_train_1=train_dataset[:,0:18]
# x_train_2=train_dataset[:,23:67]

# x_train=np.concatenate((x_train_1,x_train_2),axis=1)

x_train=train_dataset[:,0:92]


scale= StandardScaler()
scaler = MinMaxScaler(feature_range=(0, 1))
train_X_1_1=x_train

# # Test features #
# x_test_1=test_dataset[:,0:18]
# x_test_2=test_dataset[:,23:67]

# x_test=np.concatenate((x_test_1,x_test_2),axis=1)

x_test=test_dataset[:,0:92]

test_X_1_1=x_test

m1=92
m2=97


  ### Label ###

train_y_1_1=train_dataset[:,m1:m2]
test_y_1_1=test_dataset[:,m1:m2]

train_dataset_1=np.concatenate((train_X_1_1,train_y_1_1),axis=1)
test_dataset_1=np.concatenate((test_X_1_1,test_y_1_1),axis=1)

train_dataset_1=pd.DataFrame(train_dataset_1)
test_dataset_1=pd.DataFrame(test_dataset_1)

train_dataset_1.dropna(axis=0,inplace=True)
test_dataset_1.dropna(axis=0,inplace=True)

train_dataset_1=np.array(train_dataset_1)
test_dataset_1=np.array(test_dataset_1)

train_dataset_sum = np. sum(train_dataset_1)
array_has_nan = np. isinf(train_dataset_1[:,48:92])

print(array_has_nan)

print(train_dataset_1.shape)



train_X_1=train_dataset_1[:,0:48]
test_X_1=test_dataset_1[:,0:48]

train_y_1=train_dataset_1[:,m1:m2]
test_y_1=test_dataset_1[:,m1:m2]



L1=len(train_X_1)
L2=len(test_X_1)


w=50



a1=L1//w
b1=L1%w

a2=L2//w
b2=L2%w

# a3=L3//w
# b3=L3%w

     #### Features ####
train_X_2=train_X_1[L1-w+b1:L1,:]
test_X_2=test_X_1[L2-w+b2:L2,:]
# validation_X_2=validation_X_1[L3-w+b3:L3,:]


    #### Output ####

train_y_2=train_y_1[L1-w+b1:L1,:]
test_y_2=test_y_1[L2-w+b2:L2,:]
# validation_y_2=validation_y_1[L3-w+b3:L3,:]



     #### Features ####

train_X=np.concatenate((train_X_1,train_X_2),axis=0)
test_X=np.concatenate((test_X_1,test_X_2),axis=0)
# validation_X=np.concatenate((validation_X_1,validation_X_2),axis=0)


    #### Output ####

train_y=np.concatenate((train_y_1,train_y_2),axis=0)
test_y=np.concatenate((test_y_1,test_y_2),axis=0)
# validation_y=np.concatenate((validation_y_1,validation_y_2),axis=0)


print(train_y.shape)
    #### Reshaping ####
train_X_3_p= train_X.reshape((a1+1,w,train_X.shape[1]))
test_X = test_X.reshape((a2+1,w,test_X.shape[1]))


train_y_3_p= train_y.reshape((a1+1,w,5))
test_y= test_y.reshape((a2+1,w,5))



# train_X_1D=train_X_3
test_X_1D=test_X

train_X_3=train_X_3_p
train_y_3=train_y_3_p
# print(train_X_4.shape,train_y_3.shape)


train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)
#train_X_1D, X_validation_1D_ridge, train_y, Y_validation_ridge = train_test_split(train_X_1D_m,train_y_m, test_size=0.10, random_state=True)   [0:2668,:,:]

print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)

features=6



Bag_samples=train_X_1D.shape[0]
print(Bag_samples)

s=test_X_1D.shape[0]*w

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

features=6
train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,8)
test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,8)
X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],features,8)
#X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)


print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)

# from numpy import savetxt
# savetxt('train_data_check.csv', train_dataset_1[:,48:92], delimiter=',')

### IMUs- Chest, Waist, Right Foot, Right shank, Right thigh, Left Foot, Left shank, Left thigh, 2D-body coordinate
### 0:48- IMU, 48:92-2D body coordinate, 92:97-- Target


### Data Processing

batch_size = 64

val_targets = torch.Tensor(Y_validation)
test_features_1D = torch.Tensor(test_X_1D)
test_features_2D = torch.Tensor(test_X_2D)
test_targets = torch.Tensor(test_y)


## all Modality Features

train_features_1D = torch.Tensor(train_X_1D)
train_features_2D = torch.Tensor(train_X_2D)
train_targets = torch.Tensor(train_y_5)
val_features_1D = torch.Tensor(X_validation_1D)
val_features_2D = torch.Tensor(X_validation_2D)

print(test_features_2D.shape)
print(train_features_2D.shape)
print(val_features_2D.shape)



train_features_acc_8=torch.cat((train_features_1D[:,:,0:3],train_features_1D[:,:,6:9],train_features_1D[:,:,12:15],train_features_1D[:,:,18:21],train_features_1D[:,:,24:27]\
                             ,train_features_1D[:,:,30:33],train_features_1D[:,:,36:39],train_features_1D[:,:,42:45]),axis=-1)
test_features_acc_8=torch.cat((test_features_1D[:,:,0:3],test_features_1D[:,:,6:9],test_features_1D[:,:,12:15],test_features_1D[:,:,18:21],test_features_1D[:,:,24:27]\
                             ,test_features_1D[:,:,30:33],test_features_1D[:,:,36:39],test_features_1D[:,:,42:45]),axis=-1)
val_features_acc_8=torch.cat((val_features_1D[:,:,0:3],val_features_1D[:,:,6:9],val_features_1D[:,:,12:15],val_features_1D[:,:,18:21],val_features_1D[:,:,24:27]\
                             ,val_features_1D[:,:,30:33],val_features_1D[:,:,36:39],val_features_1D[:,:,42:45]),axis=-1)


train_features_gyr_8=torch.cat((train_features_1D[:,:,3:6],train_features_1D[:,:,9:12],train_features_1D[:,:,15:18],train_features_1D[:,:,21:24],train_features_1D[:,:,27:30]\
                             ,train_features_1D[:,:,33:36],train_features_1D[:,:,39:42],train_features_1D[:,:,45:48]),axis=-1)
test_features_gyr_8=torch.cat((test_features_1D[:,:,3:6],test_features_1D[:,:,9:12],test_features_1D[:,:,15:18],test_features_1D[:,:,21:24],test_features_1D[:,:,27:30]\
                             ,test_features_1D[:,:,33:36],test_features_1D[:,:,39:42],test_features_1D[:,:,45:48]),axis=-1)
val_features_gyr_8=torch.cat((val_features_1D[:,:,3:6],val_features_1D[:,:,9:12],val_features_1D[:,:,15:18],val_features_1D[:,:,21:24],val_features_1D[:,:,27:30]\
                             ,val_features_1D[:,:,33:36],val_features_1D[:,:,39:42],val_features_1D[:,:,45:48]),axis=-1)



train_features_2D_point=train_features_1D[:,:,48:92]
test_features_2D_point=test_features_1D[:,:,48:92]
val_features_2D_point=val_features_1D[:,:,48:92]



train = TensorDataset(train_features_1D,train_features_2D, train_features_acc_8,train_features_gyr_8, train_features_2D_point, train_targets)
val = TensorDataset(val_features_1D, val_features_2D, val_features_acc_8, val_features_gyr_8, val_features_2D_point,val_targets)
test = TensorDataset(test_features_1D,test_features_2D, test_features_acc_8, test_features_gyr_8, test_features_2D_point, test_targets)

train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=False)
val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, drop_last=False)
test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=False)

"""# Important Functions"""

def RMSE_prediction(yhat_4,test_y,s):

  s1=yhat_4.shape[0]*yhat_4.shape[1]

  test_o=test_y.reshape((s1,5))
  yhat=yhat_4.reshape((s1,5))


  y_1_no=yhat[:,0]
  y_2_no=yhat[:,1]
  y_3_no=yhat[:,2]
  y_4_no=yhat[:,3]
  y_5_no=yhat[:,4]


  y_1=y_1_no
  y_2=y_2_no
  y_3=y_3_no
  y_4=y_4_no
  y_5=y_5_no



  y_test_1=test_o[:,0]
  y_test_2=test_o[:,1]
  y_test_3=test_o[:,2]
  y_test_4=test_o[:,3]
  y_test_5=test_o[:,4]



  Z_1=y_1
  Z_2=y_2
  Z_3=y_3
  Z_4=y_4
  Z_5=y_5


  ###calculate RMSE

  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100

  print(rmse_1)
  print(rmse_2)
  print(rmse_3)
  print(rmse_4)
  print(rmse_5)



  p_1=np.corrcoef(y_1, y_test_1)[0, 1]
  p_2=np.corrcoef(y_2, y_test_2)[0, 1]
  p_3=np.corrcoef(y_3, y_test_3)[0, 1]
  p_4=np.corrcoef(y_4, y_test_4)[0, 1]
  p_5=np.corrcoef(y_5, y_test_5)[0, 1]


  print("\n")
  print(p_1)
  print(p_2)
  print(p_3)
  print(p_4)
  print(p_5)


              ### Correlation ###
  p=np.array([p_1,p_2,p_3,p_4,p_5])


      #### Mean and standard deviation ####

  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])

      #### Mean and standard deviation ####
  m=statistics.mean(rmse)
  SD=statistics.stdev(rmse)
  print('Mean: %.3f' % m,'+/- %.3f' %SD)

  m_c=statistics.mean(p)
  SD_c=statistics.stdev(p)
  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)



  return rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


def PCC_prediction(yhat_4,test_y,s):

  s1=yhat_4.shape[0]*yhat_4.shape[1]

  test_o=test_y.reshape((s1,5))
  yhat=yhat_4.reshape((s1,5))


  y_1_no=yhat[:,0]
  y_2_no=yhat[:,1]
  y_3_no=yhat[:,2]
  y_4_no=yhat[:,3]
  y_5_no=yhat[:,4]


  y_1=y_1_no
  y_2=y_2_no
  y_3=y_3_no
  y_4=y_4_no
  y_5=y_5_no



  y_test_1=test_o[:,0]
  y_test_2=test_o[:,1]
  y_test_3=test_o[:,2]
  y_test_4=test_o[:,3]
  y_test_5=test_o[:,4]



  Y_1=y_1
  Y_2=y_2
  Y_3=y_3
  Y_4=y_4
  Y_5=y_5


  ###calculate RMSE

  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100

  print(rmse_1)
  print(rmse_2)
  print(rmse_3)
  print(rmse_4)
  print(rmse_5)


  p_1=np.corrcoef(y_1, y_test_1)[0, 1]
  p_2=np.corrcoef(y_2, y_test_2)[0, 1]
  p_3=np.corrcoef(y_3, y_test_3)[0, 1]
  p_4=np.corrcoef(y_4, y_test_4)[0, 1]
  p_5=np.corrcoef(y_5, y_test_5)[0, 1]

  print("\n")
  print(p_1)
  print(p_2)
  print(p_3)
  print(p_4)
  print(p_5)

              ### Correlation ###
  p=np.array([p_1,p_2,p_3,p_4,p_5])


      #### Mean and standard deviation ####

  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])

      #### Mean and standard deviation ####
  m=statistics.mean(rmse)
  SD=statistics.stdev(rmse)
  print('Mean: %.3f' % m,'+/- %.3f' %SD)

  m_c=statistics.mean(p)
  SD_c=statistics.stdev(p)
  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)


  return rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


# def estimate_coef(x, y):
#     # number of observations/points
#     n = np.size(x)

#     # mean of x and y vector
#     m_x = np.mean(x)
#     m_y = np.mean(y)

#     # calculating cross-deviation and deviation about x
#     SS_xy = np.sum(y*x) - n*m_y*m_x
#     SS_xx = np.sum(x*x) - n*m_x*m_x

#     # calculating regression coefficients
#     b_1 = SS_xy / SS_xx
#     b_0 = m_y - b_1*m_x

#     return (b_0, b_1)

def estimate_coef(x, y):
    # Convert input data to PyTorch tensors
    x_tensor = torch.tensor(x, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    # Calculate the number of observations/points
    n = x_tensor.size(0)

    # Calculate the mean of x and y tensors
    m_x = torch.mean(x_tensor)
    m_y = torch.mean(y_tensor)

    # Calculate cross-deviation and deviation about x
    SS_xy = torch.sum(y_tensor * x_tensor) - n * m_y * m_x
    SS_xx = torch.sum(x_tensor * x_tensor) - n * m_x * m_x

    # Calculate regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1 * m_x

    return (b_0.item(), b_1.item())

############################################################################################################################################################################################################################################################################################################################################################################################################################################################################




def DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5):

  a_1,b_1=estimate_coef(Y_1,Z_1)
  a_2,b_2=estimate_coef(Y_2,Z_2)
  a_3,b_3=estimate_coef(Y_3,Z_3)
  a_4,b_4=estimate_coef(Y_4,Z_4)
  a_5,b_5=estimate_coef(Y_5,Z_5)

  print(a_1,b_1)
  print(a_2,b_2)
  print(a_3,b_3)
  print(a_4,b_4)
  print(a_5,b_5)


  #### All 16 angles prediction  ####


  s1=yhat_4.shape[0]*yhat_4.shape[1]

  test_o=test_y.reshape((s1,5))
  yhat=yhat_4.reshape((s1,5))


  y_1_no=yhat[:,0]
  y_2_no=yhat[:,1]
  y_3_no=yhat[:,2]
  y_4_no=yhat[:,3]
  y_5_no=yhat[:,4]


  y_1=y_1_no
  y_2=y_2_no
  y_3=y_3_no
  y_4=y_4_no
  y_5=y_5_no


  y_test_1=test_o[:,0]
  y_test_2=test_o[:,1]
  y_test_3=test_o[:,2]
  y_test_4=test_o[:,3]
  y_test_5=test_o[:,4]


  y_1=y_1*b_1+a_1
  y_2=y_2*b_2+a_2
  y_3=y_3*b_3+a_3
  y_4=y_4*b_4+a_4
  y_5=y_5*b_5+a_5


  ###calculate RMSE

  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100



  print(rmse_1)
  print(rmse_2)
  print(rmse_3)
  print(rmse_4)
  print(rmse_5)




  p_1=np.corrcoef(y_1, y_test_1)[0, 1]
  p_2=np.corrcoef(y_2, y_test_2)[0, 1]
  p_3=np.corrcoef(y_3, y_test_3)[0, 1]
  p_4=np.corrcoef(y_4, y_test_4)[0, 1]
  p_5=np.corrcoef(y_5, y_test_5)[0, 1]


  print("\n")
  print(p_1)
  print(p_2)
  print(p_3)
  print(p_4)
  print(p_5)


              ### Correlation ###
  p=np.array([p_1,p_2,p_3,p_4,p_5])



      #### Mean and standard deviation ####

  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])

      #### Mean and standard deviation ####
  m=statistics.mean(rmse)
  SD=statistics.stdev(rmse)
  print('Mean: %.3f' % m,'+/- %.3f' %SD)

  m_c=statistics.mean(p)
  SD_c=statistics.stdev(p)
  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

  return rmse, p




############################################################################################################################################################################################################################################################################################################################################################################################################################################################################

# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
is_cuda = torch.cuda.is_available()

# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
if is_cuda:
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

class RMSELoss(nn.Module):
    def __init__(self):
        super(RMSELoss, self).__init__()

    def forward(self, pred, target):
        mse = nn.MSELoss()(pred, target)
        rmse = torch.sqrt(mse)
        return rmse

"""# Kinetics-FM-DLR-Net-- 2 IMUs--Feet

## Training Function
"""

class PearsonCorrLoss(nn.Module):

  def __init__(self):
    super(PearsonCorrLoss, self).__init__()

  def forward(self, y_true, y_pred):

    # Calculate mean values
    mx = torch.mean(y_true)
    my = torch.mean(y_pred)

    # Calculate differences from mean
    xm, ym = y_true - mx, y_pred - my

    # Calculate numerator and denominator of Pearson correlation coefficient
    r_num = torch.sum(torch.mul(xm, ym))
    r_den = torch.sqrt(torch.mul(torch.sum(torch.square(xm)), torch.sum(torch.square(ym))))

    # Calculate Pearson correlation coefficient
    r = r_num / r_den

    # Clamp r between 0 and 1
    r = torch.clamp(r, 0, 1.0)

    # Calculate l2 loss
    l2 = 1 - torch.square(r)



    return l2

def train_sota(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    criterion =RMSELoss()
    # criterion =correlation_coefficient_loss_joint_pytorch()

    # criterion=PearsonCorrLoss()
    optimizer= torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(train_loader):

            optimizer.zero_grad()
            data_1D=torch.cat((data_1D[:,:,12:18],data_1D[:,:,30:36]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,2:3],data_2D[:,:,:,5:6]),dim=-1)
            output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())

                    # Compute the regularization loss for the custom linear layers
            regularization_loss = 0.0
            if hasattr(model.output_GRU, 'regularizer_loss'):
                regularization_loss += model.output_GRU.regularizer_loss()
            if hasattr(model.output_C1, 'regularizer_loss'):
                regularization_loss += model.output_C1.regularizer_loss()
            if hasattr(model.output_C2, 'regularizer_loss'):
                regularization_loss += model.output_C2.regularizer_loss()

            # output= model(data_1D.to(device).float(),data_2D.to(device).float())

            loss=criterion(output_1, target.to(device).float())+criterion(output_2, target.to(device).float())+criterion(output_3, target.to(device).float())\
            +criterion(output, target.to(device).float())+regularization_loss
            loss_1=criterion(output, target.to(device).float())

            loss.backward()
            optimizer.step()

            running_loss += loss_1.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target in val_loader:
                data_1D=torch.cat((data_1D[:,:,12:18],data_1D[:,:,30:36]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,2:3],data_2D[:,:,:,5:6]),dim=-1)
                output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())
                # output= model(data_1D.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break

    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")

    return model

def train_sota_pcc(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    # criterion =RMSELoss()
    # criterion =correlation_coefficient_loss_joint_pytorch()

    criterion=PearsonCorrLoss()
    optimizer= torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(train_loader):

            optimizer.zero_grad()
            data_1D=torch.cat((data_1D[:,:,12:18],data_1D[:,:,30:36]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,2:3],data_2D[:,:,:,5:6]),dim=-1)
            output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())

                    # Compute the regularization loss for the custom linear layers
            regularization_loss = 0.0
            if hasattr(model.output_GRU, 'regularizer_loss'):
                regularization_loss += model.output_GRU.regularizer_loss()
            if hasattr(model.output_C1, 'regularizer_loss'):
                regularization_loss += model.output_C1.regularizer_loss()
            if hasattr(model.output_C2, 'regularizer_loss'):
                regularization_loss += model.output_C2.regularizer_loss()

            # output= model(data_1D.to(device).float(),data_2D.to(device).float())

            loss=criterion(output_1, target.to(device).float())+criterion(output_2, target.to(device).float())+criterion(output_3, target.to(device).float())\
            +criterion(output, target.to(device).float())+regularization_loss
            loss_1=criterion(output, target.to(device).float())


            loss.backward()
            optimizer.step()

            running_loss += loss_1.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target in val_loader:
                data_1D=torch.cat((data_1D[:,:,12:18],data_1D[:,:,30:36]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,2:3],data_2D[:,:,:,5:6]),dim=-1)
                output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())
                # output= model(data_1D.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break

    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")

    return model

"""## Model"""

class Encoder_GRU(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder_GRU, self).__init__()
        self.lstm_1 = nn.GRU(input_dim, 512, bidirectional=True, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.GRU(1024, 256, bidirectional=True, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.dropout_1=nn.Dropout(dropout)
        self.dropout_2=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout_1(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout_2(out_2)
        out_2=self.flatten(out_2)


        return out_2

class Encoder_CNN_1D(nn.Module):
    def __init__(self, input_size, dropout, hidden_dim=256, output_size=512, kernel_size=3, stride=1, padding=1):
        super().__init__()
        self.conv1 = nn.Conv1d(input_size, hidden_dim, kernel_size, stride, padding)
        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size, stride, padding)
        self.conv3 = nn.Conv1d(hidden_dim, output_size, kernel_size, stride, padding)
        self.conv4 = nn.Conv1d(output_size, output_size, kernel_size, stride, padding)
        self.BN_1= nn.BatchNorm1d(hidden_dim)
        self.BN_2= nn.BatchNorm1d(hidden_dim)
        self.BN_3= nn.BatchNorm1d(output_size)
        self.BN_4= nn.BatchNorm1d(output_size)
        self.pool = nn.MaxPool1d(kernel_size=2)

        # Fully connected layers
        self.fc1 = nn.Linear(512, 64)
        self.dropout1 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(dropout)
        self.flatten=nn.Flatten()

    def forward(self, x):
        x = x.transpose(1, 2)  # reshape from (batch_size, seq_len, input_size) to (batch_size, input_size, seq_len)
        x = F.relu(self.conv1(x))
        x = self.BN_1(x)
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.BN_2(x)
        x = self.pool(x)
        x = F.relu(self.conv3(x))
        x = self.BN_3(x)
        x = self.pool(x)
        x = F.relu(self.conv4(x))
        x = self.BN_4(x)
        x = self.pool(x)

        x = x.transpose(1, 2)  # reshape back to (batch_size, seq_len, output_size)

        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.flatten(x)

        return x

class Encoder_CNN_2D(nn.Module):
    def __init__(self, input_size, dropout, hidden_dim=256, output_size=512, kernel_size=(3,5), stride=(1,1), padding=(1,2)):
        super().__init__()
        self.conv1 = nn.Conv2d(input_size, hidden_dim, kernel_size, stride, padding)
        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, padding)
        self.conv3 = nn.Conv2d(hidden_dim, output_size, kernel_size, stride, padding)
        self.conv4 = nn.Conv2d(output_size, output_size, kernel_size, stride, padding)
        self.BN_1= nn.BatchNorm2d(hidden_dim)
        self.BN_2= nn.BatchNorm2d(hidden_dim)
        self.BN_3= nn.BatchNorm2d(output_size)
        self.BN_4= nn.BatchNorm2d(output_size)
        self.pool_1 = nn.MaxPool2d(kernel_size=(2,2))
        self.pool_2 = nn.MaxPool2d(kernel_size=(1,2))

        # Fully connected layers
        self.fc1 = nn.Linear(512, 64)
        self.dropout1 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(dropout)
        self.flatten=nn.Flatten()

    def forward(self, x):
        x = x.transpose(1, 3)  # reshape from (batch_size, seq_len, input_size) to (batch_size, input_size, seq_len)
        x = F.relu(self.conv1(x))
        x = self.BN_1(x)
        x = self.pool_1(x)
        x = F.relu(self.conv2(x))
        x = self.BN_2(x)
        x = self.pool_1(x)
        x = F.relu(self.conv3(x))
        x = self.BN_3(x)
        x = self.pool_2(x)
        x = F.relu(self.conv4(x))
        x = self.BN_4(x)
        x = self.pool_2(x)


        x = x.transpose(1, 3)  # reshape back to (batch_size, seq_len, output_size)

        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.flatten(x)


        return x

class RegularizedLinear(nn.Module):
    def __init__(self, in_features, out_features, weight_decay=0.001):
        super(RegularizedLinear, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.weight_decay = weight_decay

    def forward(self, input):
        return self.linear(input)

    def regularizer_loss(self):
        return self.weight_decay * torch.sum(self.linear.bias**2)

class Kinetics_FM_Net(nn.Module):
    def __init__(self, input_1D,input_2D):
        super(Kinetics_FM_Net, self).__init__()
        self.w = w
        self.BN= nn.BatchNorm1d(input_1D, affine=False)
        self.BN_2= nn.BatchNorm2d(input_2D, affine=False)

        # 1D Models

        self.model_1=Encoder_GRU(input_1D,0.30)
        self.model_2=Encoder_GRU(input_1D,0.30)
        self.model_3=Encoder_GRU(input_1D,0.30)

        self.cnn_1D = Encoder_CNN_1D(input_1D,0.25)
        self.cnn_2D = Encoder_CNN_2D(input_2D,0.25)

        # # Outputs
        # self.output_GRU = nn.Linear(w*512, 5*w)
        # self.output_C2 = nn.Linear(3*32+w*512, 5*w)
        # self.output_C1 = nn.Linear(3*32+w*512, 5*w)

                # Outputs
        self.output_GRU = RegularizedLinear(w*512, 5*w)
        self.output_C2 = RegularizedLinear(3*32+w*512, 5*w)
        self.output_C1 = RegularizedLinear(3*32+w*512, 5*w)

        self.l1=nn.Linear(5,128)
        self.l2=nn.Linear(128,5)

    def forward(self, inputs_1D_N, inputs_2D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 50, inputs_1D_N_1.size(-1))

        inputs_2D_N=inputs_2D_N.transpose(1,3)
        inputs_2D_N=self.BN_2(inputs_2D_N)
        inputs_2D_N=inputs_2D_N.transpose(1,3)


        model_1_output = self.model_1(inputs_1D_N)
        model_2_output = self.model_2(inputs_1D_N)
        model_3_output = self.model_3(inputs_1D_N)

        cnn_output_1D = self.cnn_1D(inputs_1D_N)
        cnn_output_2D = self.cnn_2D(inputs_2D_N)


        model_2_output = torch.cat([model_2_output, cnn_output_1D], dim=-1)
        model_3_output = torch.cat([model_3_output, cnn_output_2D], dim=-1)


        output_GRU = self.output_GRU(model_1_output)
        output_GRU=output_GRU.view(-1,w,5)

        output_C1 = self.output_C1(model_2_output)
        output_C1=output_C1.view(-1,w,5)

        output_C2 = self.output_C2(model_3_output)
        output_C2=output_C2.view(-1,w,5)


        output_GRU_1=F.relu(self.l1(output_GRU))
        output_GRU_1=F.sigmoid(self.l2(output_GRU_1))
        output_GRU_2=output_GRU*output_GRU_1

        output_C1_1=F.relu(self.l1(output_C1))
        output_C1_1=F.sigmoid(self.l2(output_C1_1))
        output_C1_2=output_C1*output_C1_1

        output_C2_1=F.relu(self.l1(output_C2))
        output_C2_1=F.sigmoid(self.l2(output_C2_1))
        output_C2_2=output_C2*output_C2_1


        output = output_GRU_2+output_C1_2+output_C2_2
        # output = output_GRU_2

        # output = (output_GRU+output_C1+output_C2)/3

        return output_GRU, output_C1, output_C2, output
        # return output_GRU

import gc
gc.collect()
gc.collect()
gc.collect()

torch.cuda.empty_cache()

lr = 0.001
model = Kinetics_FM_Net(12,2)

sota_1 = train_sota(train_loader, lr,40,model,path+'sota_1_imu2.pth')

sota_1= Kinetics_FM_Net(12,2)
sota_1.load_state_dict(torch.load(path+'sota_1_imu2.pth'))
sota_1.to(device)

sota_1.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(test_loader):
        data_1D=torch.cat((data_1D[:,:,12:18],data_1D[:,:,30:36]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,2:3],data_2D[:,:,:,5:6]),dim=-1)
        output_1,output_2,output_3,output= sota_1(data_1D.to(device).float(),data_2D.to(device).float())

        # # Concatenate predictions and targets
        if i == 0:
            yhat_5 = output
            test_target = target
        else:
            yhat_5 = torch.cat((yhat_5, output), dim=0)
            test_target = torch.cat((test_target, target), dim=0)

yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)


rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)

ablation_1=np.hstack([rmse,p])

class Kinetics_FM_Net_pcc(nn.Module):
    def __init__(self, input_1D,input_2D):
        super(Kinetics_FM_Net_pcc, self).__init__()
        self.w = w
        self.BN= nn.BatchNorm1d(input_1D, affine=False)
        self.BN_2= nn.BatchNorm2d(input_2D, affine=False)

        # 1D Models

        self.model_1=Encoder_GRU(input_1D,0.25)
        self.model_2=Encoder_GRU(input_1D,0.30)
        self.model_3=Encoder_GRU(input_1D,0.30)

        self.cnn_1D = Encoder_CNN_1D(input_1D,0.25)
        self.cnn_2D = Encoder_CNN_2D(input_2D,0.25)

        # Outputs
        self.output_GRU = RegularizedLinear(w*512, 5*w)
        self.output_C2 = RegularizedLinear(3*32+w*512, 5*w)
        self.output_C1 = RegularizedLinear(3*32+w*512, 5*w)

        self.l1=nn.Linear(5,128)
        self.l2=nn.Linear(128,5)

        self.dropout = nn.Dropout(0.4)

    def forward(self, inputs_1D_N, inputs_2D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 50, inputs_1D_N_1.size(-1))

        inputs_2D_N=inputs_2D_N.transpose(1,3)
        inputs_2D_N=self.BN_2(inputs_2D_N)
        inputs_2D_N=inputs_2D_N.transpose(1,3)


        model_1_output = self.model_1(inputs_1D_N)
        model_2_output = self.model_2(inputs_1D_N)
        model_3_output = self.model_3(inputs_1D_N)

        cnn_output_1D = self.cnn_1D(inputs_1D_N)
        cnn_output_2D = self.cnn_2D(inputs_2D_N)


        model_2_output = torch.cat([model_2_output, cnn_output_1D], dim=-1)
        model_3_output = torch.cat([model_3_output, cnn_output_2D], dim=-1)


        output_GRU = self.output_GRU(model_1_output)
        output_GRU=output_GRU.view(-1,w,5)

        output_C1 = self.output_C1(model_2_output)
        output_C1=output_C1.view(-1,w,5)

        output_C2 = self.output_C2(model_3_output)
        output_C2=output_C2.view(-1,w,5)


        output_GRU_1=F.relu(self.l1(output_GRU))
        output_GRU_1=self.dropout(output_GRU_1)
        output_GRU_1=F.sigmoid(self.l2(output_GRU_1))
        output_GRU_2=output_GRU*output_GRU_1

        output_C1_1=F.relu(self.l1(output_C1))
        output_C1_1=self.dropout(output_C1_1)
        output_C1_1=F.sigmoid(self.l2(output_C1_1))
        output_C1_2=output_C1*output_C1_1

        output_C2_1=F.relu(self.l1(output_C2))
        output_C2_1=self.dropout(output_C2_1)
        output_C2_1=F.sigmoid(self.l2(output_C2_1))
        output_C2_2=output_C2*output_C2_1


        output = output_GRU_2+output_C1_2+output_C2_2

        return output_GRU, output_C1, output_C2, output

torch.cuda.empty_cache()

lr = 0.001
model = Kinetics_FM_Net_pcc(12,2)

sota_1_pcc = train_sota_pcc(train_loader, lr,55,model,path+'sota_1_pcc_imu2.pth')

sota_1_pcc= Kinetics_FM_Net_pcc(12,2)
sota_1_pcc.load_state_dict(torch.load(path+'sota_1_pcc_imu2.pth'))
sota_1_pcc.to(device)

sota_1_pcc.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(test_loader):
        data_1D=torch.cat((data_1D[:,:,12:18],data_1D[:,:,30:36]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,2:3],data_2D[:,:,:,5:6]),dim=-1)
        output_1,output_2,output_3,output= sota_1_pcc(data_1D.to(device).float(),data_2D.to(device).float())
        # output= sota_1(data_1D.to(device).float(),data_2D.to(device).float())

        # # Concatenate predictions and targets
        if i == 0:
            yhat_5 = output
            test_target = target
        else:
            yhat_5 = torch.cat((yhat_5, output), dim=0)
            test_target = torch.cat((test_target, target), dim=0)


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)


rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_target,s)
rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)


ablation_2=np.hstack([rmse,p])

"""# Kinetics-FM-DLR-Net-- 2 IMUs--Feet+Pelvis

## Training Function
"""

class PearsonCorrLoss(nn.Module):

  def __init__(self):
    super(PearsonCorrLoss, self).__init__()

  def forward(self, y_true, y_pred):

    # Calculate mean values
    mx = torch.mean(y_true)
    my = torch.mean(y_pred)

    # Calculate differences from mean
    xm, ym = y_true - mx, y_pred - my

    # Calculate numerator and denominator of Pearson correlation coefficient
    r_num = torch.sum(torch.mul(xm, ym))
    r_den = torch.sqrt(torch.mul(torch.sum(torch.square(xm)), torch.sum(torch.square(ym))))

    # Calculate Pearson correlation coefficient
    r = r_num / r_den

    # Clamp r between 0 and 1
    r = torch.clamp(r, 0, 1.0)

    # Calculate l2 loss
    l2 = 1 - torch.square(r)



    return l2

def train_sota(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    criterion =RMSELoss()
    # criterion =correlation_coefficient_loss_joint_pytorch()

    # criterion=PearsonCorrLoss()
    optimizer= torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(train_loader):

            optimizer.zero_grad()
            data_1D=torch.cat((data_1D[:,:,6:18],data_1D[:,:,30:36]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,1:3],data_2D[:,:,:,5:6]),dim=-1)
            output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())

                    # Compute the regularization loss for the custom linear layers
            regularization_loss = 0.0
            if hasattr(model.output_GRU, 'regularizer_loss'):
                regularization_loss += model.output_GRU.regularizer_loss()
            if hasattr(model.output_C1, 'regularizer_loss'):
                regularization_loss += model.output_C1.regularizer_loss()
            if hasattr(model.output_C2, 'regularizer_loss'):
                regularization_loss += model.output_C2.regularizer_loss()

            # output= model(data_1D.to(device).float(),data_2D.to(device).float())

            loss=criterion(output_1, target.to(device).float())+criterion(output_2, target.to(device).float())+criterion(output_3, target.to(device).float())\
            +criterion(output, target.to(device).float())+regularization_loss
            loss_1=criterion(output, target.to(device).float())

            loss.backward()
            optimizer.step()

            running_loss += loss_1.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target in val_loader:
                data_1D=torch.cat((data_1D[:,:,6:18],data_1D[:,:,30:36]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,1:3],data_2D[:,:,:,5:6]),dim=-1)
                output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())
                # output= model(data_1D.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break

    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")

    return model

def train_sota_pcc(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    # criterion =RMSELoss()
    # criterion =correlation_coefficient_loss_joint_pytorch()

    criterion=PearsonCorrLoss()
    optimizer= torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(train_loader):

            optimizer.zero_grad()
            data_1D=torch.cat((data_1D[:,:,6:18],data_1D[:,:,30:36]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,1:3],data_2D[:,:,:,5:6]),dim=-1)
            output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())

                    # Compute the regularization loss for the custom linear layers
            regularization_loss = 0.0
            if hasattr(model.output_GRU, 'regularizer_loss'):
                regularization_loss += model.output_GRU.regularizer_loss()
            if hasattr(model.output_C1, 'regularizer_loss'):
                regularization_loss += model.output_C1.regularizer_loss()
            if hasattr(model.output_C2, 'regularizer_loss'):
                regularization_loss += model.output_C2.regularizer_loss()

            # output= model(data_1D.to(device).float(),data_2D.to(device).float())

            loss=criterion(output_1, target.to(device).float())+criterion(output_2, target.to(device).float())+criterion(output_3, target.to(device).float())\
            +criterion(output, target.to(device).float())+regularization_loss
            loss_1=criterion(output, target.to(device).float())


            loss.backward()
            optimizer.step()

            running_loss += loss_1.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target in val_loader:
                data_1D=torch.cat((data_1D[:,:,6:18],data_1D[:,:,30:36]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,1:3],data_2D[:,:,:,5:6]),dim=-1)
                output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())
                # output= model(data_1D.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break

    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")

    return model

"""## Model"""

import gc
gc.collect()
gc.collect()
gc.collect()

torch.cuda.empty_cache()

lr = 0.001
model = Kinetics_FM_Net(18,3)

sota_1 = train_sota(train_loader, lr,40,model,path+'sota_1_imu3.pth')

sota_1= Kinetics_FM_Net(18,3)
sota_1.load_state_dict(torch.load(path+'sota_1_imu3.pth'))
sota_1.to(device)

sota_1.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(test_loader):
        data_1D=torch.cat((data_1D[:,:,6:18],data_1D[:,:,30:36]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,1:3],data_2D[:,:,:,5:6]),dim=-1)
        output_1,output_2,output_3,output= sota_1(data_1D.to(device).float(),data_2D.to(device).float())

        # # Concatenate predictions and targets
        if i == 0:
            yhat_5 = output
            test_target = target
        else:
            yhat_5 = torch.cat((yhat_5, output), dim=0)
            test_target = torch.cat((test_target, target), dim=0)

yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)


rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)

ablation_3=np.hstack([rmse,p])

torch.cuda.empty_cache()

lr = 0.001
model = Kinetics_FM_Net_pcc(18,3)

sota_1_pcc = train_sota_pcc(train_loader, lr,55,model,path+'sota_1_pcc_imu3.pth')

sota_1_pcc= Kinetics_FM_Net_pcc(18,3)
sota_1_pcc.load_state_dict(torch.load(path+'sota_1_pcc_imu3.pth'))
sota_1_pcc.to(device)

sota_1_pcc.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(test_loader):
        data_1D=torch.cat((data_1D[:,:,6:18],data_1D[:,:,30:36]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,1:3],data_2D[:,:,:,5:6]),dim=-1)
        output_1,output_2,output_3,output= sota_1_pcc(data_1D.to(device).float(),data_2D.to(device).float())
        # output= sota_1(data_1D.to(device).float(),data_2D.to(device).float())

        # # Concatenate predictions and targets
        if i == 0:
            yhat_5 = output
            test_target = target
        else:
            yhat_5 = torch.cat((yhat_5, output), dim=0)
            test_target = torch.cat((test_target, target), dim=0)


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)


rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_target,s)
rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)


ablation_4=np.hstack([rmse,p])

"""# Kinetics-FM-DLR-Net-- 2 IMUs--Feet+Pelvis+Shanks

## Training Function
"""

class PearsonCorrLoss(nn.Module):

  def __init__(self):
    super(PearsonCorrLoss, self).__init__()

  def forward(self, y_true, y_pred):

    # Calculate mean values
    mx = torch.mean(y_true)
    my = torch.mean(y_pred)

    # Calculate differences from mean
    xm, ym = y_true - mx, y_pred - my

    # Calculate numerator and denominator of Pearson correlation coefficient
    r_num = torch.sum(torch.mul(xm, ym))
    r_den = torch.sqrt(torch.mul(torch.sum(torch.square(xm)), torch.sum(torch.square(ym))))

    # Calculate Pearson correlation coefficient
    r = r_num / r_den

    # Clamp r between 0 and 1
    r = torch.clamp(r, 0, 1.0)

    # Calculate l2 loss
    l2 = 1 - torch.square(r)



    return l2

def train_sota(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    criterion =RMSELoss()
    # criterion =correlation_coefficient_loss_joint_pytorch()

    # criterion=PearsonCorrLoss()
    optimizer= torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(train_loader):

            optimizer.zero_grad()
            data_1D=torch.cat((data_1D[:,:,6:24],data_1D[:,:,30:42]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,1:4],data_2D[:,:,:,5:7]),dim=-1)
            output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())

                    # Compute the regularization loss for the custom linear layers
            regularization_loss = 0.0
            if hasattr(model.output_GRU, 'regularizer_loss'):
                regularization_loss += model.output_GRU.regularizer_loss()
            if hasattr(model.output_C1, 'regularizer_loss'):
                regularization_loss += model.output_C1.regularizer_loss()
            if hasattr(model.output_C2, 'regularizer_loss'):
                regularization_loss += model.output_C2.regularizer_loss()

            # output= model(data_1D.to(device).float(),data_2D.to(device).float())

            loss=criterion(output_1, target.to(device).float())+criterion(output_2, target.to(device).float())+criterion(output_3, target.to(device).float())\
            +criterion(output, target.to(device).float())+regularization_loss
            loss_1=criterion(output, target.to(device).float())

            loss.backward()
            optimizer.step()

            running_loss += loss_1.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target in val_loader:
                data_1D=torch.cat((data_1D[:,:,6:24],data_1D[:,:,30:42]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,1:4],data_2D[:,:,:,5:7]),dim=-1)
                output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())
                # output= model(data_1D.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break

    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")

    return model

def train_sota_pcc(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    # criterion =RMSELoss()
    # criterion =correlation_coefficient_loss_joint_pytorch()

    criterion=PearsonCorrLoss()
    optimizer= torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(train_loader):

            optimizer.zero_grad()
            data_1D=torch.cat((data_1D[:,:,6:24],data_1D[:,:,30:42]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,1:4],data_2D[:,:,:,5:7]),dim=-1)
            output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())

                    # Compute the regularization loss for the custom linear layers
            regularization_loss = 0.0
            if hasattr(model.output_GRU, 'regularizer_loss'):
                regularization_loss += model.output_GRU.regularizer_loss()
            if hasattr(model.output_C1, 'regularizer_loss'):
                regularization_loss += model.output_C1.regularizer_loss()
            if hasattr(model.output_C2, 'regularizer_loss'):
                regularization_loss += model.output_C2.regularizer_loss()

            # output= model(data_1D.to(device).float(),data_2D.to(device).float())

            loss=criterion(output_1, target.to(device).float())+criterion(output_2, target.to(device).float())+criterion(output_3, target.to(device).float())\
            +criterion(output, target.to(device).float())+regularization_loss
            loss_1=criterion(output, target.to(device).float())


            loss.backward()
            optimizer.step()

            running_loss += loss_1.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target in val_loader:
                data_1D=torch.cat((data_1D[:,:,6:24],data_1D[:,:,30:42]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,1:4],data_2D[:,:,:,5:7]),dim=-1)
                output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())
                # output= model(data_1D.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break

    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")

    return model

"""## Model"""

import gc
gc.collect()
gc.collect()
gc.collect()

torch.cuda.empty_cache()

lr = 0.001
model = Kinetics_FM_Net(30,5)

sota_1 = train_sota(train_loader, lr,40,model,path+'sota_1_imu5.pth')

sota_1= Kinetics_FM_Net(30,5)
sota_1.load_state_dict(torch.load(path+'sota_1_imu5.pth'))
sota_1.to(device)

sota_1.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(test_loader):
        data_1D=torch.cat((data_1D[:,:,6:24],data_1D[:,:,30:42]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,1:4],data_2D[:,:,:,5:7]),dim=-1)
        output_1,output_2,output_3,output= sota_1(data_1D.to(device).float(),data_2D.to(device).float())

        # # Concatenate predictions and targets
        if i == 0:
            yhat_5 = output
            test_target = target
        else:
            yhat_5 = torch.cat((yhat_5, output), dim=0)
            test_target = torch.cat((test_target, target), dim=0)

yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)


rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)

ablation_5=np.hstack([rmse,p])

torch.cuda.empty_cache()

lr = 0.001
model = Kinetics_FM_Net_pcc(30,5)

sota_1_pcc = train_sota_pcc(train_loader, lr,55,model,path+'sota_1_pcc_imu5.pth')

sota_1_pcc= Kinetics_FM_Net_pcc(30,5)
sota_1_pcc.load_state_dict(torch.load(path+'sota_1_pcc_imu5.pth'))
sota_1_pcc.to(device)

sota_1_pcc.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(test_loader):
        data_1D=torch.cat((data_1D[:,:,6:24],data_1D[:,:,30:42]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,1:4],data_2D[:,:,:,5:7]),dim=-1)
        output_1,output_2,output_3,output= sota_1_pcc(data_1D.to(device).float(),data_2D.to(device).float())
        # output= sota_1(data_1D.to(device).float(),data_2D.to(device).float())

        # # Concatenate predictions and targets
        if i == 0:
            yhat_5 = output
            test_target = target
        else:
            yhat_5 = torch.cat((yhat_5, output), dim=0)
            test_target = torch.cat((test_target, target), dim=0)


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)


rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_target,s)
rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)


ablation_6=np.hstack([rmse,p])

"""# Kinetics-FM-DLR-Net-- 2 IMUs--Feet+Pelvis+Shanks+Thighs

## Training Function
"""

class PearsonCorrLoss(nn.Module):

  def __init__(self):
    super(PearsonCorrLoss, self).__init__()

  def forward(self, y_true, y_pred):

    # Calculate mean values
    mx = torch.mean(y_true)
    my = torch.mean(y_pred)

    # Calculate differences from mean
    xm, ym = y_true - mx, y_pred - my

    # Calculate numerator and denominator of Pearson correlation coefficient
    r_num = torch.sum(torch.mul(xm, ym))
    r_den = torch.sqrt(torch.mul(torch.sum(torch.square(xm)), torch.sum(torch.square(ym))))

    # Calculate Pearson correlation coefficient
    r = r_num / r_den

    # Clamp r between 0 and 1
    r = torch.clamp(r, 0, 1.0)

    # Calculate l2 loss
    l2 = 1 - torch.square(r)



    return l2

def train_sota(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    criterion =RMSELoss()
    # criterion =correlation_coefficient_loss_joint_pytorch()

    # criterion=PearsonCorrLoss()
    optimizer= torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(train_loader):

            optimizer.zero_grad()
            data_1D=torch.cat((data_1D[:,:,6:30],data_1D[:,:,30:48]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,1:5],data_2D[:,:,:,5:8]),dim=-1)
            output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())

                    # Compute the regularization loss for the custom linear layers
            regularization_loss = 0.0
            if hasattr(model.output_GRU, 'regularizer_loss'):
                regularization_loss += model.output_GRU.regularizer_loss()
            if hasattr(model.output_C1, 'regularizer_loss'):
                regularization_loss += model.output_C1.regularizer_loss()
            if hasattr(model.output_C2, 'regularizer_loss'):
                regularization_loss += model.output_C2.regularizer_loss()

            # output= model(data_1D.to(device).float(),data_2D.to(device).float())

            loss=criterion(output_1, target.to(device).float())+criterion(output_2, target.to(device).float())+criterion(output_3, target.to(device).float())\
            +criterion(output, target.to(device).float())+regularization_loss
            loss_1=criterion(output, target.to(device).float())

            loss.backward()
            optimizer.step()

            running_loss += loss_1.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target in val_loader:
                data_1D=torch.cat((data_1D[:,:,6:30],data_1D[:,:,30:48]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,1:5],data_2D[:,:,:,5:8]),dim=-1)
                output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())
                # output= model(data_1D.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break

    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")

    return model

def train_sota_pcc(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    # criterion =RMSELoss()
    # criterion =correlation_coefficient_loss_joint_pytorch()

    criterion=PearsonCorrLoss()
    optimizer= torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(train_loader):

            optimizer.zero_grad()
            data_1D=torch.cat((data_1D[:,:,6:30],data_1D[:,:,30:48]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,1:5],data_2D[:,:,:,5:8]),dim=-1)
            output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())

                    # Compute the regularization loss for the custom linear layers
            regularization_loss = 0.0
            if hasattr(model.output_GRU, 'regularizer_loss'):
                regularization_loss += model.output_GRU.regularizer_loss()
            if hasattr(model.output_C1, 'regularizer_loss'):
                regularization_loss += model.output_C1.regularizer_loss()
            if hasattr(model.output_C2, 'regularizer_loss'):
                regularization_loss += model.output_C2.regularizer_loss()

            # output= model(data_1D.to(device).float(),data_2D.to(device).float())

            loss=criterion(output_1, target.to(device).float())+criterion(output_2, target.to(device).float())+criterion(output_3, target.to(device).float())\
            +criterion(output, target.to(device).float())+regularization_loss
            loss_1=criterion(output, target.to(device).float())


            loss.backward()
            optimizer.step()

            running_loss += loss_1.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target in val_loader:
                data_1D=torch.cat((data_1D[:,:,6:30],data_1D[:,:,30:48]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,1:5],data_2D[:,:,:,5:8]),dim=-1)
                output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())
                # output= model(data_1D.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break

    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")

    return model

"""## Model"""

import gc
gc.collect()
gc.collect()
gc.collect()

torch.cuda.empty_cache()

lr = 0.001
model = Kinetics_FM_Net(42,7)

sota_1 = train_sota(train_loader, lr,40,model,path+'sota_1_imu7.pth')

sota_1= Kinetics_FM_Net(42,7)
sota_1.load_state_dict(torch.load(path+'sota_1_imu7.pth'))
sota_1.to(device)

sota_1.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(test_loader):
        data_1D=torch.cat((data_1D[:,:,6:30],data_1D[:,:,30:48]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,1:5],data_2D[:,:,:,5:8]),dim=-1)
        output_1,output_2,output_3,output= sota_1(data_1D.to(device).float(),data_2D.to(device).float())

        # # Concatenate predictions and targets
        if i == 0:
            yhat_5 = output
            test_target = target
        else:
            yhat_5 = torch.cat((yhat_5, output), dim=0)
            test_target = torch.cat((test_target, target), dim=0)

yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)


rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)

ablation_7=np.hstack([rmse,p])

torch.cuda.empty_cache()

lr = 0.001
model = Kinetics_FM_Net_pcc(42,7)

sota_1_pcc = train_sota_pcc(train_loader, lr,55,model,path+'sota_1_pcc_imu7.pth')

sota_1_pcc= Kinetics_FM_Net_pcc(42,7)
sota_1_pcc.load_state_dict(torch.load(path+'sota_1_pcc_imu7.pth'))
sota_1_pcc.to(device)

sota_1_pcc.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(test_loader):
        data_1D=torch.cat((data_1D[:,:,6:30],data_1D[:,:,30:48]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,1:5],data_2D[:,:,:,5:8]),dim=-1)
        output_1,output_2,output_3,output= sota_1_pcc(data_1D.to(device).float(),data_2D.to(device).float())
        # output= sota_1(data_1D.to(device).float(),data_2D.to(device).float())

        # # Concatenate predictions and targets
        if i == 0:
            yhat_5 = output
            test_target = target
        else:
            yhat_5 = torch.cat((yhat_5, output), dim=0)
            test_target = torch.cat((test_target, target), dim=0)


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)


rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_target,s)
rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)


ablation_8=np.hstack([rmse,p])

"""# Kinetics-FM-DLR-Net-- 8 IMUs

## Training Function
"""

class PearsonCorrLoss(nn.Module):

  def __init__(self):
    super(PearsonCorrLoss, self).__init__()

  def forward(self, y_true, y_pred):

    # Calculate mean values
    mx = torch.mean(y_true)
    my = torch.mean(y_pred)

    # Calculate differences from mean
    xm, ym = y_true - mx, y_pred - my

    # Calculate numerator and denominator of Pearson correlation coefficient
    r_num = torch.sum(torch.mul(xm, ym))
    r_den = torch.sqrt(torch.mul(torch.sum(torch.square(xm)), torch.sum(torch.square(ym))))

    # Calculate Pearson correlation coefficient
    r = r_num / r_den

    # Clamp r between -1 and 1
    r = torch.clamp(r, 0, 1.0)

    # Calculate l2 loss
    l2 = 1 - torch.square(r)



    return l2

def train_sota(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    criterion =RMSELoss()
    # criterion =correlation_coefficient_loss_joint_pytorch()

    # criterion=PearsonCorrLoss()
    optimizer= torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(train_loader):

            optimizer.zero_grad()
            output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())

                    # Compute the regularization loss for the custom linear layers
            regularization_loss = 0.0
            if hasattr(model.output_GRU, 'regularizer_loss'):
                regularization_loss += model.output_GRU.regularizer_loss()
            if hasattr(model.output_C1, 'regularizer_loss'):
                regularization_loss += model.output_C1.regularizer_loss()
            if hasattr(model.output_C2, 'regularizer_loss'):
                regularization_loss += model.output_C2.regularizer_loss()

            # output= model(data_1D.to(device).float(),data_2D.to(device).float())

            loss=criterion(output_1, target.to(device).float())+criterion(output_2, target.to(device).float())+criterion(output_3, target.to(device).float())\
            +criterion(output, target.to(device).float())+regularization_loss
            loss_1=criterion(output, target.to(device).float())

            loss.backward()
            optimizer.step()

            running_loss += loss_1.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target in val_loader:
                output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())
                # output= model(data_1D.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break

    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")

    return model

def train_sota_pcc(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    # criterion =RMSELoss()
    # criterion =correlation_coefficient_loss_joint_pytorch()

    criterion=PearsonCorrLoss()
    optimizer= torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(train_loader):

            optimizer.zero_grad()
            output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())

                    # Compute the regularization loss for the custom linear layers
            regularization_loss = 0.0
            if hasattr(model.output_GRU, 'regularizer_loss'):
                regularization_loss += model.output_GRU.regularizer_loss()
            if hasattr(model.output_C1, 'regularizer_loss'):
                regularization_loss += model.output_C1.regularizer_loss()
            if hasattr(model.output_C2, 'regularizer_loss'):
                regularization_loss += model.output_C2.regularizer_loss()

            # output= model(data_1D.to(device).float(),data_2D.to(device).float())

            loss=criterion(output_1, target.to(device).float())+criterion(output_2, target.to(device).float())+criterion(output_3, target.to(device).float())\
            +criterion(output, target.to(device).float())+regularization_loss
            loss_1=criterion(output, target.to(device).float())


            loss.backward()
            optimizer.step()

            running_loss += loss_1.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target in val_loader:
                output_1, output_2, output_3, output= model(data_1D.to(device).float(),data_2D.to(device).float())
                # output= model(data_1D.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break

    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")

    return model

"""## Model"""

import gc
gc.collect()
gc.collect()
gc.collect()

torch.cuda.empty_cache()

lr = 0.001
model = Kinetics_FM_Net(48,8)

sota_1 = train_sota(train_loader, lr,40,model,path+'sota_1_imu8.pth')

sota_1= Kinetics_FM_Net(48,8)
sota_1.load_state_dict(torch.load(path+'sota_1_imu8.pth'))
sota_1.to(device)

sota_1.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(test_loader):
        output_1,output_2,output_3,output= sota_1(data_1D.to(device).float(),data_2D.to(device).float())

        # # Concatenate predictions and targets
        if i == 0:
            yhat_5 = output
            test_target = target
        else:
            yhat_5 = torch.cat((yhat_5, output), dim=0)
            test_target = torch.cat((test_target, target), dim=0)

yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)


rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)

ablation_9=np.hstack([rmse,p])

torch.cuda.empty_cache()

lr = 0.001
model = Kinetics_FM_Net_pcc(48,8)

sota_1_pcc = train_sota_pcc(train_loader, lr,55,model,path+'sota_1_pcc_imu8.pth')

sota_1_pcc= Kinetics_FM_Net_pcc(48,8)
sota_1_pcc.load_state_dict(torch.load(path+'sota_1_pcc_imu8.pth'))
sota_1_pcc.to(device)

sota_1_pcc.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_2D_joint,  target) in enumerate(test_loader):
        output_1,output_2,output_3,output= sota_1_pcc(data_1D.to(device).float(),data_2D.to(device).float())
        # output= sota_1(data_1D.to(device).float(),data_2D.to(device).float())

        # # Concatenate predictions and targets
        if i == 0:
            yhat_5 = output
            test_target = target
        else:
            yhat_5 = torch.cat((yhat_5, output), dim=0)
            test_target = torch.cat((test_target, target), dim=0)


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)


rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_target,s)
rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)


ablation_10=np.hstack([rmse,p])

"""# Keras-Tensorflow- Kinetics-FM-DLR-Net"""

# Let's import all packages that we may need:
import numpy
import tensorflow as tf
import statistics
from numpy import loadtxt
import matplotlib.pyplot as plt
import pandas
import math
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import GRU,LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statistics import stdev
import math
import h5py

import numpy as np

from scipy.signal import butter,filtfilt
from tensorflow.keras.callbacks import EarlyStopping

import sys
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph
import seaborn as sns # used for plot interactive graph.
import pandas
import matplotlib.pyplot as plt

## for Deep-learing:
import tensorflow.keras

from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
to_categorical([0, 1, 2, 3], num_classes=4)
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.callbacks import EarlyStopping
# from tensorflow.keras.utils import np_utils
import itertools
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import TimeDistributed
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Bidirectional
#import constraint

from sklearn.model_selection import train_test_split
from tensorflow.keras.regularizers import l2


###  Library for attention layers

import pandas as pd
#import pyarrow.parquet as pq # Used to read the data
import os
import numpy as np
from tensorflow.keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes
from tensorflow.keras.models import Model
#from tqdm import tqdm # Processing time measurement
from sklearn.model_selection import train_test_split
from tensorflow.keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class
from tensorflow.keras import optimizers # Allow us to access the Adam class to modify some parameters
from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model
from tensorflow.keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting

from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K
from tensorflow.keras import initializers
from tensorflow.keras import regularizers
import statistics
import gc
from numpy import savetxt


import os

### Early stopping

from tensorflow.keras.callbacks import EarlyStopping



config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth=True
sess = tf.compat.v1.Session(config=config)

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

from sklearn.model_selection import KFold
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
import pickle
from sklearn.linear_model import Ridge
from sklearn.utils import resample

"""# Loss Function"""

from keras import backend as K
def correlation_coefficient_loss(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    #r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y_pred - y_true)))
    #l2=1-K.square(r)
    l2=1-r

    l=l2
    return l

from keras import backend as K
def correlation_coefficient_loss_1(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y_pred - y_true)))
    l2=1-K.square(r)

    l=l1
    return l

from keras import backend as K
def correlation_coefficient_loss_joint(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    #r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y_pred - y_true)))
    #l2=1-K.square(r)
    l2=1-r

    l=l1+l2
    return l


custom_early_stopping=tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=15,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=True
)

def Kinetics_FM_Net(inputs_1D_N,inputs_2D_N):
  num_pred=5

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)


  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])



  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)
  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)
  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)


  output_GRU_1=Dense(128,activation='relu')(output_GRU)
  output_GRU_1=Dense(num_pred,activation='sigmoid')(output_GRU_1)
  #output_GRU_1=Dense(1,activation='sigmoid')(output_GRU_1)
  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])

  output_C2_1=Dense(128,activation='relu')(output_C2)
  output_C2_1=Dense(num_pred,activation='sigmoid')(output_C2_1)
  #output_C2_1=Dense(1,activation='sigmoid')(output_C2_1)
  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])

  output_C1_1=Dense(128,activation='relu')(output_C1)
  output_C1_1=Dense(num_pred,activation='sigmoid')(output_C1_1)
  #output_C1_1=Dense(1,activation='sigmoid')(output_C1_1)
  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])


  weight=output_GRU_1+output_C2_1+output_C1_1

  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])
  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])
  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])


  output = output_GRU_2+output_C1_2+output_C2_2

  #output = Average()([output_GRU,output_C2,output_C1])


  return (output_C2,output_GRU,output_C1,output)

def Kinetics_FM_Net_pcc(inputs_1D_N,inputs_2D_N):

  num_pred=5

  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.25)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.25)(model_1)
  model_1=Flatten()(model_1)

  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.3)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.3)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  # X=Dropout(0.05)(X)
  X=Dense(32,activation='relu')(X)
  # X=Dropout(0.05)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])


  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.3)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.3)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  # CNN=Dropout(0.05)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  # CNN=Dropout(0.05)(CNN)
  CNN=Flatten()(CNN)

  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)
  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)
  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)


  output_GRU_1=Dense(128,activation='relu')(output_GRU)
  output_GRU_1=Dropout(0.4)(output_GRU_1)
  #output_GRU_1=Dense(64,activation='relu')(output_GRU_1)
  output_GRU_1=Dense(num_pred,activation='sigmoid')(output_GRU_1)
  #output_GRU_1=Dense(1,activation='sigmoid')(output_GRU_1)
  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])

  output_C2_1=Dense(128,activation='relu')(output_C2)
  output_C2_1=Dropout(0.4)(output_C2_1)
  #output_C2_1=Dense(64,activation='relu')(output_C2_1)
  output_C2_1=Dense(num_pred,activation='sigmoid')(output_C2_1)
  #output_C2_1=Dense(1,activation='sigmoid')(output_C2_1)
  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])

  output_C1_1=Dense(128,activation='relu')(output_C1)
  output_C1_1=Dropout(0.4)(output_C1_1)
  #output_C1_1=Dense(64,activation='relu')(output_C1_1)
  output_C1_1=Dense(num_pred,activation='sigmoid')(output_C1_1)
  #output_C1_1=Dense(1,activation='sigmoid')(output_C1_1)
  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])


  weight=output_GRU_1+output_C2_1+output_C1_1

  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])
  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])
  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])

  output = output_GRU_2+output_C1_2+output_C2_2

  return (output_C1,output_C2,output_GRU,output)

# """# Kinetics-FM-Net, Kinetics-FM-DLR-NET, Kinetics-FM-JL-Net"""

# ### Kinetics-Net ###

# w1=50

# inputs_1D = tf.keras.layers.Input( shape=(w1,48) )
# inputs_2D = tf.keras.layers.Input( shape=(w1,6,8) )


# inputs_1D_N=BatchNormalization()(inputs_1D)
# inputs_2D_N=BatchNormalization()(inputs_2D)


# output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)

# model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

# model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
#                                                                                           Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])



# model_1.save(path+'model_Kinetics_FM.h5')


# gc.collect()

"""# Kinetics-FM-Net, Kinetics-FM-DLR-NET, Kinetics-FM-JL-Net"""

### Kinetics-Net ###

w1=50

inputs_1D = tf.keras.layers.Input( shape=(w1,48) )
inputs_2D = tf.keras.layers.Input( shape=(w1,6,8) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)

model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=10, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])



model_1.save(path+'model_Kinetics_FM.h5')


gc.collect()

[yhat_1,yhat_2,yhat_3,yhat_4] = model_1.predict([test_X_1D,test_X_2D])

# yhat_4 = model_1.predict([test_X_1D,test_X_2D])



rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)


gc.collect()

RMSE_Kinetics_FM=rmse
PCC_Kinetics_FM=p

RMSE_Kinetics_FM=rmse
PCC_Kinetics_FM=p

inputs_1D = tf.keras.layers.Input( shape=(w,48) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,8) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output_1,output_2,output_3,output=Kinetics_FM_Net_pcc(inputs_1D_N,inputs_2D_N)
model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])


model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=10, batch_size=64, validation_data=([X_validation_1D,X_validation_2D],\
                                                                                        [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])



model_2.save(path+'model_Kinetics_FM_PCC.h5')


gc.collect()

# [yhat_1,yhat_2,yhat_3,yhat_4] = model_2.predict([test_X_1D,test_X_2D])

# print(test_y.shape)

# rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)
rmse,p=DLR_prediction(yhat_4,test_target,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)

print(Y_1)
RMSE_Kinetics_FM_DLR=rmse
PCC_Kinetics_FM_DLR=p