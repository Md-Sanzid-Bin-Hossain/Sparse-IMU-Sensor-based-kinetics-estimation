# -*- coding: utf-8 -*-
"""Kinetics_dataset_A_Multi_modal_Estimation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mRsnmTHkVCb_N3YwbZtMqtFjXB9l3EYK
"""



import h5py
import json
import matplotlib.pyplot as plt
import numpy as np
import numpy
import statistics 
from numpy import loadtxt
import matplotlib.pyplot as plt
import pandas
import math
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statistics import stdev 
import math
import h5py
 
import numpy as np
import time

from scipy.signal import butter,filtfilt
import sys 
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph 
import seaborn as sns # used for plot interactive graph. 
import pandas
import matplotlib.pyplot as plt

# from tsf.model import TransformerForecaster


# from tensorflow.keras.utils import np_utils
import itertools
###  Library for attention layers 
import pandas as pd
import os 
import numpy as np
#from tqdm import tqdm # Processing time measurement
from sklearn.model_selection import train_test_split 

import statistics
import gc
import torch.nn.init as init

############################################################################################################################################################################
############################################################################################################################################################################

import os
import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.nn.utils.weight_norm as weight_norm
from sklearn.preprocessing import StandardScaler


import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import torch.nn.functional as F
from torchsummary import summary
from torch.nn.parameter import Parameter


import torch.optim as optim
import gc

from tqdm import tqdm_notebook
from sklearn.preprocessing import MinMaxScaler


# Set random seeds for reproducibility
def set_seed(seed):
    random.seed(seed)  # Python random seed
    np.random.seed(seed)  # NumPy random seed
    torch.manual_seed(seed)  # PyTorch random seed
    torch.cuda.manual_seed(seed)  # PyTorch GPU random seed
    torch.cuda.manual_seed_all(seed)  # If using multi-GPU
    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior
    torch.backends.cudnn.benchmark = False  # Disable auto-tuner to find best algorithms

# Set your seed value
set_seed(42)


"""# File path

# Data loader
"""

def data_loader(subject):
  with h5py.File('/home/sanzidpr/Dataset A-Kinetics/All_subjects_data.h5', 'r') as hf:
    All_subjects = hf['All_subjects']
    Subject = All_subjects[subject]

    treadmill = Subject['Treadmill']
    levelground = Subject['Levelground']
    ramp = Subject['Ramp']
    stair = Subject['Stair']
    
    All_data=np.concatenate((treadmill,levelground,ramp,stair),axis=0)

    return np.array(All_data)

subject_7_data=data_loader('Subject_7')
gc.collect()
subject_8_data=data_loader('Subject_8')
gc.collect()
subject_9_data=data_loader('Subject_9')
gc.collect()
subject_10_data=data_loader('Subject_10')
gc.collect()
subject_11_data=data_loader('Subject_11')
gc.collect()
subject_12_data=data_loader('Subject_12')
gc.collect()
subject_13_data=data_loader('Subject_13')
gc.collect()
subject_14_data=data_loader('Subject_14')
gc.collect()
subject_15_data=data_loader('Subject_15')
gc.collect()
subject_16_data=data_loader('Subject_16')
gc.collect()
subject_17_data=data_loader('Subject_17')
gc.collect()
subject_18_data=data_loader('Subject_18')
gc.collect()
subject_19_data=data_loader('Subject_19')
gc.collect()
subject_21_data=data_loader('Subject_21')
gc.collect()
subject_23_data=data_loader('Subject_23')
gc.collect()
subject_24_data=data_loader('Subject_24')
gc.collect()
subject_25_data=data_loader('Subject_25')
gc.collect()
subject_27_data=data_loader('Subject_27')
gc.collect()
subject_28_data=data_loader('Subject_28')
gc.collect()
subject_30_data=data_loader('Subject_30')
gc.collect()

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

##############################################################################################################################################################################################################



"""# Data processing"""




main_dir = "/home/sanzidpr/Journal_3/Dataset_A_model_results_IMU_emg/Subject9"
os.mkdir(main_dir) 
path="/home/sanzidpr/Journal_3/Dataset_A_model_results_IMU_emg/Subject9/"
subject='Subject_9'

train_dataset=np.concatenate((subject_7_data,subject_8_data,subject_10_data,subject_11_data,subject_12_data,
                               subject_13_data,subject_14_data,subject_15_data,subject_16_data,subject_17_data,subject_18_data,
                               subject_19_data,subject_21_data,subject_23_data,subject_24_data,subject_25_data,subject_27_data,subject_28_data,subject_30_data),axis=0)


test_dataset=subject_9_data





##############################################################################################################################################################################################################

encoder='lstm_gru'




# Data processing



# Train features #



## IMUs-0:24
## IK-24:47
## ID-47:70
## GRF-70:79
## GON-79:84
## EMG-84:106
## JP-106:129


x_train_IMUs=train_dataset[:,0:24]
x_train_Kinematics=train_dataset[:,24:47]
x_train_kinetics=train_dataset[:,47:70]
x_train_GRF=train_dataset[:,70:79]
x_train_GON=train_dataset[:,79:84]
x_train_EMG=train_dataset[:,95:106]
x_train_JP=train_dataset[:,106:129]

x_train_Kinematics=np.concatenate((x_train_Kinematics[:,6:12],x_train_Kinematics[:,13:19]),axis=1)
x_train_JP=np.concatenate((x_train_JP[:,6:9],x_train_JP[:,15:16],x_train_JP[:,17:18],x_train_JP[:,19:20],x_train_JP[:,21:22]),axis=1)

x_train_kinetics=np.concatenate((x_train_kinetics[:,6:8],x_train_kinetics[:,15:16],x_train_kinetics[:,17:18]),axis=1)

x_train=np.concatenate((x_train_IMUs,x_train_Kinematics,x_train_EMG,x_train_JP,x_train_GON,x_train_kinetics,x_train_GRF[:,0:3]),axis=1)




scale= StandardScaler()
scaler = MinMaxScaler(feature_range=(0, 1))
train_X_1_1=x_train

# # Test features #
x_test_IMUs=test_dataset[:,0:24]
x_test_Kinematics=test_dataset[:,24:47]
x_test_kinetics=test_dataset[:,47:70]
x_test_GRF=test_dataset[:,70:79]
x_test_GON=test_dataset[:,79:84]
x_test_EMG=test_dataset[:,95:106]
x_test_JP=test_dataset[:,106:129]

x_test_Kinematics=np.concatenate((x_test_Kinematics[:,6:12],x_test_Kinematics[:,13:19]),axis=1)
x_test_JP=np.concatenate((x_test_JP[:,6:9],x_test_JP[:,15:16],x_test_JP[:,17:18],x_test_JP[:,19:20],x_test_JP[:,21:22]),axis=1)

print(x_test_Kinematics.shape)
print(x_test_JP.shape)

x_test_kinetics=np.concatenate((x_test_kinetics[:,6:8],x_test_kinetics[:,15:16],x_test_kinetics[:,17:18]),axis=1)

x_test=np.concatenate((x_test_IMUs,x_test_Kinematics,x_test_EMG,x_test_JP,x_test_GON,x_test_kinetics,x_test_GRF[:,0:3]),axis=1)


test_X_1_1=x_test

print(x_test.shape)



m1=59
m2=66



  ### Label ###

train_y_1_1=train_dataset[:,m1:m2]
test_y_1_1=test_dataset[:,m1:m2]

train_dataset_1=np.concatenate((train_X_1_1,train_y_1_1),axis=1)
test_dataset_1=np.concatenate((test_X_1_1,test_y_1_1),axis=1)

train_dataset_1=pd.DataFrame(train_dataset_1)
test_dataset_1=pd.DataFrame(test_dataset_1)

train_dataset_1.dropna(axis=0,inplace=True)
test_dataset_1.dropna(axis=0,inplace=True)

train_dataset_1=np.array(train_dataset_1)
test_dataset_1=np.array(test_dataset_1)

train_dataset_sum = np. sum(train_dataset_1)
array_has_nan = np. isinf(train_dataset_1[:,0:59])

print(array_has_nan)

print(train_dataset_1.shape)



train_X_1=train_dataset_1[:,0:m1]
test_X_1=test_dataset_1[:,0:m1]

train_y_1=train_dataset_1[:,m1:m1+7]
test_y_1=test_dataset_1[:,m1:m1+7]



L1=len(train_X_1)
L2=len(test_X_1)

print(L1+L2)

w=100



a1=L1//w
b1=L1%w

a2=L2//w
b2=L2%w

# a3=L3//w
# b3=L3%w

     #### Features ####
train_X_2=train_X_1[L1-w+b1:L1,:]
test_X_2=test_X_1[L2-w+b2:L2,:]
# validation_X_2=validation_X_1[L3-w+b3:L3,:]


    #### Output ####

train_y_2=train_y_1[L1-w+b1:L1,:]
test_y_2=test_y_1[L2-w+b2:L2,:]
# validation_y_2=validation_y_1[L3-w+b3:L3,:]



     #### Features ####

train_X=np.concatenate((train_X_1,train_X_2),axis=0)
test_X=np.concatenate((test_X_1,test_X_2),axis=0)
# validation_X=np.concatenate((validation_X_1,validation_X_2),axis=0)


    #### Output ####

train_y=np.concatenate((train_y_1,train_y_2),axis=0)
test_y=np.concatenate((test_y_1,test_y_2),axis=0)
# validation_y=np.concatenate((validation_y_1,validation_y_2),axis=0)


print(train_y.shape)
    #### Reshaping ####
train_X_3_p= train_X.reshape((a1+1,w,train_X.shape[1]))
test_X = test_X.reshape((a2+1,w,test_X.shape[1]))

output_dim=7


train_y_3_p= train_y.reshape((a1+1,w,output_dim))
test_y= test_y.reshape((a2+1,w,output_dim))



# train_X_1D=train_X_3
test_X_1D=test_X

train_X_3=train_X_3_p
train_y_3=train_y_3_p
# print(train_X_4.shape,train_y_3.shape)


train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)
#train_X_1D, X_validation_1D_ridge, train_y, Y_validation_ridge = train_test_split(train_X_1D_m,train_y_m, test_size=0.10, random_state=True)   [0:2668,:,:]

print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)



Bag_samples=train_X_1D.shape[0]
print(Bag_samples)

s=test_X_1D.shape[0]*w

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

features=6
train_X_2D=train_X_1D[:,:,0:24].reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,4)
test_X_2D=test_X_1D[:,:,0:24].reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,4)
X_validation_2D= X_validation_1D[:,:,0:24].reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],features,4)


print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)

### IMUs- Chest, Waist, Right Foot, Right shank, Right thigh, Left Foot, Left shank, Left thigh, 2D-body coordinate
### 0:48- IMU, 48:92-2D body coordinate, 92:97-- Target

batch_size = 64

val_targets = torch.Tensor(Y_validation)
test_features = torch.Tensor(test_X_1D)
test_features_2D = torch.Tensor(test_X_2D)
test_targets = torch.Tensor(test_y)


## all Modality Features

train_features= torch.Tensor(train_X_1D)
train_features_2D = torch.Tensor(train_X_2D)
train_targets = torch.Tensor(train_y_5)
val_features= torch.Tensor(X_validation_1D)
val_features_2D = torch.Tensor(X_validation_2D)


train_features_acc_4=torch.cat((train_features[:,:,0:3],train_features[:,:,6:9],train_features[:,:,12:15],train_features[:,:,18:21]),axis=-1)
test_features_acc_4=torch.cat((test_features[:,:,0:3],test_features[:,:,6:9],test_features[:,:,12:15],test_features[:,:,18:21]),axis=-1)
val_features_acc_4=torch.cat((val_features[:,:,0:3],val_features[:,:,6:9],val_features[:,:,12:15],val_features[:,:,18:21]),axis=-1)


train_features_gyr_4=torch.cat((train_features[:,:,3:6],train_features[:,:,9:12],train_features[:,:,15:18],train_features[:,:,21:24]),axis=-1)
test_features_gyr_4=torch.cat((test_features[:,:,3:6],test_features[:,:,9:12],test_features[:,:,15:18],test_features[:,:,21:24]),axis=-1)
val_features_gyr_4=torch.cat((val_features[:,:,3:6],val_features[:,:,9:12],val_features[:,:,15:18],val_features[:,:,21:24]),axis=-1)


train_features_Kinematics=train_features[:,:,24:36]
test_features_Kinematics=test_features[:,:,24:36]
val_features_Kinematics=val_features[:,:,24:36]


train_features_EMG=train_features[:,:,36:47]
test_features_EMG=test_features[:,:,36:47]
val_features_EMG=val_features[:,:,36:47]


train_features_JP=train_features[:,:,52:59]
test_features_JP=test_features[:,:,52:59]
val_features_JP=val_features[:,:,52:59]


train = TensorDataset(train_features, train_features_2D, train_features_acc_4,train_features_gyr_4, train_features_Kinematics,train_features_EMG,train_features_JP, train_targets)
val = TensorDataset(val_features,val_features_2D, val_features_acc_4, val_features_gyr_4, val_features_Kinematics, val_features_EMG, val_features_JP,val_targets)
test = TensorDataset(test_features, test_features_2D, test_features_acc_4, test_features_gyr_4, test_features_Kinematics,test_features_EMG,test_features_JP, test_targets)

train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=False)
val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, drop_last=False)
test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=False)


"""# Feature Extraction """

def feature_extractor(data):
  feat=[]
  feat_final=[]

  for i in range(24):
      signal=data[:,:,i]
      A_1=np.mean(signal,axis=1)
      A_2=np.sqrt(np.mean(signal ** 2,axis=1))
      A_3=np.min(signal,axis=1)   ## max- Statistical
      A_4=np.max(signal,axis=1)   ## min- Statistical
      A_5=np.mean(np.absolute(signal),axis=1)   ## mean- Statistical
      A_6=np.std(signal,axis=1)  ## standard Deviation- Statistical
      A_7=np.mean(np.abs(np.diff(signal,prepend=data[:,0:1,i],axis=1)),axis=1) ## Mean Absolute Difference
      A_8=np.mean(np.diff(signal,prepend=data[:,0:1,i],axis=1),axis=1) ## Mean Absolute Difference
      A_9=np.median(np.diff(signal,prepend=data[:,0:1,i],axis=1),axis=1) ## Mean  Difference
      A_10=np.median(np.abs(np.diff(signal,prepend=data[:,0:1,i],axis=1)),axis=1) ## Mean Absolute Difference
      A_11=np.percentile(signal, 75,axis=1) - np.percentile(signal, 25,axis=1)  # Interquartile Range-- Statistical
      # A_12=scipy.stats.kurtosis(signal,axis=1)   ## Kurtosis--Statistical
      # A_13=scipy.stats.skew(signal,axis=1)       ## Skewness--Statistical
      A_14=np.median(signal,axis=1) ## median- Statistical
      A_15=np.var(signal,axis=1)
      median = np.median(signal, axis=1)
      A_16 = np.median(np.abs(signal - median[:, np.newaxis]), axis=1)
      # A_16=scipy.stats.median_absolute_deviation(signal,axis=1,scale=1)
      A_17=np.mean(np.abs(signal - np.mean(signal, axis=1).reshape(signal.shape[0],1)), axis=1)
      A_18=np.mean(np.diff(signal,prepend=data[:,0:1,i],axis=1),axis=1)
      dif=np.diff(data[:,:,i],prepend=data[:,0:1,i],axis=1)
      A_19=np.sum(np.absolute(dif),axis=1)  ### Waveform length
      A_20=np.sum(np.absolute(dif>0),axis=1)  ### Zero Crossing
      A_21=np.sum(np.absolute(np.diff(dif,prepend=data[:,0:1,i],axis=1))>0,axis=1)  ## Slope Sign Changes

      feat=np.vstack((A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10,A_11,A_14,A_15,A_16,A_17))
      # feat=np.vstack((A_18,A_19,A_20))
      feat_final.append((feat))
  feat_final=np.array(feat_final)
  feat_final_1=feat_final.reshape([feat_final.shape[0]*feat_final.shape[1],feat_final.shape[-1]])
  feat_final_1=np.transpose(feat_final_1)

  return(feat_final_1)


import scipy

X_train_feat=feature_extractor(train_X_1D.astype(np.float64))
X_test_feat=feature_extractor(test_X_1D.astype(np.float64))
X_validation_feat=feature_extractor(X_validation_1D.astype(np.float64))


print(X_train_feat.shape)

# Calculate the column-wise mean
column_means = np.nanmean(X_train_feat, axis=0)

# Find NaN indices for each column
nan_indices = np.isnan(X_train_feat)

# Replace NaN values in each column with column means
for col_index in range(X_train_feat.shape[1]):
    X_train_feat[nan_indices[:, col_index], col_index] = column_means[col_index]


# Convert back to PyTorch tensor
X_train_feat = torch.tensor(X_train_feat)


##############################################################################

# Calculate the column-wise mean
column_means = np.nanmean(X_test_feat, axis=0)

# Find NaN indices for each column
nan_indices = np.isnan(X_test_feat)

# Replace NaN values in each column with column means
for col_index in range(X_test_feat.shape[1]):
    X_test_feat[nan_indices[:, col_index], col_index] = column_means[col_index]


# Convert back to PyTorch tensor
X_test_feat = torch.tensor(X_test_feat)

##############################################################################

# Calculate the column-wise mean
column_means = np.nanmean(X_validation_feat, axis=0)

# Find NaN indices for each column
nan_indices = np.isnan(X_validation_feat)

# Replace NaN values in each column with column means
for col_index in range(X_validation_feat.shape[1]):
    X_validation_feat[nan_indices[:, col_index], col_index] = column_means[col_index]


# Convert back to PyTorch tensor
X_validation_feat = torch.tensor(X_validation_feat)

X_train_feat=torch.Tensor(X_train_feat)
X_test_feat=torch.Tensor(X_test_feat)
X_validation_feat=torch.Tensor(X_validation_feat)

print(X_train_feat.shape)

print(X_train_feat.shape)

train_feat = TensorDataset(X_train_feat, train_features_2D, train_features_acc_4,train_features_gyr_4, train_features_Kinematics,train_features_EMG,train_features_JP, train_targets)
val_feat = TensorDataset(X_validation_feat,val_features_2D, val_features_acc_4, val_features_gyr_4, val_features_Kinematics, val_features_EMG, val_features_JP,val_targets)
test_feat = TensorDataset(X_test_feat, test_features_2D, test_features_acc_4, test_features_gyr_4, test_features_Kinematics,test_features_EMG,test_features_JP, test_targets)

train_loader_feat = DataLoader(train_feat, batch_size=batch_size, shuffle=True, drop_last=False)
val_loader_feat = DataLoader(val_feat, batch_size=batch_size, shuffle=True, drop_last=False)
test_loader_feat = DataLoader(test_feat, batch_size=batch_size, shuffle=False, drop_last=False)

# Important Functions

def RMSE_prediction(yhat_4,test_y,s):

  s1=yhat_4.shape[0]*yhat_4.shape[1]

  test_o=test_y.reshape((s1,output_dim))
  yhat=yhat_4.reshape((s1,output_dim))




  y_1_no=yhat[:,0]
  y_2_no=yhat[:,1]
  y_3_no=yhat[:,2]
  y_4_no=yhat[:,3]
  y_5_no=yhat[:,4]
  y_6_no=yhat[:,5]
  y_7_no=yhat[:,6]


  y_1=y_1_no
  y_2=y_2_no
  y_3=y_3_no
  y_4=y_4_no
  y_5=y_5_no
  y_6=y_6_no
  y_7=y_7_no


  y_test_1=test_o[:,0]
  y_test_2=test_o[:,1]
  y_test_3=test_o[:,2]
  y_test_4=test_o[:,3]
  y_test_5=test_o[:,4]
  y_test_6=test_o[:,5]
  y_test_7=test_o[:,6]


  Z_1=y_1
  Z_2=y_2
  Z_3=y_3
  Z_4=y_4
  Z_5=y_5
  Z_6=y_6
  Z_7=y_7



  ###calculate RMSE

  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
  rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100
  rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100


  print(rmse_1)
  print(rmse_2)
  print(rmse_3)
  print(rmse_4)
  print(rmse_5)
  print(rmse_6)
  print(rmse_7)


  p_1=np.corrcoef(y_1, y_test_1)[0, 1]
  p_2=np.corrcoef(y_2, y_test_2)[0, 1]
  p_3=np.corrcoef(y_3, y_test_3)[0, 1]
  p_4=np.corrcoef(y_4, y_test_4)[0, 1]
  p_5=np.corrcoef(y_5, y_test_5)[0, 1]
  p_6=np.corrcoef(y_6, y_test_6)[0, 1]
  p_7=np.corrcoef(y_7, y_test_7)[0, 1]

  print("\n")
  print(p_1)
  print(p_2)
  print(p_3)
  print(p_4)
  print(p_5)
  print(p_6)
  print(p_7)



              ### Correlation ###
  p=np.array([p_1,p_2,p_3,p_4,p_5,p_6,p_7])




      #### Mean and standard deviation ####

  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6,rmse_7])

      #### Mean and standard deviation ####
  m=statistics.mean(rmse)
  SD=statistics.stdev(rmse)
  print('Mean: %.3f' % m,'+/- %.3f' %SD)

  m_c=statistics.mean(p)
  SD_c=statistics.stdev(p)
  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)



  return rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7





############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


def PCC_prediction(yhat_4,test_y,s):

  s1=yhat_4.shape[0]*yhat_4.shape[1]

  test_o=test_y.reshape((s1,output_dim))
  yhat=yhat_4.reshape((s1,output_dim))


  y_1_no=yhat[:,0]
  y_2_no=yhat[:,1]
  y_3_no=yhat[:,2]
  y_4_no=yhat[:,3]
  y_5_no=yhat[:,4]
  y_6_no=yhat[:,5]
  y_7_no=yhat[:,6]


  y_test_1=test_o[:,0]
  y_test_2=test_o[:,1]
  y_test_3=test_o[:,2]
  y_test_4=test_o[:,3]
  y_test_5=test_o[:,4]
  y_test_6=test_o[:,5]
  y_test_7=test_o[:,6]

  y_1=y_1_no
  y_2=y_2_no
  y_3=y_3_no
  y_4=y_4_no
  y_5=y_5_no
  y_6=y_6_no
  y_7=y_7_no


  Y_1=y_1
  Y_2=y_2
  Y_3=y_3
  Y_4=y_4
  Y_5=y_5
  Y_6=y_6
  Y_7=y_7


  ###calculate RMSE

  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
  rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100
  rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100


  print(rmse_1)
  print(rmse_2)
  print(rmse_3)
  print(rmse_4)
  print(rmse_5)
  print(rmse_6)
  print(rmse_7)


  p_1=np.corrcoef(y_1, y_test_1)[0, 1]
  p_2=np.corrcoef(y_2, y_test_2)[0, 1]
  p_3=np.corrcoef(y_3, y_test_3)[0, 1]
  p_4=np.corrcoef(y_4, y_test_4)[0, 1]
  p_5=np.corrcoef(y_5, y_test_5)[0, 1]
  p_6=np.corrcoef(y_6, y_test_6)[0, 1]
  p_7=np.corrcoef(y_7, y_test_7)[0, 1]


  print("\n")
  print(p_1)
  print(p_2)
  print(p_3)
  print(p_4)
  print(p_5)
  print(p_6)
  print(p_7)

              ### Correlation ###
  p=np.array([p_1,p_2,p_3,p_4,p_5,p_6,p_7])


      #### Mean and standard deviation ####

  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6,rmse_7])

      #### Mean and standard deviation ####
  m=statistics.mean(rmse)
  SD=statistics.stdev(rmse)
  print('Mean: %.3f' % m,'+/- %.3f' %SD)

  m_c=statistics.mean(p)
  SD_c=statistics.stdev(p)
  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)


  return rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5,Y_6,Y_7


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)

    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)

    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x

    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x

    return (b_0, b_1)


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################



def DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Y_6,Y_7,Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7):

  a_1,b_1=estimate_coef(Y_1,Z_1)
  a_2,b_2=estimate_coef(Y_2,Z_2)
  a_3,b_3=estimate_coef(Y_3,Z_3)
  a_4,b_4=estimate_coef(Y_4,Z_4)
  a_5,b_5=estimate_coef(Y_5,Z_5)
  a_6,b_6=estimate_coef(Y_6,Z_6)
  a_7,b_7=estimate_coef(Y_7,Z_7)

  #### All 16 angles prediction  ####

  s1=yhat_4.shape[0]*yhat_4.shape[1]


  test_o=test_y.reshape((s1,output_dim))
  yhat=yhat_4.reshape((s1,output_dim))


  y_1_no=yhat[:,0]
  y_2_no=yhat[:,1]
  y_3_no=yhat[:,2]
  y_4_no=yhat[:,3]
  y_5_no=yhat[:,4]
  y_6_no=yhat[:,5]
  y_7_no=yhat[:,6]



  y_test_1=test_o[:,0]
  y_test_2=test_o[:,1]
  y_test_3=test_o[:,2]
  y_test_4=test_o[:,3]
  y_test_5=test_o[:,4]
  y_test_6=test_o[:,5]
  y_test_7=test_o[:,6]

  y_1=y_1_no
  y_2=y_2_no
  y_3=y_3_no
  y_4=y_4_no
  y_5=y_5_no
  y_6=y_6_no
  y_7=y_7_no



  y_1=y_1*b_1+a_1
  y_2=y_2*b_2+a_2
  y_3=y_3*b_3+a_3
  y_4=y_4*b_4+a_4
  y_5=y_5*b_5+a_5
  y_6=y_6*b_6+a_6
  y_7=y_7*b_7+a_7


  ###calculate RMSE

  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
  rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100
  rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100



  print(rmse_1)
  print(rmse_2)
  print(rmse_3)
  print(rmse_4)
  print(rmse_5)
  print(rmse_6)
  print(rmse_7)



  p_1=np.corrcoef(y_1, y_test_1)[0, 1]
  p_2=np.corrcoef(y_2, y_test_2)[0, 1]
  p_3=np.corrcoef(y_3, y_test_3)[0, 1]
  p_4=np.corrcoef(y_4, y_test_4)[0, 1]
  p_5=np.corrcoef(y_5, y_test_5)[0, 1]
  p_6=np.corrcoef(y_6, y_test_6)[0, 1]
  p_7=np.corrcoef(y_7, y_test_7)[0, 1]


  print("\n")
  print(p_1)
  print(p_2)
  print(p_3)
  print(p_4)
  print(p_5)
  print(p_6)
  print(p_7)


              ### Correlation ###
  p=np.array([p_1,p_2,p_3,p_4,p_5,p_6,p_7])



      #### Mean and standard deviation ####

  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6,rmse_7])

      #### Mean and standard deviation ####
  m=statistics.mean(rmse)
  SD=statistics.stdev(rmse)
  print('Mean: %.3f' % m,'+/- %.3f' %SD)

  m_c=statistics.mean(p)
  SD_c=statistics.stdev(p)
  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

  return rmse, p



############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


class RMSELoss(nn.Module):
    def __init__(self):
        super(RMSELoss, self).__init__()

    def forward(self, pred, target):
        mse = nn.MSELoss()(pred, target)
        rmse = torch.sqrt(mse)
        return rmse

# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
is_cuda = torch.cuda.is_available()

# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
if is_cuda:
    device = torch.device("cuda")
else:
    device = torch.device("cpu")


####################################################################################################################################################################################################

# IMU- Foot

def train_mm_early_IMU(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader):
            optimizer.zero_grad()

            output= model(data_1D[:,:,0:6].to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader:

                output= model(data_1D[:,:,0:6].to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_early_IMU_feat(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader_feat):
            optimizer.zero_grad()

            output= model(data_1D[:,0*15:6*15].to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader_feat:

                output= model(data_1D[:,0*15:6*15].to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_early_IMU_2D(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader):
            optimizer.zero_grad()

            output= model(data_2D[:,:,:,0:1].to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader:


                output= model(data_2D[:,:,:,0:1].to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")


    return model

## FFN

class FFN(nn.Module):
    def __init__(self, input_1D):
        super(FFN, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)

        self.fc1 = nn.Linear(input_1D, 512)
        self.dropout1 = nn.Dropout(0.05)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.05)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(0.05)

        self.flatten=nn.Flatten()

        self.fc_f=nn.Linear(128*100, 7*100)


    def forward(self, inputs_1D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 100, inputs_1D_N_1.size(-1))

        x = F.relu(self.fc1(inputs_1D_N))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)

        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = FFN(6)

mm_early_ffn = train_mm_early_IMU(train_loader, lr,40,model,path + 'FFN_Foot.pth')

mm_early_ffn= FFN(6)
mm_early_ffn.load_state_dict(torch.load(path+'FFN_Foot.pth'))
mm_early_ffn.to(device)

mm_early_ffn.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader):

        output= mm_early_ffn(data_1D[:,:,0:6].to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_1=np.hstack([rmse,p])

## LSTM

class Encoder(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder, self).__init__()
        self.lstm_1 = nn.GRU(input_dim, 512, bidirectional=False, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.GRU(512, 256, bidirectional=False, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)
        out_2=self.flatten(out_2)


        return out_2

class lstm_model(nn.Module):
    def __init__(self, input_1D):
        super(lstm_model, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)
        self.lstm = Encoder(input_1D,0.40)
        self.flatten=nn.Flatten()
        self.fc_f=nn.Linear(256*100, 7*100)

    def forward(self, inputs_1D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 100, inputs_1D_N_1.size(-1))

        x=self.lstm(inputs_1D_N)
        x=self.flatten(x)
        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = lstm_model(6)

mm_early_lstm = train_mm_early_IMU(train_loader, lr, 40, model, path +'LSTM_Foot.pth')

mm_early_lstm= lstm_model(6)
mm_early_lstm.load_state_dict(torch.load(path+'LSTM_Foot.pth'))
mm_early_lstm.to(device)

mm_early_lstm.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader):

        output= mm_early_lstm(data_1D[:,:,0:6].to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_2=np.hstack([rmse,p])

## CNN_2D

class Encoder_CNN_2D(nn.Module):
    def __init__(self, input_size, dropout, hidden_dim=256, output_size=512, kernel_size=(3,5), stride=(1,1), padding=(1,2)):
        super(Encoder_CNN_2D, self).__init__()
        self.conv1 = nn.Conv2d(input_size, hidden_dim, kernel_size, stride, padding)
        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, padding)
        self.conv3 = nn.Conv2d(hidden_dim, output_size, kernel_size, stride, padding)
        self.conv4 = nn.Conv2d(output_size, output_size, kernel_size, stride, padding)
        self.BN_1= nn.BatchNorm2d(hidden_dim)
        self.BN_2= nn.BatchNorm2d(hidden_dim)
        self.BN_3= nn.BatchNorm2d(output_size)
        self.BN_4= nn.BatchNorm2d(output_size)
        self.pool_1 = nn.MaxPool2d(kernel_size=(2,2))
        self.pool_2 = nn.MaxPool2d(kernel_size=(1,2))

        # Fully connected layers
        self.fc1 = nn.Linear(512, 64)
        self.dropout1 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(dropout)
        self.flatten=nn.Flatten()

    def forward(self, x):

        x = x.transpose(1, 3)  # reshape from (batch_size, seq_len, input_size) to (batch_size, input_size, seq_len)
        x = F.relu(self.conv1(x))
        x = self.BN_1(x)
        x = self.pool_1(x)
        x = F.relu(self.conv2(x))
        x = self.BN_2(x)
        x = self.pool_1(x)
        x = F.relu(self.conv3(x))
        x = self.BN_3(x)
        x = self.pool_2(x)
        x = F.relu(self.conv4(x))
        x = self.BN_4(x)
        x = self.pool_2(x)

        # print(x.shape)

        x = x.transpose(1, 3)  # reshape back to (batch_size, seq_len, output_size)

        # print(x.shape)


        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)

        # print(x.shape)
        # x = self.flatten(x)


        return x

class conv2d_model(nn.Module):
    def __init__(self, input_2D):
        super(conv2d_model, self).__init__()

        self.BN_2= nn.BatchNorm2d(input_2D, affine=False)
        self.conv2d = Encoder_CNN_2D(input_2D,0.05)
        self.flatten=nn.Flatten()
        self.fc_f=nn.Linear(96*2, 7*100)

    def forward(self, inputs_2D_N):

        inputs_2D_N_1=inputs_2D_N.transpose(1,3)
        inputs_2D_N_2=self.BN_2(inputs_2D_N_1)
        inputs_2D_N_3=inputs_2D_N_2.transpose(1,3)

        x=self.conv2d(inputs_2D_N_3)
        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = conv2d_model(1)

mm_early_conv2d = train_mm_early_IMU_2D(train_loader, lr, 40, model, path +'Conv2D_IMU_foot.pth')

mm_early_conv2d= conv2d_model(1)
mm_early_conv2d.load_state_dict(torch.load(path+'Conv2D_IMU_foot.pth'))
mm_early_conv2d.to(device)

mm_early_conv2d.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader):

        output= mm_early_conv2d(data_2D[:,:,:,0:1].to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_3=np.hstack([rmse,p])

## FFN (HF)

class FFN_HF(nn.Module):
    def __init__(self, input_1D):
        super(FFN_HF, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)

        self.fc1 = nn.Linear(input_1D, 512)
        self.dropout1 = nn.Dropout(0.05)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.05)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(0.05)

        self.flatten=nn.Flatten()

        self.fc_f=nn.Linear(128, 7*100)


    def forward(self, inputs_1D_N):



        inputs_1D_N=self.BN(inputs_1D_N)

        x = F.relu(self.fc1(inputs_1D_N))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)

        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = FFN_HF(6*15)

mm_early_ffn_hf = train_mm_early_IMU_feat(train_loader, lr,40,model,path + 'FFN_HF_Foot.pth')

mm_early_ffn_hf= FFN_HF(6*15)
mm_early_ffn_hf.load_state_dict(torch.load(path+'FFN_HF_Foot.pth'))
mm_early_ffn_hf.to(device)

mm_early_ffn_hf.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader_feat):


        output= mm_early_ffn_hf(data_1D[:,0*15:6*15].to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del  target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_4=np.hstack([rmse,p])

# IMU- Foot+ Trunk

def train_mm_early_IMU(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,:,0:6],data_1D[:,:,18:24]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:1],data_2D[:,:,:,3:4]),dim=-1)

            output= model(data_1D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader:

                data_1D=torch.cat((data_1D[:,:,0:6],data_1D[:,:,18:24]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:1],data_2D[:,:,:,3:4]),dim=-1)

                output= model(data_1D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_early_IMU_feat(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader_feat):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,0*15:6*15],data_1D[:,18*15:24*15]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:1],data_2D[:,:,:,3:4]),dim=-1)

            output= model(data_1D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader_feat:

                data_1D=torch.cat((data_1D[:,0*15:6*15],data_1D[:,18*15:24*15]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:1],data_2D[:,:,:,3:4]),dim=-1)

                output= model(data_1D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_early_IMU_2D(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,:,0:6],data_1D[:,:,18:24]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:1],data_2D[:,:,:,3:4]),dim=-1)

            output= model(data_2D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader:

                data_1D=torch.cat((data_1D[:,:,0:6],data_1D[:,:,18:24]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:1],data_2D[:,:,:,3:4]),dim=-1)

                output= model(data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

## FFN

class FFN(nn.Module):
    def __init__(self, input_1D):
        super(FFN, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)

        self.fc1 = nn.Linear(input_1D, 512)
        self.dropout1 = nn.Dropout(0.05)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.05)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(0.05)

        self.flatten=nn.Flatten()

        self.fc_f=nn.Linear(128*100, 7*100)


    def forward(self, inputs_1D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 100, inputs_1D_N_1.size(-1))

        x = F.relu(self.fc1(inputs_1D_N))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)

        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = FFN(12)

mm_early_ffn = train_mm_early_IMU(train_loader, lr,40,model,path + 'FFN_Foot_Trunk.pth')

mm_early_ffn= FFN(12)
mm_early_ffn.load_state_dict(torch.load(path+'FFN_Foot_Trunk.pth'))
mm_early_ffn.to(device)

mm_early_ffn.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics, data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:6],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:1],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_ffn(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_5=np.hstack([rmse,p])

## LSTM

class Encoder(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder, self).__init__()
        self.lstm_1 = nn.GRU(input_dim, 512, bidirectional=False, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.GRU(512, 256, bidirectional=False, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)
        out_2=self.flatten(out_2)


        return out_2

class lstm_model(nn.Module):
    def __init__(self, input_1D):
        super(lstm_model, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)
        self.lstm = Encoder(input_1D,0.40)
        self.flatten=nn.Flatten()
        self.fc_f=nn.Linear(256*100, 7*100)

    def forward(self, inputs_1D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 100, inputs_1D_N_1.size(-1))

        x=self.lstm(inputs_1D_N)
        x=self.flatten(x)
        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = lstm_model(12)

mm_early_lstm = train_mm_early_IMU(train_loader, lr, 40, model, path +'LSTM_Foot_trunk.pth')

mm_early_lstm= lstm_model(12)
mm_early_lstm.load_state_dict(torch.load(path+'LSTM_Foot_trunk.pth'))
mm_early_lstm.to(device)

mm_early_lstm.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:6],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:1],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_lstm(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_6=np.hstack([rmse,p])

## CNN_2D

class Encoder_CNN_2D(nn.Module):
    def __init__(self, input_size, dropout, hidden_dim=256, output_size=512, kernel_size=(3,5), stride=(1,1), padding=(1,2)):
        super(Encoder_CNN_2D, self).__init__()
        self.conv1 = nn.Conv2d(input_size, hidden_dim, kernel_size, stride, padding)
        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, padding)
        self.conv3 = nn.Conv2d(hidden_dim, output_size, kernel_size, stride, padding)
        self.conv4 = nn.Conv2d(output_size, output_size, kernel_size, stride, padding)
        self.BN_1= nn.BatchNorm2d(hidden_dim)
        self.BN_2= nn.BatchNorm2d(hidden_dim)
        self.BN_3= nn.BatchNorm2d(output_size)
        self.BN_4= nn.BatchNorm2d(output_size)
        self.pool_1 = nn.MaxPool2d(kernel_size=(2,2))
        self.pool_2 = nn.MaxPool2d(kernel_size=(1,2))

        # Fully connected layers
        self.fc1 = nn.Linear(512, 64)
        self.dropout1 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(dropout)
        self.flatten=nn.Flatten()

    def forward(self, x):

        x = x.transpose(1, 3)  # reshape from (batch_size, seq_len, input_size) to (batch_size, input_size, seq_len)
        x = F.relu(self.conv1(x))
        x = self.BN_1(x)
        x = self.pool_1(x)
        x = F.relu(self.conv2(x))
        x = self.BN_2(x)
        x = self.pool_1(x)
        x = F.relu(self.conv3(x))
        x = self.BN_3(x)
        x = self.pool_2(x)
        x = F.relu(self.conv4(x))
        x = self.BN_4(x)
        x = self.pool_2(x)

        # print(x.shape)

        x = x.transpose(1, 3)  # reshape back to (batch_size, seq_len, output_size)

        # print(x.shape)


        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)

        # print(x.shape)
        # x = self.flatten(x)


        return x

class conv2d_model(nn.Module):
    def __init__(self, input_2D):
        super(conv2d_model, self).__init__()

        self.BN_2= nn.BatchNorm2d(input_2D, affine=False)
        self.conv2d = Encoder_CNN_2D(input_2D,0.05)
        self.flatten=nn.Flatten()
        self.fc_f=nn.Linear(96*2, 7*100)

    def forward(self, inputs_2D_N):

        inputs_2D_N_1=inputs_2D_N.transpose(1,3)
        inputs_2D_N_2=self.BN_2(inputs_2D_N_1)
        inputs_2D_N_3=inputs_2D_N_2.transpose(1,3)

        x=self.conv2d(inputs_2D_N_3)
        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = conv2d_model(2)

mm_early_conv2d = train_mm_early_IMU_2D(train_loader, lr, 40, model, path +'Conv2D_IMU_foot_trunk.pth')

mm_early_conv2d= conv2d_model(2)
mm_early_conv2d.load_state_dict(torch.load(path+'Conv2D_IMU_foot_trunk.pth'))
mm_early_conv2d.to(device)

mm_early_conv2d.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:6],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:1],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_conv2d(data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_7=np.hstack([rmse,p])

## FFN (HF)

class FFN_HF(nn.Module):
    def __init__(self, input_1D):
        super(FFN_HF, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)

        self.fc1 = nn.Linear(input_1D, 512)
        self.dropout1 = nn.Dropout(0.05)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.05)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(0.05)

        self.flatten=nn.Flatten()

        self.fc_f=nn.Linear(128, 7*100)


    def forward(self, inputs_1D_N):


        inputs_1D_N=self.BN(inputs_1D_N)

        x = F.relu(self.fc1(inputs_1D_N))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)

        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = FFN_HF(12*15)

mm_early_ffn_hf = train_mm_early_IMU_feat(train_loader, lr,40,model,path + 'FFN_HF_Foot_trunk.pth')

mm_early_ffn_hf= FFN_HF(12*15)
mm_early_ffn_hf.load_state_dict(torch.load(path+'FFN_HF_Foot_trunk.pth'))
mm_early_ffn_hf.to(device)

mm_early_ffn_hf.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader_feat):

        data_1D=torch.cat((data_1D[:,0*15:6*15],data_1D[:,18*15:24*15]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:1],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_ffn_hf(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del  target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_8=np.hstack([rmse,p])

# IMU- Foot+ Trunk +Shank

def train_mm_early_IMU(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,:,0:12],data_1D[:,:,18:24]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:2],data_2D[:,:,:,3:4]),dim=-1)

            output= model(data_1D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader:

                data_1D=torch.cat((data_1D[:,:,0:12],data_1D[:,:,18:24]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:2],data_2D[:,:,:,3:4]),dim=-1)

                output= model(data_1D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_early_IMU_feat(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader_feat):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,0*15:12*15],data_1D[:,18*15:24*15]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:2],data_2D[:,:,:,3:4]),dim=-1)

            output= model(data_1D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader_feat:

                data_1D=torch.cat((data_1D[:,0*15:12*15],data_1D[:,18*15:24*15]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:2],data_2D[:,:,:,3:4]),dim=-1)

                output= model(data_1D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_early_IMU_2D(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,:,0:12],data_1D[:,:,18:24]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:2],data_2D[:,:,:,3:4]),dim=-1)

            output= model(data_2D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader:

                data_1D=torch.cat((data_1D[:,:,0:12],data_1D[:,:,18:24]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:2],data_2D[:,:,:,3:4]),dim=-1)

                output= model(data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

## FFN

class FFN(nn.Module):
    def __init__(self, input_1D):
        super(FFN, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)

        self.fc1 = nn.Linear(input_1D, 512)
        self.dropout1 = nn.Dropout(0.05)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.05)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(0.05)

        self.flatten=nn.Flatten()

        self.fc_f=nn.Linear(128*100, 7*100)


    def forward(self, inputs_1D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 100, inputs_1D_N_1.size(-1))

        x = F.relu(self.fc1(inputs_1D_N))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)

        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = FFN(18)

mm_early_ffn = train_mm_early_IMU(train_loader, lr,40,model,path + 'FFN_Foot_Trunk_shank.pth')

mm_early_ffn= FFN(18)
mm_early_ffn.load_state_dict(torch.load(path+'FFN_Foot_Trunk_shank.pth'))
mm_early_ffn.to(device)

mm_early_ffn.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics, data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:12],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:2],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_ffn(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_9=np.hstack([rmse,p])

## LSTM

class Encoder(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder, self).__init__()
        self.lstm_1 = nn.GRU(input_dim, 512, bidirectional=False, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.GRU(512, 256, bidirectional=False, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)
        out_2=self.flatten(out_2)


        return out_2

class lstm_model(nn.Module):
    def __init__(self, input_1D):
        super(lstm_model, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)
        self.lstm = Encoder(input_1D,0.40)
        self.flatten=nn.Flatten()
        self.fc_f=nn.Linear(256*100, 7*100)

    def forward(self, inputs_1D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 100, inputs_1D_N_1.size(-1))

        x=self.lstm(inputs_1D_N)
        x=self.flatten(x)
        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = lstm_model(18)

mm_early_lstm = train_mm_early_IMU(train_loader, lr, 40, model, path +'LSTM_Foot_trunk_shank.pth')

mm_early_lstm= lstm_model(18)
mm_early_lstm.load_state_dict(torch.load(path+'LSTM_Foot_trunk_shank.pth'))
mm_early_lstm.to(device)

mm_early_lstm.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:12],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:2],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_lstm(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_10=np.hstack([rmse,p])

## CNN_2D

class Encoder_CNN_2D(nn.Module):
    def __init__(self, input_size, dropout, hidden_dim=256, output_size=512, kernel_size=(3,5), stride=(1,1), padding=(1,2)):
        super(Encoder_CNN_2D, self).__init__()
        self.conv1 = nn.Conv2d(input_size, hidden_dim, kernel_size, stride, padding)
        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, padding)
        self.conv3 = nn.Conv2d(hidden_dim, output_size, kernel_size, stride, padding)
        self.conv4 = nn.Conv2d(output_size, output_size, kernel_size, stride, padding)
        self.BN_1= nn.BatchNorm2d(hidden_dim)
        self.BN_2= nn.BatchNorm2d(hidden_dim)
        self.BN_3= nn.BatchNorm2d(output_size)
        self.BN_4= nn.BatchNorm2d(output_size)
        self.pool_1 = nn.MaxPool2d(kernel_size=(2,2))
        self.pool_2 = nn.MaxPool2d(kernel_size=(1,2))

        # Fully connected layers
        self.fc1 = nn.Linear(512, 64)
        self.dropout1 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(dropout)
        self.flatten=nn.Flatten()

    def forward(self, x):

        x = x.transpose(1, 3)  # reshape from (batch_size, seq_len, input_size) to (batch_size, input_size, seq_len)
        x = F.relu(self.conv1(x))
        x = self.BN_1(x)
        x = self.pool_1(x)
        x = F.relu(self.conv2(x))
        x = self.BN_2(x)
        x = self.pool_1(x)
        x = F.relu(self.conv3(x))
        x = self.BN_3(x)
        x = self.pool_2(x)
        x = F.relu(self.conv4(x))
        x = self.BN_4(x)
        x = self.pool_2(x)

        # print(x.shape)

        x = x.transpose(1, 3)  # reshape back to (batch_size, seq_len, output_size)

        # print(x.shape)


        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)

        # print(x.shape)
        # x = self.flatten(x)


        return x

class conv2d_model(nn.Module):
    def __init__(self, input_2D):
        super(conv2d_model, self).__init__()

        self.BN_2= nn.BatchNorm2d(input_2D, affine=False)
        self.conv2d = Encoder_CNN_2D(input_2D,0.05)
        self.flatten=nn.Flatten()
        self.fc_f=nn.Linear(96*2, 7*100)

    def forward(self, inputs_2D_N):

        inputs_2D_N_1=inputs_2D_N.transpose(1,3)
        inputs_2D_N_2=self.BN_2(inputs_2D_N_1)
        inputs_2D_N_3=inputs_2D_N_2.transpose(1,3)

        x=self.conv2d(inputs_2D_N_3)
        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = conv2d_model(3)

mm_early_conv2d = train_mm_early_IMU_2D(train_loader, lr, 40, model, path +'Conv2D_IMU_foot_trunk_shank.pth')

mm_early_conv2d= conv2d_model(3)
mm_early_conv2d.load_state_dict(torch.load(path+'Conv2D_IMU_foot_trunk_shank.pth'))
mm_early_conv2d.to(device)

mm_early_conv2d.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:12],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:2],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_conv2d(data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_11=np.hstack([rmse,p])

## FFN (HF)

class FFN_HF(nn.Module):
    def __init__(self, input_1D):
        super(FFN_HF, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)

        self.fc1 = nn.Linear(input_1D, 512)
        self.dropout1 = nn.Dropout(0.05)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.05)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(0.05)

        self.flatten=nn.Flatten()

        self.fc_f=nn.Linear(128, 7*100)


    def forward(self, inputs_1D_N):


        inputs_1D_N=self.BN(inputs_1D_N)

        x = F.relu(self.fc1(inputs_1D_N))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)

        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = FFN_HF(18*15)

mm_early_ffn_hf = train_mm_early_IMU_feat(train_loader, lr,40,model,path + 'FFN_HF_Foot_trunk_shank.pth')

mm_early_ffn_hf= FFN_HF(18*15)
mm_early_ffn_hf.load_state_dict(torch.load(path+'FFN_HF_Foot_trunk_shank.pth'))
mm_early_ffn_hf.to(device)

mm_early_ffn_hf.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader_feat):

        data_1D=torch.cat((data_1D[:,0*15:12*15],data_1D[:,18*15:24*15]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:2],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_ffn_hf(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del  target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_12=np.hstack([rmse,p])

# IMU- Foot+ Trunk +Shank + Thigh

def train_mm_early_IMU(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

            output= model(data_1D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader:

                data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

                output= model(data_1D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_early_IMU_feat(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader_feat):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,0*15:18*15],data_1D[:,18*15:24*15]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

            output= model(data_1D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader_feat:

                data_1D=torch.cat((data_1D[:,0*15:18*15],data_1D[:,18*15:24*15]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

                output= model(data_1D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_early_IMU_2D(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

            output= model(data_2D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader:

                data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

                output= model(data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

## FFN

class FFN(nn.Module):
    def __init__(self, input_1D):
        super(FFN, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)

        self.fc1 = nn.Linear(input_1D, 512)
        self.dropout1 = nn.Dropout(0.05)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.05)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(0.05)

        self.flatten=nn.Flatten()

        self.fc_f=nn.Linear(128*100, 7*100)


    def forward(self, inputs_1D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 100, inputs_1D_N_1.size(-1))

        x = F.relu(self.fc1(inputs_1D_N))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)

        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = FFN(24)

mm_early_ffn = train_mm_early_IMU(train_loader, lr,40,model,path + 'FFN_Foot_Trunk_shank_thigh.pth')

mm_early_ffn= FFN(24)
mm_early_ffn.load_state_dict(torch.load(path+'FFN_Foot_Trunk_shank_thigh.pth'))
mm_early_ffn.to(device)

mm_early_ffn.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics, data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_ffn(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_13=np.hstack([rmse,p])

## LSTM

class Encoder(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder, self).__init__()
        self.lstm_1 = nn.GRU(input_dim, 512, bidirectional=False, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.GRU(512, 256, bidirectional=False, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)
        out_2=self.flatten(out_2)


        return out_2

class lstm_model(nn.Module):
    def __init__(self, input_1D):
        super(lstm_model, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)
        self.lstm = Encoder(input_1D,0.40)
        self.flatten=nn.Flatten()
        self.fc_f=nn.Linear(256*100, 7*100)

    def forward(self, inputs_1D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 100, inputs_1D_N_1.size(-1))

        x=self.lstm(inputs_1D_N)
        x=self.flatten(x)
        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = lstm_model(24)

mm_early_lstm = train_mm_early_IMU(train_loader, lr, 40, model, path +'LSTM_Foot_trunk_shank_thigh.pth')

mm_early_lstm= lstm_model(24)
mm_early_lstm.load_state_dict(torch.load(path+'LSTM_Foot_trunk_shank_thigh.pth'))
mm_early_lstm.to(device)

mm_early_lstm.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_lstm(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_14=np.hstack([rmse,p])

## CNN_2D

class Encoder_CNN_2D(nn.Module):
    def __init__(self, input_size, dropout, hidden_dim=256, output_size=512, kernel_size=(3,5), stride=(1,1), padding=(1,2)):
        super(Encoder_CNN_2D, self).__init__()
        self.conv1 = nn.Conv2d(input_size, hidden_dim, kernel_size, stride, padding)
        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, padding)
        self.conv3 = nn.Conv2d(hidden_dim, output_size, kernel_size, stride, padding)
        self.conv4 = nn.Conv2d(output_size, output_size, kernel_size, stride, padding)
        self.BN_1= nn.BatchNorm2d(hidden_dim)
        self.BN_2= nn.BatchNorm2d(hidden_dim)
        self.BN_3= nn.BatchNorm2d(output_size)
        self.BN_4= nn.BatchNorm2d(output_size)
        self.pool_1 = nn.MaxPool2d(kernel_size=(2,2))
        self.pool_2 = nn.MaxPool2d(kernel_size=(1,2))

        # Fully connected layers
        self.fc1 = nn.Linear(512, 64)
        self.dropout1 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(dropout)
        self.flatten=nn.Flatten()

    def forward(self, x):

        x = x.transpose(1, 3)  # reshape from (batch_size, seq_len, input_size) to (batch_size, input_size, seq_len)
        x = F.relu(self.conv1(x))
        x = self.BN_1(x)
        x = self.pool_1(x)
        x = F.relu(self.conv2(x))
        x = self.BN_2(x)
        x = self.pool_1(x)
        x = F.relu(self.conv3(x))
        x = self.BN_3(x)
        x = self.pool_2(x)
        x = F.relu(self.conv4(x))
        x = self.BN_4(x)
        x = self.pool_2(x)

        # print(x.shape)

        x = x.transpose(1, 3)  # reshape back to (batch_size, seq_len, output_size)

        # print(x.shape)


        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)

        # print(x.shape)
        # x = self.flatten(x)


        return x

class conv2d_model(nn.Module):
    def __init__(self, input_2D):
        super(conv2d_model, self).__init__()

        self.BN_2= nn.BatchNorm2d(input_2D, affine=False)
        self.conv2d = Encoder_CNN_2D(input_2D,0.05)
        self.flatten=nn.Flatten()
        self.fc_f=nn.Linear(96*2, 7*100)

    def forward(self, inputs_2D_N):

        inputs_2D_N_1=inputs_2D_N.transpose(1,3)
        inputs_2D_N_2=self.BN_2(inputs_2D_N_1)
        inputs_2D_N_3=inputs_2D_N_2.transpose(1,3)

        x=self.conv2d(inputs_2D_N_3)
        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = conv2d_model(4)

mm_early_conv2d = train_mm_early_IMU_2D(train_loader, lr, 40, model, path +'Conv2D_IMU_foot_trunk_shank_thigh.pth')

mm_early_conv2d= conv2d_model(4)
mm_early_conv2d.load_state_dict(torch.load(path+'Conv2D_IMU_foot_trunk_shank_thigh.pth'))
mm_early_conv2d.to(device)

mm_early_conv2d.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_conv2d(data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_15=np.hstack([rmse,p])

## FFN (HF)

class FFN_HF(nn.Module):
    def __init__(self, input_1D):
        super(FFN_HF, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)

        self.fc1 = nn.Linear(input_1D, 512)
        self.dropout1 = nn.Dropout(0.05)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.05)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(0.05)

        self.flatten=nn.Flatten()

        self.fc_f=nn.Linear(128, 7*100)


    def forward(self, inputs_1D_N):


        inputs_1D_N=self.BN(inputs_1D_N)

        x = F.relu(self.fc1(inputs_1D_N))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)

        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = FFN_HF(24*15)

mm_early_ffn_hf = train_mm_early_IMU_feat(train_loader, lr,40,model,path + 'FFN_HF_Foot_trunk_shank_thigh.pth')

mm_early_ffn_hf= FFN_HF(24*15)
mm_early_ffn_hf.load_state_dict(torch.load(path+'FFN_HF_Foot_trunk_shank_thigh.pth'))
mm_early_ffn_hf.to(device)

mm_early_ffn_hf.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader_feat):

        data_1D=torch.cat((data_1D[:,0*15:18*15],data_1D[:,18*15:24*15]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_ffn_hf(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del  target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_16=np.hstack([rmse,p])

# 4 IMUs + 11 EMGs

## Data Processing

"""# Feature Extraction """

def feature_extractor(data):
  feat=[]
  feat_final=[]

  for i in range(35):
      signal=data[:,:,i]
      A_1=np.mean(signal,axis=1)
      A_2=np.sqrt(np.mean(signal ** 2,axis=1))
      A_3=np.min(signal,axis=1)   ## max- Statistical
      A_4=np.max(signal,axis=1)   ## min- Statistical
      A_5=np.mean(np.absolute(signal),axis=1)   ## mean- Statistical
      A_6=np.std(signal,axis=1)  ## standard Deviation- Statistical
      A_7=np.mean(np.abs(np.diff(signal,prepend=data[:,0:1,i],axis=1)),axis=1) ## Mean Absolute Difference
      A_8=np.mean(np.diff(signal,prepend=data[:,0:1,i],axis=1),axis=1) ## Mean Absolute Difference
      A_9=np.median(np.diff(signal,prepend=data[:,0:1,i],axis=1),axis=1) ## Mean  Difference
      A_10=np.median(np.abs(np.diff(signal,prepend=data[:,0:1,i],axis=1)),axis=1) ## Mean Absolute Difference
      A_11=np.percentile(signal, 75,axis=1) - np.percentile(signal, 25,axis=1)  # Interquartile Range-- Statistical
      # A_12=scipy.stats.kurtosis(signal,axis=1)   ## Kurtosis--Statistical
      # A_13=scipy.stats.skew(signal,axis=1)       ## Skewness--Statistical
      A_14=np.median(signal,axis=1) ## median- Statistical
      A_15=np.var(signal,axis=1)
      median = np.median(signal, axis=1)
      A_16 = np.median(np.abs(signal - median[:, np.newaxis]), axis=1)
      # A_16=scipy.stats.median_absolute_deviation(signal,axis=1,scale=1)
      A_17=np.mean(np.abs(signal - np.mean(signal, axis=1).reshape(signal.shape[0],1)), axis=1)
      A_18=np.mean(np.diff(signal,prepend=data[:,0:1,i],axis=1),axis=1)
      dif=np.diff(data[:,:,i],prepend=data[:,0:1,i],axis=1)
      A_19=np.sum(np.absolute(dif),axis=1)  ### Waveform length
      A_20=np.sum(np.absolute(dif>0),axis=1)  ### Zero Crossing
      A_21=np.sum(np.absolute(np.diff(dif,prepend=data[:,0:1,i],axis=1))>0,axis=1)  ## Slope Sign Changes

      feat=np.vstack((A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10,A_11,A_14,A_15,A_16,A_17))
      # feat=np.vstack((A_18,A_19,A_20))
      feat_final.append((feat))
  feat_final=np.array(feat_final)
  feat_final_1=feat_final.reshape([feat_final.shape[0]*feat_final.shape[1],feat_final.shape[-1]])
  feat_final_1=np.transpose(feat_final_1)

  return(feat_final_1)


import scipy

X_train_feat=feature_extractor(np.concatenate((train_X_1D[:,:,0:24],train_X_1D[:,:,36:47]),axis=-1))
X_test_feat=feature_extractor(np.concatenate((test_X_1D[:,:,0:24],test_X_1D[:,:,36:47]),axis=-1))
X_validation_feat=feature_extractor(np.concatenate((X_validation_1D[:,:,0:24],X_validation_1D[:,:,36:47]),axis=-1))


print(train_X_1D.shape)

print(X_train_feat.shape)

# Calculate the column-wise mean
column_means = np.nanmean(X_train_feat, axis=0)

# Find NaN indices for each column
nan_indices = np.isnan(X_train_feat)

# Replace NaN values in each column with column means
for col_index in range(X_train_feat.shape[1]):
    X_train_feat[nan_indices[:, col_index], col_index] = column_means[col_index]


# Convert back to PyTorch tensor
X_train_feat = torch.tensor(X_train_feat)


##############################################################################

# Calculate the column-wise mean
column_means = np.nanmean(X_test_feat, axis=0)

# Find NaN indices for each column
nan_indices = np.isnan(X_test_feat)

# Replace NaN values in each column with column means
for col_index in range(X_test_feat.shape[1]):
    X_test_feat[nan_indices[:, col_index], col_index] = column_means[col_index]


# Convert back to PyTorch tensor
X_test_feat = torch.tensor(X_test_feat)

##############################################################################

# Calculate the column-wise mean
column_means = np.nanmean(X_validation_feat, axis=0)

# Find NaN indices for each column
nan_indices = np.isnan(X_validation_feat)

# Replace NaN values in each column with column means
for col_index in range(X_validation_feat.shape[1]):
    X_validation_feat[nan_indices[:, col_index], col_index] = column_means[col_index]


# Convert back to PyTorch tensor
X_validation_feat = torch.tensor(X_validation_feat)

X_train_feat=torch.Tensor(X_train_feat)
X_test_feat=torch.Tensor(X_test_feat)
X_validation_feat=torch.Tensor(X_validation_feat)

print(X_train_feat.shape)

print(X_train_feat.shape)

train_feat = TensorDataset(X_train_feat, train_features_2D, train_features_acc_4,train_features_gyr_4, train_features_Kinematics,train_features_EMG,train_features_JP, train_targets)
val_feat = TensorDataset(X_validation_feat,val_features_2D, val_features_acc_4, val_features_gyr_4, val_features_Kinematics, val_features_EMG, val_features_JP,val_targets)
test_feat = TensorDataset(X_test_feat, test_features_2D, test_features_acc_4, test_features_gyr_4, test_features_Kinematics,test_features_EMG,test_features_JP, test_targets)

train_loader_feat = DataLoader(train_feat, batch_size=batch_size, shuffle=True, drop_last=False)
val_loader_feat = DataLoader(val_feat, batch_size=batch_size, shuffle=True, drop_last=False)
test_loader_feat = DataLoader(test_feat, batch_size=batch_size, shuffle=False, drop_last=False)

## Training Function

def train_mm_early_IMU(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

            data_1D=torch.cat((data_1D,data_EMG),dim=-1)

            output= model(data_1D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader:

                data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

                data_1D=torch.cat((data_1D,data_EMG),dim=-1)

                output= model(data_1D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_early_IMU_feat(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    # criterion =nn.MSELoss()
    criterion =RMSELoss()

    # criterion=PearsonCorrLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)

    # optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(train_loader_feat):
            optimizer.zero_grad()

            data_1D=torch.cat((data_1D[:,0*15:18*15],data_1D[:,18*15:35*15]),dim=-1)
            data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

            output= model(data_1D.to(device).float())

            # l2_regularization = 0.0
            # for param in model.parameters():
            #     l2_regularization += torch.norm(param, p=2)  # Compute the L2 norm of the parameter


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target in val_loader_feat:

                data_1D=torch.cat((data_1D[:,0*15:18*15],data_1D[:,18*15:35*15]),dim=-1)
                data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)


                output= model(data_1D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break


    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

## FFN

class FFN(nn.Module):
    def __init__(self, input_1D):
        super(FFN, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)

        self.fc1 = nn.Linear(input_1D, 512)
        self.dropout1 = nn.Dropout(0.05)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.05)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(0.05)

        self.flatten=nn.Flatten()

        self.fc_f=nn.Linear(128*100, 7*100)


    def forward(self, inputs_1D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 100, inputs_1D_N_1.size(-1))

        x = F.relu(self.fc1(inputs_1D_N))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)

        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = FFN(35)

mm_early_ffn = train_mm_early_IMU(train_loader, lr,40,model,path + 'FFN_IMUs_EMGs.pth')

mm_early_ffn= FFN(35)
mm_early_ffn.load_state_dict(torch.load(path+'FFN_IMUs_EMGs.pth'))
mm_early_ffn.to(device)

mm_early_ffn.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics, data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

        data_1D=torch.cat((data_1D,data_EMG),dim=-1)

        output= mm_early_ffn(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_17=np.hstack([rmse,p])

## LSTM

class Encoder(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder, self).__init__()
        self.lstm_1 = nn.GRU(input_dim, 512, bidirectional=False, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.GRU(512, 256, bidirectional=False, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)
        out_2=self.flatten(out_2)


        return out_2

class lstm_model(nn.Module):
    def __init__(self, input_1D):
        super(lstm_model, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)
        self.lstm = Encoder(input_1D,0.40)
        self.flatten=nn.Flatten()
        self.fc_f=nn.Linear(256*100, 7*100)

    def forward(self, inputs_1D_N):

        inputs_1D_N_1=inputs_1D_N.view(inputs_1D_N.size(0)*inputs_1D_N.size(1),inputs_1D_N.size(-1))
        inputs_1D_N_1=self.BN(inputs_1D_N_1)
        inputs_1D_N=inputs_1D_N_1.view(-1, 100, inputs_1D_N_1.size(-1))

        x=self.lstm(inputs_1D_N)
        x=self.flatten(x)
        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = lstm_model(35)

mm_early_lstm = train_mm_early_IMU(train_loader, lr, 40, model, path +'LSTM_IMUs_EMGs.pth')

mm_early_lstm= lstm_model(35)
mm_early_lstm.load_state_dict(torch.load(path+'LSTM_IMUs_EMGs.pth'))
mm_early_lstm.to(device)

mm_early_lstm.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader):

        data_1D=torch.cat((data_1D[:,:,0:18],data_1D[:,:,18:24]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

        data_1D=torch.cat((data_1D,data_EMG),dim=-1)

        output= mm_early_lstm(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data_1D, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_18=np.hstack([rmse,p])

## FFN (HF)

class FFN_HF(nn.Module):
    def __init__(self, input_1D):
        super(FFN_HF, self).__init__()

        self.BN= nn.BatchNorm1d(input_1D, affine=False)

        self.fc1 = nn.Linear(input_1D, 512)
        self.dropout1 = nn.Dropout(0.05)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.05)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(0.05)

        self.flatten=nn.Flatten()

        self.fc_f=nn.Linear(128, 7*100)


    def forward(self, inputs_1D_N):


        inputs_1D_N=self.BN(inputs_1D_N)

        x = F.relu(self.fc1(inputs_1D_N))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)

        x=self.flatten(x)

        x = self.fc_f(x).view(-1,100,7)

        return x


lr = 3e-4
model = FFN_HF(35*15)

mm_early_ffn_hf = train_mm_early_IMU_feat(train_loader, lr,40,model,path + 'FFN_HF_IMUs_EMGs.pth')

mm_early_ffn_hf= FFN_HF(35*15)
mm_early_ffn_hf.load_state_dict(torch.load(path+'FFN_HF_IMUs_EMGs.pth'))
mm_early_ffn_hf.to(device)

mm_early_ffn_hf.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data_1D,data_2D, data_acc, data_gyr, data_Kinematics,data_EMG,data_JP, target) in enumerate(test_loader_feat):

        data_1D=torch.cat((data_1D[:,0*15:18*15],data_1D[:,18*15:35*15]),dim=-1)
        data_2D=torch.cat((data_2D[:,:,:,0:3],data_2D[:,:,:,3:4]),dim=-1)

        output= mm_early_ffn_hf(data_1D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del  target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_target,s)

ablation_19=np.hstack([rmse,p])


##############################################################################################################################################


mm_result_IMU4=np.vstack([ablation_1,ablation_2,ablation_3,ablation_4,ablation_5,ablation_6,ablation_7,ablation_8,ablation_9,ablation_10,ablation_11,ablation_12,ablation_13,ablation_14,ablation_15,ablation_16,ablation_17,ablation_18,ablation_19])


path_1='/home/sanzidpr/Journal_3/Dataset_A_model_results_IMU_emg/Results/'

from numpy import savetxt
savetxt(path_1+subject+'_'+'group_1_sota_results.csv', mm_result_IMU4, delimiter=',')



