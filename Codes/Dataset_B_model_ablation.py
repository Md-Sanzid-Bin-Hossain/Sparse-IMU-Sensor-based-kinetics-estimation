# -*- coding: utf-8 -*-
"""Journal_3_Github_Dataset_B_Kinetics_Multi_modal_Estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M3GbJMoxryjEneRO18vPVXZLa0-Ex0kz
"""

import h5py
import json
import matplotlib.pyplot as plt
import numpy as np
import numpy
import statistics
from numpy import loadtxt
import matplotlib.pyplot as plt
import pandas
import math
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statistics import stdev
import math
import h5py

import numpy as np
import time

from scipy.signal import butter,filtfilt
import sys
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph
import seaborn as sns # used for plot interactive graph.
import pandas
import matplotlib.pyplot as plt

# from tsf.model import TransformerForecaster


# from tensorflow.keras.utils import np_utils
import itertools
###  Library for attention layers
import pandas as pd
import os
import numpy as np
#from tqdm import tqdm # Processing time measurement
from sklearn.model_selection import train_test_split

import statistics
import gc
import torch.nn.init as init

############################################################################################################################################################################
############################################################################################################################################################################

import os
import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.nn.utils.weight_norm as weight_norm


import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import torch.nn.functional as F
from torchsummary import summary
from torch.nn.parameter import Parameter


import torch.optim as optim


from tqdm import tqdm_notebook
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler


# Set random seeds for reproducibility
def set_seed(seed):
    random.seed(seed)  # Python random seed
    np.random.seed(seed)  # NumPy random seed
    torch.manual_seed(seed)  # PyTorch random seed
    torch.cuda.manual_seed(seed)  # PyTorch GPU random seed
    torch.cuda.manual_seed_all(seed)  # If using multi-GPU
    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior
    torch.backends.cudnn.benchmark = False  # Disable auto-tuner to find best algorithms

# Set your seed value
set_seed(42)



from google.colab import drive
drive.mount('/content/drive',force_remount=True)

"""# File path

# Data loader
"""

if __name__ == '__main__':
    with h5py.File('/content/drive/My Drive/public dataset/all_17_subjects.h5', 'r') as hf:
        data_all_sub = {subject: subject_data[:] for subject, subject_data in hf.items()}
        data_fields = json.loads(hf.attrs['columns'])

def data_extraction(A):
  for k in range(len(A)):
    zero_index_1=np.all(A[k:k+1,:,:] == 0, axis=0)
    zero_index = np.multiply(zero_index_1, 1)
    zero_index=np.array(zero_index)

    for i in range(len(zero_index)):
      if (sum(zero_index[i])==256):
        index=i
        break;

    # print(index)
### Taking only the stance phase of the gait
###################################################################################################################################################
    B=A[k:k+1,0:index,:]  ### Taking only the stance phase of the gait
    C_1=B.reshape((B.shape[0]*B.shape[1],B.shape[2]))
    if (k==0):
      C=C_1
    else:
      C=np.append(C,C_1,axis=0)

  index_24 = data_fields.index('body weight')
  index_25 = data_fields.index('body height')

  BW=(C[0:1, index_24]*9.8)
  BWH=(C[0:1, index_24]*9.8)*C[:, index_25]

  V=C[:,110:154]
  V=V.reshape(V.shape[0],11,4)

  V=(V-V[:,2:3,:])/C[0:1, index_25]
  V=V.reshape(-1,44)

      ### IMUs- Chest, Waist, Right Foot, Right shank, Right thigh, Left Foot, Left shank, Left thigh, 2D-body coordinate
    ### 0:48- IMU, 48:92-2D body coordinate, 92:97-- Target

  D=np.hstack((C[:,71:77],C[:,58:64],C[:,19:25],C[:,32:38],C[:,45:51],C[:,6:12],C[:,84:90],C[:,97:103],V,C[:,3:5],-C[:, 154:155]/BW,
              -C[:, 156:157]/BW,-C[:, 155:156]/BW))


  # D=np.hstack((C[:,70:76],C[:,57:63],C[:,18:24],C[:,31:37],C[:,44:50],C[:,5:11],C[:,83:89],C[:,96:102],C[:,109:153]))

  return D

# print(np.array(data_fields))

# index_21 = data_fields.index('plate_2_force_x')
# print(index_21)

data_subject_01 = data_all_sub['subject_01']
subject_1=data_extraction(data_subject_01)

print(subject_1.shape)

data_subject_01 = data_all_sub['subject_01']
data_subject_02 = data_all_sub['subject_02']
data_subject_03 = data_all_sub['subject_03']
data_subject_04 = data_all_sub['subject_04']
data_subject_05 = data_all_sub['subject_05']
data_subject_06 = data_all_sub['subject_06']
data_subject_07 = data_all_sub['subject_07']
data_subject_08 = data_all_sub['subject_08']
data_subject_09 = data_all_sub['subject_09']
data_subject_10 = data_all_sub['subject_10']
data_subject_11 = data_all_sub['subject_11']
data_subject_12 = data_all_sub['subject_12']
data_subject_13 = data_all_sub['subject_13']
data_subject_14 = data_all_sub['subject_14']
data_subject_15 = data_all_sub['subject_15']
data_subject_16 = data_all_sub['subject_16']
data_subject_17 = data_all_sub['subject_17']


subject_1=data_extraction(data_subject_01)
subject_2=data_extraction(data_subject_02)
subject_3=data_extraction(data_subject_03)
subject_4=data_extraction(data_subject_04)
subject_5=data_extraction(data_subject_05)
subject_6=data_extraction(data_subject_06)
subject_7=data_extraction(data_subject_07)
subject_8=data_extraction(data_subject_08)
subject_9=data_extraction(data_subject_09)
subject_10=data_extraction(data_subject_10)
subject_11=data_extraction(data_subject_11)
subject_12=data_extraction(data_subject_12)
subject_13=data_extraction(data_subject_13)
subject_14=data_extraction(data_subject_14)
subject_15=data_extraction(data_subject_15)
subject_16=data_extraction(data_subject_16)
subject_17=data_extraction(data_subject_17)

subject_1.shape

"""# Data processing"""

main_dir = "/content/drive/My Drive/public dataset/Public_dataset_2/Subject01"
# os.mkdir(main_dir)
path="/content/"
subject='Subject_01'

train_dataset=np.concatenate((subject_1,subject_2,subject_3,subject_4,subject_5,
                              subject_6,subject_7,subject_8,subject_9,subject_10,subject_11,subject_12,subject_13,subject_14,subject_15,subject_17),axis=0)

test_dataset=subject_16

encoder='CNN'

# Train features #

x_train=train_dataset[:,0:92]


scale= StandardScaler()
scaler = MinMaxScaler(feature_range=(0, 1))
train_X_1_1=x_train


x_test=test_dataset[:,0:92]

test_X_1_1=x_test

m1=92
m2=97


  ### Label ###

train_y_1_1=train_dataset[:,m1:m2]
test_y_1_1=test_dataset[:,m1:m2]

train_dataset_1=np.concatenate((train_X_1_1,train_y_1_1),axis=1)
test_dataset_1=np.concatenate((test_X_1_1,test_y_1_1),axis=1)

train_dataset_1=pd.DataFrame(train_dataset_1)
test_dataset_1=pd.DataFrame(test_dataset_1)

train_dataset_1.dropna(axis=0,inplace=True)
test_dataset_1.dropna(axis=0,inplace=True)

train_dataset_1=np.array(train_dataset_1)
test_dataset_1=np.array(test_dataset_1)

train_dataset_sum = np. sum(train_dataset_1)
array_has_nan = np. isinf(train_dataset_1[:,48:92])

print(array_has_nan)

print(train_dataset_1.shape)



train_X_1=train_dataset_1[:,0:m1]
test_X_1=test_dataset_1[:,0:m1]

train_y_1=train_dataset_1[:,m1:m2]
test_y_1=test_dataset_1[:,m1:m2]



L1=len(train_X_1)
L2=len(test_X_1)


w=50



a1=L1//w
b1=L1%w

a2=L2//w
b2=L2%w



     #### Features ####
train_X_2=train_X_1[L1-w+b1:L1,:]
test_X_2=test_X_1[L2-w+b2:L2,:]
# validation_X_2=validation_X_1[L3-w+b3:L3,:]


    #### Output ####

train_y_2=train_y_1[L1-w+b1:L1,:]
test_y_2=test_y_1[L2-w+b2:L2,:]
# validation_y_2=validation_y_1[L3-w+b3:L3,:]



     #### Features ####

train_X=np.concatenate((train_X_1,train_X_2),axis=0)
test_X=np.concatenate((test_X_1,test_X_2),axis=0)
# validation_X=np.concatenate((validation_X_1,validation_X_2),axis=0)


    #### Output ####

train_y=np.concatenate((train_y_1,train_y_2),axis=0)
test_y=np.concatenate((test_y_1,test_y_2),axis=0)
# validation_y=np.concatenate((validation_y_1,validation_y_2),axis=0)


print(train_y.shape)
    #### Reshaping ####
train_X_3_p= train_X.reshape((a1+1,w,train_X.shape[1]))
test_X = test_X.reshape((a2+1,w,test_X.shape[1]))


train_y_3_p= train_y.reshape((a1+1,w,5))
test_y= test_y.reshape((a2+1,w,5))



test_X_1D=test_X

train_X_3=train_X_3_p
train_y_3=train_y_3_p


train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)

print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)

features=6



Bag_samples=train_X_1D.shape[0]
print(Bag_samples)

s=test_X_1D.shape[0]*w

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

### IMUs- Chest, Waist, Right Foot, Right shank, Right thigh, Left Foot, Left shank, Left thigh, 2D-body coordinate
### 0:48- IMU, 48:92-2D body coordinate, 92:97-- Target


### Data Processing

batch_size = 64

val_targets = torch.Tensor(Y_validation)
test_features = torch.Tensor(test_X_1D)
test_targets = torch.Tensor(test_y)


## all Modality Features

train_features = torch.Tensor(train_X_1D)
train_targets = torch.Tensor(train_y_5)
val_features = torch.Tensor(X_validation_1D)


train_features_acc_8=torch.cat((train_features[:,:,0:3],train_features[:,:,6:9],train_features[:,:,12:15],train_features[:,:,18:21],train_features[:,:,24:27]\
                             ,train_features[:,:,30:33],train_features[:,:,36:39],train_features[:,:,42:45]),axis=-1)
test_features_acc_8=torch.cat((test_features[:,:,0:3],test_features[:,:,6:9],test_features[:,:,12:15],test_features[:,:,18:21],test_features[:,:,24:27]\
                             ,test_features[:,:,30:33],test_features[:,:,36:39],test_features[:,:,42:45]),axis=-1)
val_features_acc_8=torch.cat((val_features[:,:,0:3],val_features[:,:,6:9],val_features[:,:,12:15],val_features[:,:,18:21],val_features[:,:,24:27]\
                             ,val_features[:,:,30:33],val_features[:,:,36:39],val_features[:,:,42:45]),axis=-1)


train_features_gyr_8=torch.cat((train_features[:,:,3:6],train_features[:,:,9:12],train_features[:,:,15:18],train_features[:,:,21:24],train_features[:,:,27:30]\
                             ,train_features[:,:,33:36],train_features[:,:,39:42],train_features[:,:,45:48]),axis=-1)
test_features_gyr_8=torch.cat((test_features[:,:,3:6],test_features[:,:,9:12],test_features[:,:,15:18],test_features[:,:,21:24],test_features[:,:,27:30]\
                             ,test_features[:,:,33:36],test_features[:,:,39:42],test_features[:,:,45:48]),axis=-1)
val_features_gyr_8=torch.cat((val_features[:,:,3:6],val_features[:,:,9:12],val_features[:,:,15:18],val_features[:,:,21:24],val_features[:,:,27:30]\
                             ,val_features[:,:,33:36],val_features[:,:,39:42],val_features[:,:,45:48]),axis=-1)



train_features_2D_point=train_features[:,:,48:92]
test_features_2D_point=test_features[:,:,48:92]
val_features_2D_point=val_features[:,:,48:92]





train = TensorDataset(train_features, train_features_acc_8,train_features_gyr_8, train_features_2D_point, train_targets)
val = TensorDataset(val_features, val_features_acc_8, val_features_gyr_8, val_features_2D_point,val_targets)
test = TensorDataset(test_features, test_features_acc_8, test_features_gyr_8, test_features_2D_point, test_targets)

train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=False)
val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, drop_last=False)
test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, drop_last=False)

"""# Important Functions"""

def RMSE_prediction(yhat_4,test_y,s):

  s1=yhat_4.shape[0]*yhat_4.shape[1]

  test_o=test_y.reshape((s1,5))
  yhat=yhat_4.reshape((s1,5))




  y_1_no=yhat[:,0]
  y_2_no=yhat[:,1]
  y_3_no=yhat[:,2]
  y_4_no=yhat[:,3]
  y_5_no=yhat[:,4]



  y_1=y_1_no
  y_2=y_2_no
  y_3=y_3_no
  y_4=y_4_no
  y_5=y_5_no



  y_test_1=test_o[:,0]
  y_test_2=test_o[:,1]
  y_test_3=test_o[:,2]
  y_test_4=test_o[:,3]
  y_test_5=test_o[:,4]





  Z_1=y_1
  Z_2=y_2
  Z_3=y_3
  Z_4=y_4
  Z_5=y_5




  ###calculate RMSE

  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100



  print(rmse_1)
  print(rmse_2)
  print(rmse_3)
  print(rmse_4)
  print(rmse_5)


  p_1=np.corrcoef(y_1, y_test_1)[0, 1]
  p_2=np.corrcoef(y_2, y_test_2)[0, 1]
  p_3=np.corrcoef(y_3, y_test_3)[0, 1]
  p_4=np.corrcoef(y_4, y_test_4)[0, 1]
  p_5=np.corrcoef(y_5, y_test_5)[0, 1]


  print("\n")
  print(p_1)
  print(p_2)
  print(p_3)
  print(p_4)
  print(p_5)


              ### Correlation ###
  p=np.array([p_1,p_2,p_3,p_4,p_5])




      #### Mean and standard deviation ####

  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])

      #### Mean and standard deviation ####
  m=statistics.mean(rmse)
  SD=statistics.stdev(rmse)
  print('Mean: %.3f' % m,'+/- %.3f' %SD)

  m_c=statistics.mean(p)
  SD_c=statistics.stdev(p)
  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)



  return rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################
def RMSE_prediction_WN(yhat_4,test_y,s):

  test_o=test_y.reshape((s,5))
  yhat=yhat_4.reshape((s,5))




  y_1_no=yhat[:,0]
  y_2_no=yhat[:,1]
  y_3_no=yhat[:,2]
  y_4_no=yhat[:,3]
  y_5_no=yhat[:,4]





  y_test_1=test_o[:,0]
  y_test_2=test_o[:,1]
  y_test_3=test_o[:,2]
  y_test_4=test_o[:,3]
  y_test_5=test_o[:,4]







  Z_1=y_1
  Z_2=y_2
  Z_3=y_3
  Z_4=y_4
  Z_5=y_5



  ###calculate RMSE

  rmse_1 =np.sqrt(mean_squared_error(y_test_1,y_1))
  rmse_2 =np.sqrt(mean_squared_error(y_test_2,y_2))
  rmse_3 =np.sqrt(mean_squared_error(y_test_3,y_3))
  rmse_4 =np.sqrt(mean_squared_error(y_test_4,y_4))
  rmse_5 =np.sqrt(mean_squared_error(y_test_5,y_5))



  print(rmse_1)
  print(rmse_2)
  print(rmse_3)
  print(rmse_4)
  print(rmse_5)



  p_1=np.corrcoef(y_1, y_test_1)[0, 1]
  p_2=np.corrcoef(y_2, y_test_2)[0, 1]
  p_3=np.corrcoef(y_3, y_test_3)[0, 1]
  p_4=np.corrcoef(y_4, y_test_4)[0, 1]
  p_5=np.corrcoef(y_5, y_test_5)[0, 1]


  print("\n")
  print(p_1)
  print(p_2)
  print(p_3)
  print(p_4)
  print(p_5)



              ### Correlation ###
  p=np.array([p_1,p_2,p_3,p_4,p_5])




      #### Mean and standard deviation ####

  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])

      #### Mean and standard deviation ####
  m=statistics.mean(rmse)
  SD=statistics.stdev(rmse)
  print('Mean: %.3f' % m,'+/- %.3f' %SD)

  m_c=statistics.mean(p)
  SD_c=statistics.stdev(p)
  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)



  return rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


def PCC_prediction(yhat_4,test_y,s):

  test_o=test_y.reshape((s,5))
  yhat=yhat_4.reshape((s,5))



  y_1_no=yhat[:,0]
  y_2_no=yhat[:,1]
  y_3_no=yhat[:,2]
  y_4_no=yhat[:,3]
  y_5_no=yhat[:,4]


  y_test_1=test_o[:,0]
  y_test_2=test_o[:,1]
  y_test_3=test_o[:,2]
  y_test_4=test_o[:,3]
  y_test_5=test_o[:,4]



  Y_1=y_1
  Y_2=y_2
  Y_3=y_3
  Y_4=y_4
  Y_5=y_5
  # Y_6=y_6
  # Y_7=y_7


  ###calculate RMSE

  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100



  print(rmse_1)
  print(rmse_2)
  print(rmse_3)
  print(rmse_4)
  print(rmse_5)



  p_1=np.corrcoef(y_1, y_test_1)[0, 1]
  p_2=np.corrcoef(y_2, y_test_2)[0, 1]
  p_3=np.corrcoef(y_3, y_test_3)[0, 1]
  p_4=np.corrcoef(y_4, y_test_4)[0, 1]
  p_5=np.corrcoef(y_5, y_test_5)[0, 1]


  print("\n")
  print(p_1)
  print(p_2)
  print(p_3)
  print(p_4)
  print(p_5)



              ### Correlation ###
  p=np.array([p_1,p_2,p_3,p_4,p_5])




      #### Mean and standard deviation ####

  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])

      #### Mean and standard deviation ####
  m=statistics.mean(rmse)
  SD=statistics.stdev(rmse)
  print('Mean: %.3f' % m,'+/- %.3f' %SD)

  m_c=statistics.mean(p)
  SD_c=statistics.stdev(p)
  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)


  return rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)

    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)

    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x

    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x

    return (b_0, b_1)


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################



def DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5):

  a_1,b_1=estimate_coef(Y_1,Z_1)
  a_2,b_2=estimate_coef(Y_2,Z_2)
  a_3,b_3=estimate_coef(Y_3,Z_3)
  a_4,b_4=estimate_coef(Y_4,Z_4)
  a_5,b_5=estimate_coef(Y_5,Z_5)

  #### All 16 angles prediction  ####


  test_o=test_y.reshape((s,5))
  yhat=yhat_4.reshape((s,5))


  y_1=yhat[:,0]
  y_2=yhat[:,1]
  y_3=yhat[:,2]
  y_4=yhat[:,3]
  y_5=yhat[:,4]



  y_test_1=test_o[:,0]
  y_test_2=test_o[:,1]
  y_test_3=test_o[:,2]
  y_test_4=test_o[:,3]
  y_test_5=test_o[:,4]



  y_1=y_1*b_1+a_1
  y_2=y_2*b_2+a_2
  y_3=y_3*b_3+a_3
  y_4=y_4*b_4+a_4
  y_5=y_5*b_5+a_5


  ###calculate RMSE

  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100




  print(rmse_1)
  print(rmse_2)
  print(rmse_3)
  print(rmse_4)
  print(rmse_5)



  p_1=np.corrcoef(y_1, y_test_1)[0, 1]
  p_2=np.corrcoef(y_2, y_test_2)[0, 1]
  p_3=np.corrcoef(y_3, y_test_3)[0, 1]
  p_4=np.corrcoef(y_4, y_test_4)[0, 1]
  p_5=np.corrcoef(y_5, y_test_5)[0, 1]



  print("\n")
  print(p_1)
  print(p_2)
  print(p_3)
  print(p_4)
  print(p_5)



              ### Correlation ###
  p=np.array([p_1,p_2,p_3,p_4,p_5])



      #### Mean and standard deviation ####

  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])

      #### Mean and standard deviation ####
  m=statistics.mean(rmse)
  SD=statistics.stdev(rmse)
  print('Mean: %.3f' % m,'+/- %.3f' %SD)

  m_c=statistics.mean(p)
  SD_c=statistics.stdev(p)
  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

  return rmse, p





############################################################################################################################################################################################################################################################################################################################################################################################################################################################################

# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
is_cuda = torch.cuda.is_available()

# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
if is_cuda:
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

class RMSELoss(nn.Module):
    def __init__(self):
        super(RMSELoss, self).__init__()

    def forward(self, pred, target):
        mse = nn.MSELoss()(pred, target)
        rmse = torch.sqrt(mse)
        return rmse

"""# 8 IMUS

## Training Function
"""

def train_mm_early(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    criterion =RMSELoss()

    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data, data_acc, data_gyr, data_2D, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output= model(data[:,:,0:48].to(device).float())

            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data, data_acc, data_gyr, data_2D, target in val_loader:
                output= model(data[:,:,0:48].to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break



    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_m(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer

    criterion =RMSELoss()

    optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data, data_acc, data_gyr, data_2D, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output= model(data_acc.to(device).float(),data_gyr.to(device).float())

            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data, data_acc, data_gyr, data_2D,  target in val_loader:
                output= model(data_acc.to(device).float(),data_gyr.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        torch.set_printoptions(precision=4)

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break



    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")


    return model

"""## Single Encoder

### Encoders
"""

class Encoder(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder, self).__init__()
        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.fc = nn.Linear(128, 32)
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)

        return out_2

"""### Early Fusion"""

class MM_early(nn.Module):

    def __init__(self, input, drop_prob=0.25):
        super(MM_early, self).__init__()
        self.encoder_input=Encoder(input,drop_prob)
        self.fc = nn.Linear(128, 5)
        self.BN= nn.BatchNorm1d(input, affine=False)

    def forward(self, input_x):

        input_x_1=input_x.view(input_x.size(0)*input_x.size(1),input_x.size(-1))
        input_x_1=self.BN(input_x_1)
        input_x_2=input_x_1.view(-1, 50, input_x_1.size(-1))
        out=self.encoder_input(input_x_2)

        out = self.fc(out)

        return out

lr = 0.001
model = MM_early(48)

mm_early = train_mm_early(train_loader, lr,40,model,path + encoder + '_early_IMU8.pth')

mm_early= MM_early(48)
mm_early.load_state_dict(torch.load(path+encoder+'_early_IMU8.pth'))
mm_early.to(device)

mm_early.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,target) in enumerate(test_loader):
        output= mm_early(data[:,:,0:48].to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()



yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_1=np.hstack([rmse,p])

"""### Feature Concatentaion"""

class MM_concat(nn.Module):
    def __init__(self, input_acc, input_gyr, drop_prob=0.25):
        super(MM_concat, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc = nn.Linear(2*128, 5)

    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        out = self.fc(x)

        return out

lr = 0.001
model = MM_concat(24,24)

mm_concat = train_mm_m(train_loader, lr,40,model,path+encoder+'_concat_IMU8.pth')

mm_concat= MM_concat(24,24)
mm_concat.load_state_dict(torch.load(path+encoder+'_concat_IMU8.pth'))
mm_concat.to(device)

mm_concat.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_concat(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_2=np.hstack([rmse,p])

"""### Tensor Fusion with Multiplication"""

class MM_wfs(nn.Module):
    def __init__(self, input_acc, input_gyr, dropout=0.10):
        super(MM_wfs, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc = nn.Linear(128, 5)

               # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())


    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        out=x_1+x_2

        out = self.fc(out)

        return out

lr = 0.001
model = MM_wfs(24,24)
mm_wfs= train_mm_m(train_loader, lr,40,model,path+encoder+'_wfs_IMU8.pth')

mm_wfs= MM_wfs(24,24)
mm_wfs.load_state_dict(torch.load(path+encoder+'_wfs_IMU8.pth'))
mm_wfs.to(device)

mm_wfs.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_wfs(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_4=np.hstack([rmse,p])

"""### Gated Multi-modal fusion"""

class MM_gmf(nn.Module):
    def __init__(self, input_acc, input_gyr,  dropout=0.25):
        super(MM_gmf, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc = nn.Linear(2*128, 5)

        self.dropout=nn.Dropout(p=0.05)

        # Define the gating network
        self.gating_net = nn.Sequential(
            nn.Linear(128*2, 2*128),
            nn.Sigmoid()
        )



    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        gating_weights = self.gating_net(x)

        out=gating_weights*x
        out = self.fc(out)


        return out

lr = 0.001
model = MM_gmf(24,24)

mm_gmf= train_mm_m(train_loader, lr,40,model,path+encoder+ '_gmf_IMU8.pth')

mm_gmf= MM_gmf(24,24)
mm_gmf.load_state_dict(torch.load(path+encoder+'_gmf_IMU8.pth'))
mm_gmf.to(device)

mm_gmf.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_gmf(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_5=np.hstack([rmse,p])

"""### Multi-Head self Attention Module"""

class MM_mha(nn.Module):
    def __init__(self, input_acc, input_gyr, dropout=0.25):
        super(MM_mha, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc = nn.Linear(2*128, 5)

        self.dropout=nn.Dropout(p=0.05)

        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)



    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        out, attn_output_weights=self.attention(x,x,x)

        out = self.fc(out)

        return out

lr = 0.001
model = MM_mha(24,24)

mm_mha = train_mm_m(train_loader, lr,40,model,path+encoder+'_mha_IMU8.pth')

mm_mha= MM_mha(24,24)
mm_mha.load_state_dict(torch.load(path+encoder+'_mha_IMU8.pth'))
mm_mha.to(device)

mm_mha.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_6=np.hstack([rmse,p])

"""### MHA+Weighted Feature Fusion"""

class MM_mha_wf(nn.Module):
    def __init__(self, input_acc, input_gyr, dropout=0.25):
        super(MM_mha_wf, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc = nn.Linear(2*2*128,5)

        self.dropout=nn.Dropout(p=0.05)

        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)


        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())



    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x


        out=torch.cat((out_1,out_2),dim=-1)


        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wf(24,24)

mm_mha_wf = train_mm_m(train_loader, lr,40,model,path+encoder+'_mha_wf_IMU8.pth')

mm_mha_wf= MM_mha_wf(24,24)
mm_mha_wf.load_state_dict(torch.load(path+encoder+'_mha_wf_IMU8.pth'))
mm_mha_wf.to(device)

mm_mha_wf.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wf(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_7=np.hstack([rmse,p])

"""### Weighted Fusion of MHA+Weighted Feature Fusion"""

class MM_mha_wf_fusion(nn.Module):
    def __init__(self, input_acc, input_gyr, dropout=0.25):
        super(MM_mha_wf_fusion, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc_1 = nn.Linear(2*128, 128)
        self.fc_2 = nn.Linear(128, 64)
        self.fc = nn.Linear(2*2*128,5)

        self.dropout=nn.Dropout(p=0.05)

        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)


        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128, 2*2*128), nn.Sigmoid())



    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        out=torch.cat((out_1,out_2),dim=-1)

        gating_weights_1 = self.gating_net_1(out)
        out=gating_weights_1*out

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wf_fusion(24,24)

mm_mha_wf_fusion = train_mm_m(train_loader, lr,40,model,path+encoder+'_mha_wf_fusion_IMU8.pth')

mm_mha_wf_fusion= MM_mha_wf_fusion(24,24)
mm_mha_wf_fusion.load_state_dict(torch.load(path+encoder+'_mha_wf_fusion_IMU8.pth'))
mm_mha_wf_fusion.to(device)

mm_mha_wf_fusion.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wf_fusion(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_8=np.hstack([rmse,p])

"""### MHA+Tensor Multiplication"""

class MM_mha_wfs(nn.Module):
    def __init__(self, input_acc, input_gyr, dropout=0.25):
        super(MM_mha_wfs, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc_1 = nn.Linear(2*128, 128)
        self.fc_2 = nn.Linear(128, 64)
        self.fc = nn.Linear(2*128+128,5)

        self.dropout=nn.Dropout(p=0.05)

        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)


        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())

                # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())


    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)


        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        out_3=x_1+x_2


        out=torch.cat((out_1,out_3),dim=-1)

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wfs(24,24)

mm_mha_wfs = train_mm_m(train_loader, lr,40,model,path+encoder+'_mha_wfs_IMU8.pth')

mm_mha_wfs= MM_mha_wfs(24,24)
mm_mha_wfs.load_state_dict(torch.load(path+encoder+'_mha_wfs_IMU8.pth'))
mm_mha_wfs.to(device)

mm_mha_wfs.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wfs(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_9=np.hstack([rmse,p])

"""### Weighted Fusion of MHA+Tensor Multiplication"""

class MM_mha_wfs_fusion(nn.Module):
    def __init__(self, input_acc, input_gyr, dropout=0.25):
        super(MM_mha_wfs_fusion, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc_1 = nn.Linear(2*128, 128)
        self.fc_2 = nn.Linear(128, 64)
        self.fc = nn.Linear(2*128+128,5)

        self.dropout=nn.Dropout(p=0.05)

        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)

                # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())


        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*128+128, 2*128+128), nn.Sigmoid())



    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)


        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        out_3=x_1+x_2




        out=torch.cat((out_1,out_3),dim=-1)

        gating_weights_1 = self.gating_net_1(out)
        out=gating_weights_1*out

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wfs_fusion(24,24)

mm_mha_wfs_fusion = train_mm_m(train_loader, lr,40,model,path+encoder+'_mha_wfs_fusion_IMU8.pth')

mm_mha_wfs_fusion= MM_mha_wfs_fusion(24,24)
mm_mha_wfs_fusion.load_state_dict(torch.load(path+encoder+'_mha_wfs_fusion_IMU8.pth'))
mm_mha_wfs_fusion.to(device)

mm_mha_wfs_fusion.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wfs_fusion(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_10=np.hstack([rmse,p])

"""### Weighted Features + Tensor Multiplication"""

class MM_wf_wfs(nn.Module):
    def __init__(self, input_acc, input_gyr, dropout=0.25):
        super(MM_wf_wfs, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc_1 = nn.Linear(2*128, 128)
        self.fc_2 = nn.Linear(128, 64)
        self.fc = nn.Linear(2*128+128,5)

        self.dropout=nn.Dropout(p=0.05)

        # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())


        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)


        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())



    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        out_3=x_1+x_2


        out=torch.cat((out_2,out_3),dim=-1)

        out=self.fc(out)

        return out

lr = 0.001
model = MM_wf_wfs(24,24)

mm_wf_wfs = train_mm_m(train_loader, lr,40,model,path+encoder+'_wf_wfs_IMU8.pth')

mm_wf_wfs= MM_wf_wfs(24,24)
mm_wf_wfs.load_state_dict(torch.load(path+encoder+'_wf_wfs_IMU8.pth'))
mm_wf_wfs.to(device)

mm_wf_wfs.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_wf_wfs(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_11=np.hstack([rmse,p])

"""### Weighted Fusion of weighted features +Tensor Multiplication"""

class MM_wf_wfs_fusion(nn.Module):
    def __init__(self, input_acc, input_gyr, dropout=0.25):
        super(MM_wf_wfs_fusion, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc_1 = nn.Linear(2*128, 128)
        self.fc_2 = nn.Linear(128, 64)
        self.fc = nn.Linear(2*128+128,5)

        self.dropout=nn.Dropout(p=0.05)
                # Define the gating network

        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())


        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)


        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*128+128, 2*128+128), nn.Sigmoid())



    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)


        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        out_3=x_1+x_2




        out=torch.cat((out_2,out_3),dim=-1)

        gating_weights_1 = self.gating_net_1(out)
        out=gating_weights_1*out

        out=self.fc(out)

        return out

lr = 0.001
model = MM_wf_wfs_fusion(24,24)

mm_wf_wfs_fusion = train_mm_m(train_loader, lr,40,model,path+encoder+'_wf_wfs_fusion_IMU8.pth')

mm_wf_wfs_fusion= MM_wf_wfs_fusion(24,24)
mm_wf_wfs_fusion.load_state_dict(torch.load(path+encoder+'_wf_wfs_fusion_IMU8.pth'))
mm_wf_wfs_fusion.to(device)

mm_wf_wfs_fusion.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_wf_wfs_fusion(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_12=np.hstack([rmse,p])

"""### MHA+Weighted Feature+ Tensor Multiplication Fusion"""

class MM_mha_wf_wfs(nn.Module):
    def __init__(self, input_acc, input_gyr, dropout=0.25):
        super(MM_mha_wf_wfs, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc_1 = nn.Linear(2*128, 128)
        self.fc_2 = nn.Linear(128, 64)
        self.fc = nn.Linear(2*2*128+128,5)

        # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())



        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)


        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())



    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        out_3=x_1+x_2


        out=torch.cat((out_1,out_2,out_3),dim=-1)

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wf_wfs(24,24)

mm_mha_wf_wfs = train_mm_m(train_loader, lr,40,model,path+encoder+'_mha_wf_wfs_IMU8.pth')

mm_mha_wf_wfs= MM_mha_wf_wfs(24,24)
mm_mha_wf_wfs.load_state_dict(torch.load(path+encoder+'_mha_wf_wfs_IMU8.pth'))
mm_mha_wf_wfs.to(device)

mm_mha_wf_wfs.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wf_wfs(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_13=np.hstack([rmse,p])

"""### Weighted Fusion of MHA+Weighted Feature+ Tensor Multiplication Fusion"""

class MM_mha_wf_wfs_fusion(nn.Module):
    def __init__(self, input_acc, input_gyr, dropout=0.25):
        super(MM_mha_wf_wfs_fusion, self).__init__()

        self.encoder_acc=Encoder(input_acc, dropout)
        self.encoder_gyr=Encoder(input_gyr, dropout)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)

        self.fc = nn.Linear(2*2*128+128,5)

        self.dropout=nn.Dropout(p=0.05)
                # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())

        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)

        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())



    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)

        x=torch.cat((x_acc,x_gyr),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        out_3=x_1+x_2


        out=torch.cat((out_1,out_2,out_3),dim=-1)

        gating_weights_1 = self.gating_net_1(out)
        out=gating_weights_1*out

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wf_wfs_fusion(24,24)

mm_mha_wf_wfs_fusion = train_mm_m(train_loader, lr,40,model,path+encoder+'_mha_wf_wfs_fusion_IMU8.pth')

mm_mha_wf_wfs_fusion= MM_mha_wf_wfs_fusion(24,24)
mm_mha_wf_wfs_fusion.load_state_dict(torch.load(path+encoder+'_mha_wf_wfs_fusion_IMU8.pth'))
mm_mha_wf_wfs_fusion.to(device)

mm_mha_wf_wfs_fusion.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wf_wfs_fusion(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_14=np.hstack([rmse,p])

"""## Multi_Encoder Fusion"""

class Encoder_1(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder_1, self).__init__()
        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.fc = nn.Linear(128, 32)
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)

        return out_2


class Encoder_2(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder_2, self).__init__()
        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.fc = nn.Linear(128, 32)
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)

        return out_2

class ME_mha_wf_wfs_fusion(nn.Module):
    def __init__(self, input_acc, input_gyr,hidden_size, dropout=0.25):
        super(ME_mha_wf_wfs_fusion, self).__init__()

        self.encoder_1_acc=Encoder_1(input_acc, dropout)
        self.encoder_1_gyr=Encoder_1(input_gyr, dropout)

        self.encoder_2_acc=Encoder_2(input_acc, dropout)
        self.encoder_2_gyr=Encoder_2(input_gyr, dropout)


        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)


        self.fc = nn.Linear(4*2*hidden_size+2*hidden_size,5)


        self.attention=nn.MultiheadAttention(4*hidden_size,4,batch_first=True)


        self.gating_net = nn.Sequential(nn.Linear(hidden_size*4, 4*hidden_size), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(4*2*hidden_size+2*hidden_size, 4*2*hidden_size+2*hidden_size), nn.Sigmoid())

        self.attention=nn.MultiheadAttention(4*hidden_size,4,batch_first=True)

        self.gating_net_acc = nn.Sequential(nn.Linear(2*hidden_size, 2*hidden_size), nn.Sigmoid())
        self.gating_net_gyr = nn.Sequential(nn.Linear(2*hidden_size, 2*hidden_size), nn.Sigmoid())


        self.fc_out = nn.Linear(2*hidden_size, 2*hidden_size)
        self.fc_out_last = nn.Linear(4*2*hidden_size+2*hidden_size, 4*2*hidden_size+2*hidden_size)
        self.fc_out_1 = nn.Linear(hidden_size*4, hidden_size*4)



    def forward(self, x_acc, x_gyr):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))


        x_acc_1=self.encoder_1_acc(x_acc_2)
        x_gyr_1=self.encoder_1_gyr(x_gyr_2)

        x_acc_2=self.encoder_2_acc(x_acc_2)
        x_gyr_2=self.encoder_2_gyr(x_gyr_2)

        x_acc=torch.cat((x_acc_1,x_acc_2),dim=-1)

        x_gyr=torch.cat((x_gyr_1,x_gyr_2),dim=-1)

        gating_weights_acc= self.gating_net_acc(x_acc)
        x_acc=gating_weights_acc*x_acc

        gating_weights_gyr= self.gating_net_gyr(x_gyr)
        x_gyr=gating_weights_gyr*x_gyr



        x=torch.cat((x_acc,x_gyr),dim=-1)


        out_1, attn_output_weights=self.attention(x,x,x)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        out_3=x_acc*x_gyr

        out=torch.cat((out_1,out_2,out_3),dim=-1)

        gating_weights_1 = self.gating_net_1(out)
        out=gating_weights_1*out



        out=self.fc(out)

        return out

lr = 0.001
model = ME_mha_wf_wfs_fusion(24, 24)


me_mha_wf_wfs_fusion = train_mm_m(train_loader, lr,40,model,path+encoder+'_me_mha_wf_wfs_fusion_IMU8.pth')

me_mha_wf_wfs_fusion= ME_mha_wf_wfs_fusion(24,24)
me_mha_wf_wfs_fusion.load_state_dict(torch.load(path+encoder+'_me_mha_wf_wfs_fusion_IMU8.pth'))
me_mha_wf_wfs_fusion.to(device)

me_mha_wf_wfs_fusion.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = me_mha_wf_wfs_fusion(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_15=np.hstack([rmse,p])

"""# 8 IMUS+ 2D point from video

## Training Function
"""

def train_mm_early(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()

    # Defining loss function and optimizer
    criterion =RMSELoss()

    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)


    running_loss=0

    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data, data_acc, data_gyr, data_2D, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output= model(data[:,:,0:92].to(device).float())


            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()


            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data, data_acc, data_gyr, data_2D, target in val_loader:
                output= model(data[:,:,0:92].to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break



    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")



    return model

def train_mm_m(train_loader, learn_rate, EPOCHS, model,filename):

    if torch.cuda.is_available():
      model.cuda()
    # Defining loss function and optimizer
    criterion =RMSELoss()


    optimizer = torch.optim.Adam(model.parameters())


    running_loss=0
    # Train the model
    start_time = time.time()

    # Train the model with early stopping
    best_val_loss = float('inf')
    patience = 10


    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        for i, (data, data_acc, data_gyr, data_2D, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output= model(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())

            loss = criterion(output, target.to(device).float())
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        train_loss=running_loss/len(train_loader)

       # Validate
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data, data_acc, data_gyr, data_2D,  target in val_loader:
                output= model(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
                val_loss += criterion(output, target.to(device).float())

        val_loss /= len(val_loader)

        epoch_end_time = time.time()
        epoch_training_time = epoch_end_time - epoch_start_time

        torch.set_printoptions(precision=4)

        print(f"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}")

        running_loss=0

        epoch_end_time = time.time()

                # Check if the validation loss has improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), filename)
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping if the validation loss hasn't improved for `patience` epochs
        if patience_counter >= patience:
            print(f"Stopping early after {epoch+1} epochs")
            break



    end_time = time.time()

    training_time = end_time - start_time
    print(f"Training time: {training_time} seconds")


    return model

"""## Encoder

### Encoder
"""

class Encoder(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder, self).__init__()
        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)

        return out_2

"""### Early Fusion"""

class MM_early(nn.Module):

    def __init__(self, input, drop_prob=0.25):
        super(MM_early, self).__init__()
        self.encoder_input=Encoder(input,drop_prob)
        self.fc = nn.Linear(128, 5)
        self.BN= nn.BatchNorm1d(input, affine=False)

    def forward(self, input_x):

        input_x_1=input_x.view(input_x.size(0)*input_x.size(1),input_x.size(-1))
        input_x_1=self.BN(input_x_1)
        input_x_2=input_x_1.view(-1, 50, input_x_1.size(-1))
        out=self.encoder_input(input_x_2)

        out = self.fc(out)

        return out

lr = 0.001
model = MM_early(92)

mm_early = train_mm_early(train_loader, lr,40,model,path + encoder + '_early_IMU8_2D.pth')

mm_early= MM_early(92)
mm_early.load_state_dict(torch.load(path+encoder+'_early_IMU8_2D.pth'))
mm_early.to(device)

mm_early.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,target) in enumerate(test_loader):
        output= mm_early(data[:,:,0:92].to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()



yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_1=np.hstack([rmse,p])

"""### Feature Concatentaion"""

class MM_concat(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(MM_concat, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)


        self.fc = nn.Linear(3*128, 5)

    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)

        out = self.fc(x)

        return out

lr = 0.001
model = MM_concat(24,24,44)

mm_concat = train_mm_m(train_loader, lr,40,model,path+encoder+'_concat_IMU8_2D.pth')

mm_concat= MM_concat(24,24,44)
mm_concat.load_state_dict(torch.load(path+encoder+'_concat_IMU8_2D.pth'))
mm_concat.to(device)

mm_concat.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_concat(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_2=np.hstack([rmse,p])

"""### Tensor Fusion with Feature Multiplication"""

class MM_wfs(nn.Module):
    def __init__(self, input_acc, input_gyr, input_2D, drop_prob=0.10):
        super(MM_wfs, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())

        self.fc = nn.Linear(128, 5)

    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        x_3=weights_3*x[:,:,2*128:3*128]
        out=x_1+x_2+x_3


        out = self.fc(out)

        return out

lr = 0.001
model = MM_wfs(24,24,44)
mm_wfs= train_mm_m(train_loader, lr,40,model,path+encoder+'_wfs_IMU8_2D.pth')

mm_wfs= MM_wfs(24,24,44)
mm_wfs.load_state_dict(torch.load(path+encoder+'_wfs_IMU8_2D.pth'))
mm_wfs.to(device)

mm_wfs.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_wfs(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_4=np.hstack([rmse,p])

"""### Weighted Feature Fusion"""

class MM_wff(nn.Module):
    def __init__(self, input_acc, input_gyr, input_2D, drop_prob=0.40):
        super(MM_wff, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(3*128, 5)

        self.dropout=nn.Dropout(p=0.05)

        # Define the gating network
        self.gating_net = nn.Sequential(
            nn.Linear(128 * 3, 3*128),
            nn.Sigmoid()
        )



    def forward(self, x_acc, x_gyr,x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)

        gating_weights = self.gating_net(x)

        out=gating_weights*x

        out = self.fc(out)


        return out

lr = 0.001
model = MM_wff(24,24,44)

mm_wff= train_mm_m(train_loader, lr,40,model,path+encoder+ '_wff_IMU8_2D.pth')

mm_wff= MM_wff(24,24,44)
mm_wff.load_state_dict(torch.load(path+encoder+'_wff_IMU8_2D.pth'))
mm_wff.to(device)

mm_wff.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_wff(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_5=np.hstack([rmse,p])

"""### Multi-Head self Attention Module"""

class MM_mha(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(MM_mha, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(3*128, 5)

        self.dropout=nn.Dropout(p=0.05)

        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)



    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)

        out, attn_output_weights=self.attention(x,x,x)

        out = self.fc(out)

        return out

lr = 0.001
model = MM_mha(24,24,44)

mm_mha = train_mm_m(train_loader, lr,40,model,path+encoder+'_mha_IMU8_2D.pth')

mm_mha= MM_mha(24,24,44)
mm_mha.load_state_dict(torch.load(path+encoder+'_mha_IMU8_2D.pth'))
mm_mha.to(device)

mm_mha.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_6=np.hstack([rmse,p])

"""### MHA+Weighted Feature Fusion"""

class MM_mha_wf(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(MM_mha_wf, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(2*3*128,5)

        self.dropout=nn.Dropout(p=0.05)

        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)

        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*3*128+128, 2*3*128+128), nn.Sigmoid())


    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        # out_3=x_acc*x_gyr*x_2D

        out=torch.cat((out_1,out_2),dim=-1)

        # gating_weights_1 = self.gating_net_1(out)
        # out=gating_weights_1*out

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wf(24,24,44)

mm_mha_wf = train_mm_m(train_loader, lr,60,model,path+encoder+'_mha_wf_IMU8_2D.pth')

mm_mha_wf= MM_mha_wf(24,24,44)
mm_mha_wf.load_state_dict(torch.load(path+encoder+'_mha_wf_IMU8_2D.pth'))
mm_mha_wf.to(device)

mm_mha_wf.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wf(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_7=np.hstack([rmse,p])

"""### Weighted Fusion of MHA+Weighted Feature Fusion"""

class MM_mha_wf_fusion(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(MM_mha_wf_fusion, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(2*3*128,5)

        self.dropout=nn.Dropout(p=0.05)

        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)

        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*3*128, 2*3*128), nn.Sigmoid())


    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        out=torch.cat((out_1,out_2),dim=-1)

        gating_weights_1 = self.gating_net_1(out)
        out=gating_weights_1*out

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wf_fusion(24,24,44)

mm_mha_wf_fusion = train_mm_m(train_loader, lr,60,model,path+encoder+'_mha_wf_fusion_IMU8_2D.pth')

mm_mha_wf_fusion= MM_mha_wf_fusion(24,24,44)
mm_mha_wf_fusion.load_state_dict(torch.load(path+encoder+'_mha_wf_fusion_IMU8_2D.pth'))
mm_mha_wf_fusion.to(device)

mm_mha_wf_fusion.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wf_fusion(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_8=np.hstack([rmse,p])

"""### MHA+Tensor Multiplication"""

class MM_mha_wfs(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(MM_mha_wfs, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(3*128+128,5)

        self.dropout=nn.Dropout(p=0.05)
               # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())

        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)

        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(3*128+128, 3*128+128), nn.Sigmoid())


    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        x_3=weights_3*x[:,:,2*128:3*128]
        out_3=x_1+x_2+x_3

        out=torch.cat((out_1,out_3),dim=-1)

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wfs(24,24,44)

mm_mha_wfs = train_mm_m(train_loader, lr,60,model,path+encoder+'_mha_wfs_IMU8_2D.pth')

mm_mha_wfs= MM_mha_wfs(24,24,44)
mm_mha_wfs.load_state_dict(torch.load(path+encoder+'_mha_wfs_IMU8_2D.pth'))
mm_mha_wfs.to(device)

mm_mha_wfs.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wfs(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_9=np.hstack([rmse,p])

"""### Weighted Fusion of MHA+Tensor Multiplication"""

class MM_mha_wfs_fusion(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(MM_mha_wfs_fusion, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(3*128+128,5)

        self.dropout=nn.Dropout(p=0.05)

               # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())



        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)

        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(3*128+128, 3*128+128), nn.Sigmoid())


    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)


        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        x_3=weights_3*x[:,:,2*128:3*128]
        out_3=x_1+x_2+x_3


        out=torch.cat((out_1,out_3),dim=-1)

        gating_weights_1 = self.gating_net_1(out)
        out=gating_weights_1*out

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wfs_fusion(24,24,44)

mm_mha_wfs_fusion = train_mm_m(train_loader, lr,60,model,path+encoder+'_mha_wfs_fusion_IMU8_2D.pth')

mm_mha_wfs_fusion= MM_mha_wfs_fusion(24,24,44)
mm_mha_wfs_fusion.load_state_dict(torch.load(path+encoder+'_mha_wfs_fusion_IMU8_2D.pth'))
mm_mha_wfs_fusion.to(device)

mm_mha_wfs_fusion.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wfs_fusion(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_10=np.hstack([rmse,p])

"""### Weighted Features + Tensor Multiplication"""

class MM_wf_wfs(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(MM_wf_wfs, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(3*128+128,5)

        self.dropout=nn.Dropout(p=0.05)

               # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())


        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)

        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(3*128+128, 3*128+128), nn.Sigmoid())


    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)


        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        x_3=weights_3*x[:,:,2*128:3*128]
        out_3=x_1+x_2+x_3

        out=torch.cat((out_2,out_3),dim=-1)

        out=self.fc(out)

        return out

lr = 0.001
model = MM_wf_wfs(24,24,44)

mm_wf_wfs = train_mm_m(train_loader, lr,60,model,path+encoder+'_wf_wfs_IMU8_2D.pth')

mm_wf_wfs= MM_wf_wfs(24,24,44)
mm_wf_wfs.load_state_dict(torch.load(path+encoder+'_wf_wfs_IMU8_2D.pth'))
mm_wf_wfs.to(device)

mm_wf_wfs.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_wf_wfs(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_11=np.hstack([rmse,p])

"""### Weighted Fusion of weighted features +Tensor Multiplication"""

class MM_wf_wfs_fusion(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(MM_wf_wfs_fusion, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(3*128+128,5)

        self.dropout=nn.Dropout(p=0.05)

               # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())


        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)

        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(3*128+128, 3*128+128), nn.Sigmoid())


    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)


        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        x_3=weights_3*x[:,:,2*128:3*128]
        out_3=x_1+x_2+x_3


        out=torch.cat((out_2,out_3),dim=-1)

        gating_weights_1 = self.gating_net_1(out)
        out=gating_weights_1*out

        out=self.fc(out)

        return out

lr = 0.001
model = MM_wf_wfs_fusion(24,24,44)

mm_wf_wfs_fusion = train_mm_m(train_loader, lr,60,model,path+encoder+'_wf_wfs_fusion_IMU8_2D.pth')

mm_wf_wfs_fusion= MM_wf_wfs_fusion(24,24,44)
mm_wf_wfs_fusion.load_state_dict(torch.load(path+encoder+'_wf_wfs_fusion_IMU8_2D.pth'))
mm_wf_wfs_fusion.to(device)

mm_wf_wfs_fusion.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_wf_wfs_fusion(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_12=np.hstack([rmse,p])

"""### MHA+Weighted Feature+ Tensor Multiplication Fusion"""

class MM_mha_wf_wfs(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(MM_mha_wf_wfs, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(2*3*128+128,5)

        self.dropout=nn.Dropout(p=0.05)

               # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())


        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)

        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*3*128+128, 2*3*128+128), nn.Sigmoid())


    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        x_3=weights_3*x[:,:,2*128:3*128]
        out_3=x_1+x_2+x_3


        out=torch.cat((out_1,out_2,out_3),dim=-1)

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wf_wfs(24,24,44)

mm_mha_wf_wfs = train_mm_m(train_loader, lr,60,model,path+encoder+'_mha_wf_wfs_IMU8_2D.pth')

mm_mha_wf_wfs= MM_mha_wf_wfs(24,24,44)
mm_mha_wf_wfs.load_state_dict(torch.load(path+encoder+'_mha_wf_wfs_IMU8_2D.pth'))
mm_mha_wf_wfs.to(device)

mm_mha_wf_wfs.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wf_wfs(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_13=np.hstack([rmse,p])

"""### Weighted Fusion of MHA+Weighted Feature+ Tensor Multiplication Fusion"""

class MM_mha_wf_wfs_fusion(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(MM_mha_wf_wfs_fusion, self).__init__()

        self.encoder_acc=Encoder(input_acc, drop_prob)
        self.encoder_gyr=Encoder(input_gyr, drop_prob)
        self.encoder_2d=Encoder(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(2*3*128+128,5)

        self.dropout=nn.Dropout(p=0.05)

               # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())


        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)

        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*3*128+128, 2*3*128+128), nn.Sigmoid())


    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, 50, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, 50, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, 50, x_2d_1.size(-1))

        x_acc=self.encoder_acc(x_acc_2)
        x_gyr=self.encoder_gyr(x_gyr_2)
        x_2d=self.encoder_2d(x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)

        out_1, attn_output_weights=self.attention(x,x,x)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        x_3=weights_3*x[:,:,2*128:3*128]
        out_3=x_1+x_2+x_3

        out=torch.cat((out_1,out_2,out_3),dim=-1)

        gating_weights_1 = self.gating_net_1(out)
        out=gating_weights_1*out

        out=self.fc(out)

        return out

lr = 0.001
model = MM_mha_wf_wfs_fusion(24,24,44)

mm_mha_wf_wfs_fusion = train_mm_m(train_loader, lr,60,model,path+encoder+'_mha_wf_wfs_fusion_IMU8_2D.pth')

mm_mha_wf_wfs_fusion= MM_mha_wf_wfs_fusion(24,24,44)
mm_mha_wf_wfs_fusion.load_state_dict(torch.load(path+encoder+'_mha_wf_wfs_fusion_IMU8_2D.pth'))
mm_mha_wf_wfs_fusion.to(device)

mm_mha_wf_wfs_fusion.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = mm_mha_wf_wfs_fusion(data_acc.to(device).float(),data_gyr.to(device).float(),data_2D.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_14=np.hstack([rmse,p])

"""## Multi_Encoder Fusion"""

class Encoder_1(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder_1, self).__init__()
        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.fc = nn.Linear(128, 32)
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)

        return out_2


class Encoder_2(nn.Module):
    def __init__(self, input_dim, dropout):
        super(Encoder_2, self).__init__()
        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)
        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)
        self.flatten=nn.Flatten()
        self.fc = nn.Linear(128, 32)
        self.dropout=nn.Dropout(dropout)


    def forward(self, x):
        out_1, _ = self.lstm_1(x)
        out_1=self.dropout(out_1)
        out_2, _ = self.lstm_2(out_1)
        out_2=self.dropout(out_2)

        return out_2

class ME_mha_wf_wfs_fusion(nn.Module):
    def __init__(self, input_acc, input_gyr,input_2D, drop_prob=0.25):
        super(ME_mha_wf_wfs_fusion, self).__init__()

        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)
        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)
        self.encoder_1_2d=Encoder_1(input_2D, drop_prob)

        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)
        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)
        self.encoder_2_2d=Encoder_2(input_2D, drop_prob)

        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)
        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)
        self.BN_2d= nn.BatchNorm1d(input_2D, affine=False)

        self.fc = nn.Linear(2*3*128+128,5)
        self.dropout=nn.Dropout(p=0.05)

        self.gate_1=GatingModule(128)
        self.gate_2=GatingModule(128)
        self.gate_3=GatingModule(128)

        self.fc_kd = nn.Linear(3*128, 128)

               # Define the gating network
        self.weighted_feat = nn.Sequential(
            nn.Linear(128, 1),
            nn.Sigmoid())

        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)
        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())
        self.gating_net_1 = nn.Sequential(nn.Linear(2*3*128+128, 2*3*128+128), nn.Sigmoid())


    def forward(self, x_acc, x_gyr, x_2d):

        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))
        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))
        x_2d_1=x_2d.view(x_2d.size(0)*x_2d.size(1),x_2d.size(-1))

        x_acc_1=self.BN_acc(x_acc_1)
        x_gyr_1=self.BN_gyr(x_gyr_1)
        x_2d_1=self.BN_2d(x_2d_1)

        x_acc_2=x_acc_1.view(-1, w, x_acc_1.size(-1))
        x_gyr_2=x_gyr_1.view(-1, w, x_gyr_1.size(-1))
        x_2d_2=x_2d_1.view(-1, w, x_2d_1.size(-1))

        x_acc_1=self.encoder_1_acc(x_acc_2)
        x_gyr_1=self.encoder_1_gyr(x_gyr_2)
        x_2d_1=self.encoder_1_2d(x_2d_2)

        x_acc_2=self.encoder_2_acc(x_acc_2)
        x_gyr_2=self.encoder_2_gyr(x_gyr_2)
        x_2d_2=self.encoder_2_2d(x_2d_2)

        x_acc=self.gate_1(x_acc_1,x_acc_2)
        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)
        x_2d=self.gate_3(x_2d_1,x_2d_2)

        x=torch.cat((x_acc,x_gyr,x_2d),dim=-1)
        x_kd=self.fc_kd(x)

        out_1, attn_output_weights=self.attention(x,x,x)

        gating_weights = self.gating_net(x)
        out_2=gating_weights*x

        weights_1 = self.weighted_feat(x[:,:,0:128])
        weights_2 = self.weighted_feat(x[:,:,128:2*128])
        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])
        x_1=weights_1*x[:,:,0:128]
        x_2=weights_2*x[:,:,128:2*128]
        x_3=weights_3*x[:,:,2*128:3*128]
        out_3=x_1+x_2+x_3

        out=torch.cat((out_1,out_2,out_3),dim=-1)

        gating_weights_1 = self.gating_net_1(out)
        out=gating_weights_1*out

        out=self.fc(out)

        return out,x_kd

lr = 0.001
model = ME_mha_wf_wfs_fusion(24, 24, 44)


me_mha_wf_wfs_fusion = train_mm_m(train_loader, lr,60,model,path+encoder+'_me_mha_wf_wfs_fusion_IMU8_2D.pth')

me_mha_wf_wfs_fusion= ME_mha_wf_wfs_fusion(24,24,44)
me_mha_wf_wfs_fusion.load_state_dict(torch.load(path+encoder+'_me_mha_wf_wfs_fusion_IMU8_2D.pth'))
me_mha_wf_wfs_fusion.to(device)

me_mha_wf_wfs_fusion.eval()

# iterate through batches of test data
with torch.no_grad():
    for i, (data, data_acc, data_gyr, data_2D,  target) in enumerate(test_loader):
        output = me_mha_wf_wfs_fusion(data_acc.to(device).float(),data_gyr.to(device).float())
        if i==0:
          yhat_5=output
          test_target=target

        yhat_5=torch.cat((yhat_5,output),dim=0)
        test_target=torch.cat((test_target,target),dim=0)

        # clear memory
        del data, target,output
        torch.cuda.empty_cache()


yhat_4 = yhat_5.detach().cpu().numpy()
test_target = test_target.detach().cpu().numpy()
print(yhat_4.shape)

rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_target,s)

ablation_15=np.hstack([rmse,p])
